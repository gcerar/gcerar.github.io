{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bernoulli Multi-Armed Bandit Problem\n",
        "\n",
        "Gregor Cerar  \n",
        "2023-12-20\n",
        "\n",
        "Bernoulli Multi-Armed Bandit Problem is a foundation and the simplest\n",
        "form of this kind of problem …\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Credits to [Lil’s blog\n",
        "post](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/).\n",
        "I slightly improved and extended it for myself to better understand\n",
        "terms from statistics."
      ],
      "id": "f89f8822-0252-43da-ab68-002b32cc327b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import cast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "\n",
        "%config InlineBackend.figure_formats = {'retina', 'png'}"
      ],
      "id": "cell-2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseBandit(ABC):\n",
        "    n: int\n",
        "    best_proba: float | np.float64\n",
        "    probas: npt.NDArray[np.float64]\n",
        "\n",
        "    @abstractmethod\n",
        "    def __init__(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate_reward(self, i: int) -> float:\n",
        "        \"\"\"Returns reward after lever `i` is pulled.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class BaseSolver(ABC):\n",
        "    actions: list[int]\n",
        "    rewards: list[float]\n",
        "    regrets: list[float]\n",
        "\n",
        "    bandit: BaseBandit\n",
        "    counts: npt.NDArray[np.int64]\n",
        "\n",
        "    @abstractmethod\n",
        "    def __init__(self, bandit: BaseBandit) -> None:\n",
        "        \"\"\"bandit (BaseBandit): the target bandit to solve.\"\"\"\n",
        "        assert isinstance(bandit, BaseBandit)\n",
        "        self.bandit = bandit\n",
        "\n",
        "        self.counts = np.zeros(self.bandit.n, dtype=np.int64)\n",
        "\n",
        "        self.actions = []  # a list of machine ids, 0 to bandit n-1.\n",
        "        self.rewards = []  # a list of collected rewards.\n",
        "        self.regrets = []  # a list of regrets for taken actions.\n",
        "\n",
        "    def update_regret(self, i: int) -> None:\n",
        "        \"\"\"Update the regret after the lever `i` is pulled.\"\"\"\n",
        "        # i (int): index of the selected machine.\n",
        "        regret = self.bandit.best_proba - self.bandit.probas[i]\n",
        "        self.regrets.append(regret)\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def estimated_probas(self) -> npt.NDArray[np.float64]:\n",
        "        \"\"\"Helper object property returns corresponding probability for each\n",
        "        arm `n` of the bandit.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def run_one_step(self) -> tuple[int, float]:\n",
        "        \"\"\"Return the machine index and reward to take action on.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def run(self, num_steps: int) -> None:\n",
        "        \"\"\"Run simulation for `num_steps` steps.\"\"\"\n",
        "        for _ in range(num_steps):\n",
        "            i, r = self.run_one_step()\n",
        "\n",
        "            self.counts[i] += 1\n",
        "            self.actions.append(i)\n",
        "            self.update_regret(i)\n",
        "\n",
        "            self.rewards.append(r)"
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition\n",
        "\n",
        "A common formulation is the Binary multi-armed bandit or Bernoulli\n",
        "multi-armed bandit, which issues a reward of one with probability $p$,\n",
        "and otherwise a reward of zero.\n",
        "\n",
        "A Bernoulli multi-armed bandit can be described as a tuple of\n",
        "$\\langle \\mathcal{A}, \\mathcal{R} \\rangle$, where:\n",
        "\n",
        "-   We have $K$ machines with probabilities\n",
        "    $\\{ \\theta_1, \\ldots, \\theta_K \\}$.\n",
        "-   At each time step $t$, we take an action $a$ on one slot machine and\n",
        "    receive a reward $r$.\n",
        "-   $\\mathcal{A}$ is a set of actions, each referring to the interaction\n",
        "    with one of the slot machines. The value of action $a$ is expected\n",
        "    reward $Q(a) = \\mathbb{E}[r|a] = \\theta$. If action $a_t$ at the\n",
        "    time step $t$ is on the $i$-th machine, then $Q(a_t) = \\theta_i$.\n",
        "-   $\\mathcal{R}$ is a reward function. In the case of Bernoulli bandit,\n",
        "    we observe a reward $r$ in a stochastic fashion. At the time step\n",
        "    $t$, $r_t = \\mathcal{R}(a_t)$ may return reward $1$ with a\n",
        "    probability $Q(a_t)$ or $0$ otherwise.\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> Let’s stop for a moment and think about given facts. The [Bernoulli\n",
        "> distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) is\n",
        "> a kind of a discrete probability distribution, which takes the value 1\n",
        "> with probability $p$ and the value 0 with probability $q = 1 - p$.\n",
        ">\n",
        "> The $\\mathbb{E}[\\cdot]$ is the [expected value\n",
        "> operator](https://en.wikipedia.org/wiki/Expected_value), which returns\n",
        "> “expected” value. The expected value (also called expectation,\n",
        "> expectancy, expectation operator, mathematical expectation, mean,\n",
        "> average, or first moment) is a generalization of the weighted average.\n",
        ">\n",
        "> In English, the expression $\\mathbb{E}[r|a]$ reads like “the expected\n",
        "> value of reward ($r$) given action ($a$).” It represents the\n",
        "> conditional expectation of the reward ($r$) when specific actions\n",
        "> ($a$) are taken.\n",
        ">\n",
        "> The probabilities $\\{\\theta_1, \\ldots, \\theta_k\\}$ are **NOT** known\n",
        "> to model in advance.\n",
        "\n",
        "A Bernoulli multi-armed bandit is a simplified version of Markov\n",
        "decision process, as there is not state $S$. The goal is to maximize the\n",
        "cumulative rewards $\\sum_{t=1}^{T} r_{t}$. If we know the optimal action\n",
        "with the best reward, then the goal is same as to minimize the potential\n",
        "**regret** or loss by not picking the optimal action.\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> Other resources also introduce $H$ as a “horizon”, which is a number\n",
        "> steps, and $\\mu$ as a step.\n",
        ">\n",
        "> **How can we define loss function if we don’t know the algorithm\n",
        "> doesn’t know the optimal reward?**\n",
        "\n",
        "The optimal reward probability $\\theta^*$ of the optimal action $a^*$\n",
        "is:\n",
        "\n",
        "$$ \\theta^_ = Q(a^_) = \\max*{a \\in \\mathcal{A}} Q(a) = \\max*{1 \\leq i \\leq K} \\theta_i $$\n",
        "\n",
        "Our loss function is the total regret we might have by not selecting the\n",
        "optimal action up to the time step $T$:\n",
        "\n",
        "$$ \\mathcal{L}_T = \\mathbb{E}[\\sum_{t-1}^T(\\theta^\\* - Q(a_t))] $$"
      ],
      "id": "736210bb-39d6-4df6-83e9-947a357c91be"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BernoulliBandit(BaseBandit):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n: int,\n",
        "        probas: list[float] | npt.NDArray[np.float64] | None = None,\n",
        "        random_state: int | None = None,\n",
        "    ):\n",
        "        # Sanity check: `probas` needs to be None or of size `n`.\n",
        "        assert probas is None or len(probas) == n\n",
        "\n",
        "        # Save number of bandits\n",
        "        self.n = n\n",
        "\n",
        "        # np.random.seed(int(time.time()))\n",
        "        self.rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "        # Random probabilities, if they are explicitly defined\n",
        "        if probas is None:\n",
        "            probas = self.rng.random(size=self.n)\n",
        "\n",
        "        # convert to numpy array for easier operations later\n",
        "        self.probas = np.asarray(probas)\n",
        "\n",
        "        # In case of Bernoulli MAB, highest probabily is equal to optimal\n",
        "        self.best_proba = np.max(self.probas)\n",
        "\n",
        "    def generate_reward(self, i: int) -> int:\n",
        "        # The player selected the i-th machine.\n",
        "        return int(self.rng.random() < self.probas[i])"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note**\n",
        ">\n",
        "> Up until now, we only defined the environments.\n",
        "\n",
        "# Bandit Strategies\n",
        "\n",
        "Based on how we do exploration, there several ways to solve the\n",
        "multi-armed bandit.\n",
        "\n",
        "-   No exploration: the most naive approach and a bad one.\n",
        "-   Exploration at random\n",
        "-   Exploration smartly with preference to uncertainty\n",
        "\n",
        "## Epsilon-Greedy Algorithm\n",
        "\n",
        "The $\\epsilon$-greedy algorithm takes the best action most of the time,\n",
        "but does random exploration occasionally. The action value is estimated\n",
        "according to the past experience by averaging the rewards associated\n",
        "with the target action a that we have observed so far (up to the current\n",
        "time step $t$):\n",
        "\n",
        "$$ \\hat{Q}_t(a) = \\frac{1}{N_t(a)} \\sum_{\\tau = 1}^{t} r*{\\tau} \\mathbb{1}[a*{\\tau} = a] $$\n",
        "\n",
        "where $\\mathbb{1}$ is a binary indicator function and $N_t(a)$ is how\n",
        "many times the action $a$ has been selected so far,\n",
        "$N_t(a) = \\sum_{\\tau = 1}^t \\mathbb{1}[a_{\\tau} = a]$.\n",
        "\n",
        "According to the $\\epsilon$-greedy algorithm, with a small probability\n",
        "$\\epsilon$ we take a random action, but otherwise (which should be the\n",
        "most of the time, probability $1 - \\epsilon$) we pick the best action\n",
        "that we have learnt so far:\n",
        "\n",
        "$$ \\hat{a}^{\\*}_t = \\arg\\max_{a \\in \\mathcal{A}} \\hat{Q}\\_t(a) $$"
      ],
      "id": "0408de71-caa3-441c-96a1-88c32f54a711"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EpsilonGreedy(BaseSolver):\n",
        "    def __init__(self, bandit: BaseBandit, eps: float, init_proba: float = 1.0, random_state: int | None = None):\n",
        "        \"\"\"\n",
        "        eps (float): the probability to explore at each time step.\n",
        "        init_proba (float): default to be 1.0; optimistic initialization\n",
        "        \"\"\"\n",
        "        super().__init__(bandit)\n",
        "\n",
        "        assert 0.0 <= eps <= 1.0\n",
        "        self.eps = eps\n",
        "\n",
        "        # Optimistic initialization\n",
        "        self.estimates = np.full(self.bandit.n, init_proba)\n",
        "\n",
        "        # Define random generator with seed for reproducibility\n",
        "        self.rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "    @property\n",
        "    def estimated_probas(self):\n",
        "        return self.estimates\n",
        "\n",
        "    def run_one_step(self):\n",
        "        # With probability epsilon pick random exploration, or pick best recorded lever.\n",
        "        i = self.rng.integers(0, self.bandit.n) if (self.rng.random() < self.eps) else int(np.argmax(self.estimates))\n",
        "\n",
        "        r = self.bandit.generate_reward(i)\n",
        "        self.estimates[i] += 1.0 / (self.counts[i] + 1) * (r - self.estimates[i])\n",
        "\n",
        "        return i, r"
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upper Confidence Bounds\n",
        "\n",
        "Random exploration gives us an opportunity to try out options that we\n",
        "have not known much about. However, due to the randomness, it is\n",
        "possible we end up exploring a bad action which we have confirmed in the\n",
        "past (bad luck!). To avoid such inefficient exploration, one approach is\n",
        "to decrease the parameter $\\epsilon$ in time and the other is to be\n",
        "optimistic about options with *high uncertainty* and thus to prefer\n",
        "actions for which we haven’t had a confident value estimation yet. Or in\n",
        "other words, we favor exploration of actions with a strong potential to\n",
        "have a optimal value.\n",
        "\n",
        "The Upper Confidence Bounds (UCB) algorithm measures this potential by\n",
        "an upper confidence bound of the reward value, $\\hat{U}_t(a)$, so that\n",
        "the true value is below with bound\n",
        "$Q(a) \\lt \\hat{Q}_t(a) + \\hat{U}_t(a)$ with high probability. The upper\n",
        "bound $\\hat{U}_t(a)$ is a function of $N_t(a)$; a larger number of\n",
        "trials should give us a smaller bound $\\hat{U}_t(a)$.\n",
        "\n",
        "In UCB algorithm, we always select the greediest action to maximize the\n",
        "upper confidence bound:\n",
        "\n",
        "$$ a*t^{\\textrm{UCB}} = \\arg\\max*{a \\in \\mathcal{A}} \\hat{Q}\\_t(a) + \\hat{U}\\_t(a) $$\n",
        "\n",
        "Now, the question is *how to estimate the upper confidence bound*.\n",
        "\n",
        "## Hoeffding’s Inequality\n",
        "\n",
        "If we do not want to assign any prior knowledge on how the distribution\n",
        "looks like, we can get help from “[Hoeffding’s\n",
        "Inequality](http://cs229.stanford.edu/extra-notes/hoeffding.pdf)” – a\n",
        "theorem applicable to any bounded distribution. **(What is bounded\n",
        "distribution???)**\n",
        "\n",
        "Let $X_1, \\ldots, X_t$ be i.i.d. (independent and identically\n",
        "distributed) random variables and they are all bounded by interval\n",
        "$[0, 1]$. The sample mean is\n",
        "$\\overline{X}_t = \\frac{1}{t} \\sum_{\\tau = 1}^t X_{\\tau}$. Then for\n",
        "$u \\gt 0$, we have:\n",
        "\n",
        "$$ \\mathbb{P}[\\mathbb{E}[X] \\gt \\overline{X}\\_t + u] \\leq e^{-2tu^2} $$\n",
        "\n",
        "Given one target action a, let us consider:\n",
        "\n",
        "-   $r_t(a)$ as random variable,\n",
        "-   $Q(a)$ as the true mean,\n",
        "-   $\\hat{Q}_t(a)$ as the sample mean,\n",
        "-   and $u$ as the upper confidence bound, $u = U_t(a)$\n",
        "\n",
        "Then we have,\n",
        "\n",
        "$$ \\mathbb{P}[Q(a) > \\hat{Q}_t(a) + U_t(a)] \\leq e^{-2tU_t(a)^2} $$\n",
        "\n",
        "We want to pick a bound so that with higher chances the mean is below\n",
        "the true mean + the upper confidence bound. Thus $e^{-2tU_t(a)^2}$\n",
        "should be small probability. Let’s say we are OK, with a tiny threshold\n",
        "$p$:\n",
        "\n",
        "$$ e^{-2tU(a)^2} = p, \\quad\\textrm{thus}\\quad U_t(a) = \\sqrt{\\frac{-\\log{p}}{2N_t(a)}} $$\n",
        "\n",
        "## UCB1\n",
        "\n",
        "One heuristic is to reduce the threshold $p$ in time, as we want ot make\n",
        "more confident bound estimation with more rewards observed. set\n",
        "$p = t^{-4}$ we get **UCB1** algorithm:\n",
        "\n",
        "$$ U*t(a) = \\sqrt{\\frac{2\\log{t}}{N_t(a)}}, \\quad\\textrm{and}\\quad a_t^\\textrm{UCB1} = \\arg\\max*{a \\in \\mathbb{A}} Q(a) + \\sqrt{\\frac{2\\log{t}}{N_t(a)}} $$"
      ],
      "id": "81076cba-bf17-4aef-9936-af298ce302f0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UCB1(BaseSolver):\n",
        "    def __init__(self, bandit: BaseBandit, init_proba: float = 1.0):\n",
        "        super().__init__(bandit)\n",
        "        self.t = 0\n",
        "        self.estimates = np.full(self.bandit.n, init_proba)\n",
        "\n",
        "    @property\n",
        "    def estimated_probas(self):\n",
        "        return self.estimates\n",
        "\n",
        "    def run_one_step(self) -> tuple[int, float]:\n",
        "        self.t += 1\n",
        "\n",
        "        # Pick the best one with consideration of upper confidence bounds.\n",
        "        i = np.argmax(self.estimates + np.sqrt(2 * np.log(self.t) / (1 + self.counts)))\n",
        "        i = cast(int, i)\n",
        "\n",
        "        r = self.bandit.generate_reward(i)\n",
        "        self.estimates[i] += 1.0 / (self.counts[i] + 1) * (r - self.estimates[i])\n",
        "\n",
        "        return i, r"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian UCB\n",
        "\n",
        "In UCB or UCB1 algorithm, we do not assume any prior on the reward\n",
        "distribution and therefore we have to rely on the Hoeffding’s Inequality\n",
        "for a very generalize estimation. If we are able to know the\n",
        "distribution upfront, we would be able to make better bound estimation.\n",
        "\n",
        "For example, if we expect the mean reward of every slot machine to be\n",
        "Gaussian as in Fig 2, we can set the upper bound as 95% confidence\n",
        "interval by setting $\\hat{U}_t(a)$ to be twice the standard deviation.\n",
        "\n",
        "TODO\n",
        "\n",
        "# Benchmark"
      ],
      "id": "ac976d50-74b8-4524-b9f8-5e96fb99d3cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_STEPS = 10_000\n",
        "SEED = 42\n",
        "\n",
        "# Probabilities {0.0, 0.1, ..., 0.9} then shuffle them\n",
        "probas = np.arange(0, 1, 0.1, dtype=np.float64)\n",
        "np.random.shuffle(probas)\n",
        "\n",
        "bbandit = BernoulliBandit(n=10, probas=probas, random_state=SEED)\n",
        "\n",
        "epsgreedy = EpsilonGreedy(bbandit, eps=0.01, random_state=SEED)\n",
        "epsgreedy.run(N_STEPS)\n",
        "\n",
        "# Random is a special case of EpsilogGreedy\n",
        "random = EpsilonGreedy(bbandit, eps=1.0, random_state=SEED)\n",
        "random.run(N_STEPS)\n",
        "\n",
        "ucb1 = UCB1(bbandit)\n",
        "ucb1.run(N_STEPS)"
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "fig, axes = plt.subplots(ncols=3, figsize=(8, 3), dpi=100, tight_layout=True)\n",
        "\n",
        "solvers_labels = {\"Random\": random, \"$\\\\epsilon$-greedy\": epsgreedy, \"UCB1\": ucb1}\n",
        "\n",
        "# Figure with cumulative regret\n",
        "ax = axes[0]\n",
        "for label, solver in solvers_labels.items():\n",
        "    ax.plot(np.cumsum(solver.regrets), label=label, clip_on=False)\n",
        "\n",
        "ax.set_xlabel(\"Time steps\")\n",
        "ax.set_ylabel(\"Cumulative regret\")\n",
        "\n",
        "\n",
        "# Figure with estimated probability per action\n",
        "ax = axes[1]\n",
        "sorted_indices = np.argsort(bbandit.probas)\n",
        "ax.plot(bbandit.probas[sorted_indices], \"k--\", markersize=12)\n",
        "\n",
        "for label, solver in solvers_labels.items():\n",
        "    ax.plot(\n",
        "        solver.estimated_probas[sorted_indices],\n",
        "        \"x\",\n",
        "        markeredgewidth=2,\n",
        "        label=label,\n",
        "        clip_on=False,\n",
        "        alpha=0.7,\n",
        "    )\n",
        "\n",
        "ax.set_xlabel(\"Actions sorted by $\\\\theta$\")\n",
        "ax.set_ylabel(\"Estimated\")\n",
        "ax.xaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0))\n",
        "ax.xaxis.set_minor_locator(ticker.NullLocator())\n",
        "ax.set_ylim(0.0, 1.0)\n",
        "# ax.legend()\n",
        "\n",
        "# Figure with action selection rate\n",
        "ax = axes[2]\n",
        "for label, solver in solvers_labels.items():\n",
        "    ax.step(\n",
        "        range(bbandit.n),\n",
        "        solver.counts[sorted_indices] / len(solver.regrets),\n",
        "        lw=2,\n",
        "        label=label,\n",
        "        clip_on=False,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "\n",
        "ax.set_xlabel(\"Actions\")\n",
        "ax.set_ylabel(\"Frac. # trials\")\n",
        "ax.xaxis.set_major_locator(ticker.IndexLocator(base=1, offset=0))\n",
        "ax.xaxis.set_minor_locator(ticker.NullLocator())\n",
        "ax.set_ylim(0.0, 1.0)\n",
        "\n",
        "handles, labels = [], []\n",
        "for ax in fig.axes:\n",
        "    _handles, _labels = ax.get_legend_handles_labels()\n",
        "    handles.extend(_handles)\n",
        "    labels.extend(_labels)\n",
        "\n",
        "by_label = dict(zip(labels, handles, strict=True))\n",
        "fig.legend(\n",
        "    by_label.values(),\n",
        "    by_label.keys(),\n",
        "    loc=8,\n",
        "    ncols=len(by_label),\n",
        "    bbox_to_anchor=(0.5, -0.1),\n",
        "    fancybox=True,\n",
        "    frameon=True,\n",
        ")\n",
        "\n",
        "plt.show()"
      ],
      "id": "cell-fig-benchmark"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions"
      ],
      "id": "4fbae652-28a9-4fe4-bb86-7cfb8da51fea"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "blog",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  }
}