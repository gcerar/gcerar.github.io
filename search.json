[
  {
    "objectID": "posts/simple-generative-advsersarial-networks/index.html",
    "href": "posts/simple-generative-advsersarial-networks/index.html",
    "title": "Generative Adversarial Networks",
    "section": "",
    "text": "Generative Adversarial Networks (GANs) are unsuperiv AI models used for unsupervised learning. They have two neural networks: the generator (creates data) and the discriminator (evaluates data authenticity). The generator aims to fool the discriminator by producing realistic data, while the discriminator tries to differentiate real from fake. Over iterations, the generator’s data becomes more convincing.\nAn analogy: consider two kids, one drawing counterfeit money (“Generator”) and another assessing its realism (“Discriminator”). Over time, the counterfeit drawings become increasingly convincing."
  },
  {
    "objectID": "posts/simple-generative-advsersarial-networks/index.html#vanilla-gan-overview",
    "href": "posts/simple-generative-advsersarial-networks/index.html#vanilla-gan-overview",
    "title": "Generative Adversarial Networks",
    "section": "Vanilla GAN Overview",
    "text": "Vanilla GAN Overview\nStarting with the basic “vanilla” GAN is crucial for foundational understanding. Here, “Vanilla” refers to the original and simplest form of the model, not a flavor.\nRemember our two kids: the Generator and the Discriminator? Let’s formalize their actions:\n\nGenerator \\(G\\) takes random noise \\(z\\) as input and produces a fabricated data.\nDiscriminator \\(D\\) differentiates between real and generated data.\n\n\n\n\nGAN architecture\n\n\n\nGANs Objective Function\nThe interaction between the Generator and the Discriminator can be quantified by their objective or loss functions:\n\nDiscriminator’s Objective: For real data \\(x\\), \\(D\\) wants \\(D(x)\\) near \\(1\\). For generated data \\(G(z)\\), it targets \\(D(G(z))\\) close to \\(0\\). Its objective is:\n\n\\[\n\\mathcal{L}(D) = \\log(D(x)) + \\log(1 - D(G(z))).\n\\]\n\nGenerator’s Objective: \\(G\\) aims for \\(D(G(z))\\) to approach \\(1\\), given by:\n\n\\[\n\\mathcal{L}(G) = \\log⁡(1 − D(G(z)))\n\\]\nBoth \\(G\\) and \\(D\\) continuously improve to outperform each other in this game.\n\n\nMinimax Game in GANs\nVanilla GANs are structured around the minimax game from game theory:\n\\[\n\\min_{G}\\max_{D} \\mathcal{L}(D, G) = \\log(D(x)) + \\log(1 - D(G(z)))\n\\]\nIn essence:\n\nDiscriminator: Maximizes its capacity to differentiate real data from generated.\nGenerator: Minimizes the discriminator’s success rate by producing superior forgeries.\n\nThe iterative competition refines both, targeting a proficient Generator and a perceptive Discriminator.\n\n\nThe Implementation\n\nSetting Up Vanilla GANs\nWith the theory behind Vanilla GANs clear, we’ll proceed to the code. In the upcoming sections, we’ll:\n\nImport necessary libraries, primarily PyTorch and matplotlib.\nDefine constants, including project path and seed for consistency.\nDetermine the computational device (e.g., GPU).\nProvide a weight initialization helper function.\n\n\nfrom typing import List, Tuple, Callable\nfrom pathlib import Path\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n%config InlineBackend.figure_formats = {'retina', 'png'}\n\nimport torch\nfrom torch import Tensor, nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import ConcatDataset, DataLoader, Dataset\nfrom torchvision import transforms as T\nfrom torchvision.utils import make_grid\n\nfrom tqdm import tqdm\n\nfrom torchinfo import summary\n\nSEED = 0\n\nPROJECT_PATH = Path('.').resolve()\nFIGURE_PATH = PROJECT_PATH / 'figures'\nROOT_PATH = PROJECT_PATH.parents[1]\n#DATASET_PATH = ROOT_PATH / 'datasets'\nDATASET_PATH = Path.home() / 'datasets'\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    # Allow CuDNN internal benchmarking for architecture-specific optimizations\n    torch.backends.cudnn.benchmark = True\n\n\ndef weights_init(net: nn.Module) -&gt; None:\n    for m in net.modules():\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n            nn.init.normal_(m.weight, 0.0, 0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n            nn.init.constant_(m.weight, 1.0)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)\n            #nn.init.normal_(m.weight, 1.0, 0.02)\n            #if m.bias is not None:\n            #    nn.init.constant_(m.bias, 0)\n\n        elif isinstance(m, nn.Linear):\n            nn.init.normal_(m.weight, 0, 0.01)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n\n\nGenerator in GANs\nThe Generator in GANs acts as an artist, crafting data.\nInput: Takes random noise, typically from a standard normal distribution. Architecture: Uses dense layers, progressively increasing data dimensions. Output: Reshapes data to desired format (e.g., image). Often uses ‘tanh’ for activation. Objective: Generate data indistinguishable from real by the Discriminator.\n\nclass Generator(nn.Module):\n    def __init__(self, out_dim:list[int], nz:int=100, ngf:int=256, alpha:float=0.2):\n        \"\"\"\n        :param out_dim: output image dimension\n        :param nz: size of the latent z vector\n        :param ngf: size of feature maps (units in the hidden layers) in the generator\n        :param alpha: negative slope of leaky ReLU activation\n        \"\"\"\n        super().__init__()\n        self.out_dim = out_dim\n        self.model = nn.Sequential(\n            nn.Linear(nz, ngf),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Linear(ngf, ngf*2),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Linear(ngf*2, ngf*4),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Linear(ngf*4, int(np.prod(self.out_dim))),\n            nn.Tanh(),\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = self.model(x)\n        x = torch.reshape(x, (x.size(0), *self.out_dim))\n        return x\n\nsummary(Generator(out_dim=[1, 28, 28]), input_size=[128, 100])\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nGenerator                                [128, 1, 28, 28]          --\n├─Sequential: 1-1                        [128, 784]                --\n│    └─Linear: 2-1                       [128, 256]                25,856\n│    └─LeakyReLU: 2-2                    [128, 256]                --\n│    └─Linear: 2-3                       [128, 512]                131,584\n│    └─LeakyReLU: 2-4                    [128, 512]                --\n│    └─Linear: 2-5                       [128, 1024]               525,312\n│    └─LeakyReLU: 2-6                    [128, 1024]               --\n│    └─Linear: 2-7                       [128, 784]                803,600\n│    └─Tanh: 2-8                         [128, 784]                --\n==========================================================================================\nTotal params: 1,486,352\nTrainable params: 1,486,352\nNon-trainable params: 0\nTotal mult-adds (M): 190.25\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 2.64\nParams size (MB): 5.95\nEstimated Total Size (MB): 8.63\n==========================================================================================\n\n\n\n\nDiscriminator in GANs\nThe Discriminator is GAN’s evaluator, distinguishing real from fake data.\n\nInput: Takes either real data samples or those from the Generator.\nArchitecture: Employs dense layers for binary classification of the input.\nOutput: Uses a sigmoid activation, yielding a score between 0-1 reflecting the data’s authenticity.\nObjective: Recognize real data and identify fake data from the Generator.\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_dim, ndf=128, alpha=0.2):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(input_dim)), ndf*4),\n            nn.LeakyReLU(alpha, inplace=True),\n            nn.Dropout(0.3),\n\n            nn.Linear(ndf*4, ndf*2),\n            nn.LeakyReLU(alpha, inplace=True),\n            nn.Dropout(0.3),\n\n            nn.Linear(ndf*2, ndf),\n            nn.LeakyReLU(alpha, inplace=True),\n            nn.Dropout(0.3),\n\n            nn.Linear(ndf, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = torch.reshape(x, (x.size(0), -1))\n        return self.model(x)\n\nsummary(Discriminator(input_dim=(1, 28, 28)), input_size=[128, 1, 28, 28])\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nDiscriminator                            [128, 1]                  --\n├─Sequential: 1-1                        [128, 1]                  --\n│    └─Linear: 2-1                       [128, 512]                401,920\n│    └─LeakyReLU: 2-2                    [128, 512]                --\n│    └─Dropout: 2-3                      [128, 512]                --\n│    └─Linear: 2-4                       [128, 256]                131,328\n│    └─LeakyReLU: 2-5                    [128, 256]                --\n│    └─Dropout: 2-6                      [128, 256]                --\n│    └─Linear: 2-7                       [128, 128]                32,896\n│    └─LeakyReLU: 2-8                    [128, 128]                --\n│    └─Dropout: 2-9                      [128, 128]                --\n│    └─Linear: 2-10                      [128, 1]                  129\n│    └─Sigmoid: 2-11                     [128, 1]                  --\n==========================================================================================\nTotal params: 566,273\nTrainable params: 566,273\nNon-trainable params: 0\nTotal mult-adds (M): 72.48\n==========================================================================================\nInput size (MB): 0.40\nForward/backward pass size (MB): 0.92\nParams size (MB): 2.27\nEstimated Total Size (MB): 3.59\n==========================================================================================\n\n\n\n\n\nTraning the Vanilla GAN\nThe training process is iterative:\n\nUpdate Discriminator: With the Generator static, improve the Discriminator’s detection of real vs. fake.\nUpdate Generator: With a static Discriminator, enhance the Generator’s ability to deceive.\n\nTraining continues until the Generator produces almost authentic data. Equilibrium is reached when the Discriminator sees every input as equally likely real or fake, assigning a probability of \\(\\frac{1}{2}\\).\n\n\n\n\n\n\nNote\n\n\n\nUsing .eval() and .train() modes initially seemed promising for faster training. However, they affected layers like BatchNorm2d and Dropout, making the GAN diverge.\n\n\n\ndef train_step(\n    generator: nn.Module,\n    discriminator: nn.Module,\n    optim_G: optim.Optimizer,\n    optim_D: optim.Optimizer,\n    criterion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    real_data: torch.Tensor,\n    noise_dim: int,\n    device: torch.device,\n) -&gt; tuple[float, float]:\n    batch_size = real_data.size(0)\n    \n    real_data = real_data.to(device)\n    \n    # Labels for real and fake data\n    real_labels = torch.ones(batch_size, 1, device=device)\n    fake_labels = torch.zeros(batch_size, 1, device=device)\n    \n    ### Train Discriminator\n    #generator.eval()\n    #discriminator.train()\n    discriminator.zero_grad()\n    \n    # Real data\n    output_real = discriminator(real_data)\n    loss_D_real = criterion(output_real, real_labels)\n    \n    # Fake data\n    noise = torch.randn(batch_size, noise_dim, device=device)\n    fake_data = generator(noise)\n    output_fake = discriminator(fake_data.detach())\n    loss_D_fake = criterion(output_fake, fake_labels)\n    \n    # Backprop and optimize for discriminator\n    loss_D = (loss_D_real + loss_D_fake) / 2.0\n    loss_D.backward()\n    optim_D.step()\n    \n    ### Train Generator\n    #discriminator.eval()\n    #generator.train()\n    generator.zero_grad()\n    \n    # Recompute fake data’s discriminator scores\n    output_fake = discriminator(fake_data)\n    loss_G = criterion(output_fake, real_labels)\n    \n    # Backprop and optimize for generator\n    loss_G.backward()\n    optim_G.step()\n    \n    return loss_G.item(), loss_D.item()\n\n\n\nEvaluation\nBefore evaluation, we configured the learning rate (LR), optimizer’s \\(\\beta\\) parameters, batch size, and data loader settings for all experiments. We used the MNIST digits and MNIST fashion datasets for assessment.\n\nLR = 0.0002\nBETAS = (0.5, 0.999)\nN_EPOCHS = 100\nBATCH_SIZE = 128\n\n\nloader_kwargs = {\n    'num_workers': 12,\n    'pin_memory': True,\n    'shuffle': True,\n    'batch_size': BATCH_SIZE,\n    'prefetch_factor': 16,\n    'pin_memory_device': device.type,\n}\n\n\nMNIST Digits Dataset\nThe MNIST (Modified National Institute of Standards and Technology) dataset is a well-known collection of handwritten digits, extensively used in the fields of machine learning and computer vision for training and testing purposes. Its simplicity and size make it a popular choice for introductory courses and experiments in image recognition.\nThe dataset contains 70,000 grayscale images of handwritten digits (from 0 to 9). Each image is 28x28 pixels.\n\ndef get_minst_dataset(transform=None) -&gt; Dataset:\n    from torchvision.datasets import MNIST\n    root = str(DATASET_PATH)\n    trainset = MNIST(root=root, train=True, download=True, transform=transform)\n    testset = MNIST(root=root, train=False, download=True, transform=transform)\n    # Combine train and test dataset for more samples.\n    dataset = ConcatDataset([trainset, testset])\n    return dataset\n\n\nIMG_DIM = (1, 28, 28)\nNOISE_DIM = 100\n\n\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.5], [0.5])\n])\n\ndata = get_minst_dataset(transform=transform)\ndataloader = DataLoader(data, **loader_kwargs)\n\ntorch.manual_seed(SEED)\n\n# benchmark_noise is used for the animation to show how output evolve on the same vector\nbenchmark_noise = torch.randn(16*16, NOISE_DIM, device=device)\n\ngenerator = Generator(out_dim=IMG_DIM, nz=NOISE_DIM, ngf=128).to(device)\ngenerator.apply(weights_init)\n\ndiscriminator = Discriminator(input_dim=IMG_DIM, ndf=128).to(device)\ndiscriminator.apply(weights_init)\n\noptimizer_G = optim.AdamW(\n    generator.parameters(),\n    lr=LR,\n    betas=BETAS,\n    weight_decay=1e-4,\n)\n\noptimizer_D = optim.AdamW(\n    discriminator.parameters(),\n    lr=LR,\n    betas=BETAS,\n    weight_decay=1e-4,\n)\n\ncriterion = nn.BCELoss().to(device)\n\n\nanimation = []\n\ng_losses, d_losses = [], []\nfor _ in tqdm(range(N_EPOCHS), unit='epochs'):\n\n    generator.train()\n    discriminator.train()\n\n    for samples_real, _ in dataloader:\n        g_loss, d_loss = train_step(\n            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device\n        )\n\n        g_losses.append(g_loss)\n        d_losses.append(d_loss)\n\n    generator.eval()\n    with torch.inference_mode():\n        images = generator(benchmark_noise)\n        images = images.detach().cpu()\n\n        images = make_grid(images, nrow=16, normalize=True)\n        animation.append(images)\n\n100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [09:44&lt;00:00,  5.85s/epochs]\n\n\n\n\n\n\n\nGenerator and Discriminator loss evolution over epochs using Vanilla GAN on the MNIST digit dataset.\n\n\n\n\n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\nMNIST Fashion Dataset\nThe Fashion MNIST dataset is a collection of grayscale images of 10 different categories of clothing items, designed as a more challenging alternative to the classic MNIST dataset of handwritten digits. Each image in the dataset is 28x28 pixels. The 10 categories include items like t-shirts/tops, trousers, pullovers, dresses, coats, sandals, and more. With 60,000 training images and 10,000 testing images, Fashion MNIST is commonly used for benchmarking machine learning algorithms, especially in image classification tasks.\n\nIMG_DIM = (1, 28, 28)\nNOISE_DIM = 100\n\n\ndef get_mnist_fashion_dataset(transform=None):\n    from torchvision.datasets import FashionMNIST\n    root = str(DATASET_PATH)\n    trainset = FashionMNIST(root=root, train=True, download=True, transform=transform)\n    testset = FashionMNIST(root=root, train=False, download=True, transform=transform)\n    # Combine train and test dataset for more samples.\n    dataset = ConcatDataset([trainset, testset])\n    return dataset\n\n\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.5], [0.5])\n])\n\ndata = get_mnist_fashion_dataset(transform=transform)\ndataloader = DataLoader(data, **loader_kwargs)\n\ntorch.manual_seed(SEED)\n\n# benchmark_noise is used for the animation to show how output evolve on same vector\nbenchmark_noise = torch.randn(16*16, NOISE_DIM, device=device)\n\ngenerator = Generator(out_dim=IMG_DIM, nz=NOISE_DIM).to(device)\ngenerator.apply(weights_init)\n\ndiscriminator = Discriminator(input_dim=IMG_DIM).to(device)\ndiscriminator.apply(weights_init)\n\noptimizer_G = optim.AdamW(\n    generator.parameters(),\n    lr=LR,\n    betas=BETAS,\n    weight_decay=1e-4,\n)\n\noptimizer_D = optim.AdamW(\n    discriminator.parameters(),\n    lr=LR,\n    betas=BETAS,\n    weight_decay=1e-4,\n)\n\ncriterion = nn.BCELoss().to(device)\n\n\nanimation = []\n\ng_losses, d_losses = [], []\nfor _ in tqdm(range(N_EPOCHS), unit='epochs'):\n\n    generator.train()\n    discriminator.train()\n\n    for samples_real, _ in dataloader:\n        g_loss, d_loss = train_step(\n            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device\n        )\n\n        g_losses.append(g_loss)\n        d_losses.append(d_loss)\n\n    generator.eval()\n    with torch.inference_mode():\n        images = generator(benchmark_noise)\n        images = images.detach().cpu()\n\n        images = make_grid(images, nrow=16, normalize=True)\n\n        animation.append(images)\n\n100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [09:57&lt;00:00,  5.98s/epochs]\n\n\n\n\n\n\n\nGenerator and Discriminator loss evolution over epochs using Vanilla GAN on the MNIST fashion dataset.\n\n\n\n\n\n\n\n \n Your browser does not support the video tag."
  },
  {
    "objectID": "posts/simple-generative-advsersarial-networks/index.html#dcgan-overview",
    "href": "posts/simple-generative-advsersarial-networks/index.html#dcgan-overview",
    "title": "Generative Adversarial Networks",
    "section": "DCGAN Overview",
    "text": "DCGAN Overview\nDCGAN, short for Deep Convolutional Generative Adversarial Network, differs from vanilla GAN by using convolutional layers. This design makes DCGAN better for image data. With specific architectural guidelines, DCGAN trains more consistently and generates clearer images than vanilla GANs across various hyperparameters.\n\nImplementation\n\nclass Generator(nn.Module):\n    def __init__(self, nz:int=100, ngf:int=32, nc:int=1):\n        \"\"\"\n        :param nz: size of the latent z vector\n        :param ngf: size of feature maps in generator\n        :param nc: number of channels in the training images.\n        \"\"\"\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf*4, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf*4),\n            nn.ReLU(inplace=True),\n\n            nn.ConvTranspose2d(ngf*4, ngf*2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf*2),\n            nn.ReLU(inplace=True),\n\n            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(inplace=True),\n\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = torch.reshape(x, (x.size(0), -1, 1, 1))\n        return self.layers(x)\n\nsummary(Generator(), input_size=(128, 100))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nGenerator                                [128, 1, 28, 28]          --\n├─Sequential: 1-1                        [128, 1, 28, 28]          --\n│    └─ConvTranspose2d: 2-1              [128, 128, 4, 4]          204,800\n│    └─BatchNorm2d: 2-2                  [128, 128, 4, 4]          256\n│    └─ReLU: 2-3                         [128, 128, 4, 4]          --\n│    └─ConvTranspose2d: 2-4              [128, 64, 7, 7]           73,728\n│    └─BatchNorm2d: 2-5                  [128, 64, 7, 7]           128\n│    └─ReLU: 2-6                         [128, 64, 7, 7]           --\n│    └─ConvTranspose2d: 2-7              [128, 32, 14, 14]         32,768\n│    └─BatchNorm2d: 2-8                  [128, 32, 14, 14]         64\n│    └─ReLU: 2-9                         [128, 32, 14, 14]         --\n│    └─ConvTranspose2d: 2-10             [128, 1, 28, 28]          512\n│    └─Tanh: 2-11                        [128, 1, 28, 28]          --\n==========================================================================================\nTotal params: 312,256\nTrainable params: 312,256\nNon-trainable params: 0\nTotal mult-adds (G): 1.76\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 24.26\nParams size (MB): 1.25\nEstimated Total Size (MB): 25.56\n==========================================================================================\n\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf:int=32, nc:int=1, alpha=0.2):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf*2),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Conv2d(ndf*2, ndf*4, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf*4),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Conv2d(ndf*4, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = self.layers(x)\n        x = torch.reshape(x, (x.size(0), -1))\n        return x\n    \n\nsummary(Discriminator(), input_size=(128, 1, 28, 28))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nDiscriminator                            [128, 1]                  --\n├─Sequential: 1-1                        [128, 1, 1, 1]            --\n│    └─Conv2d: 2-1                       [128, 32, 14, 14]         512\n│    └─BatchNorm2d: 2-2                  [128, 32, 14, 14]         64\n│    └─LeakyReLU: 2-3                    [128, 32, 14, 14]         --\n│    └─Conv2d: 2-4                       [128, 64, 7, 7]           32,768\n│    └─BatchNorm2d: 2-5                  [128, 64, 7, 7]           128\n│    └─LeakyReLU: 2-6                    [128, 64, 7, 7]           --\n│    └─Conv2d: 2-7                       [128, 128, 4, 4]          73,728\n│    └─BatchNorm2d: 2-8                  [128, 128, 4, 4]          256\n│    └─LeakyReLU: 2-9                    [128, 128, 4, 4]          --\n│    └─Conv2d: 2-10                      [128, 1, 1, 1]            2,048\n│    └─Sigmoid: 2-11                     [128, 1, 1, 1]            --\n==========================================================================================\nTotal params: 109,504\nTrainable params: 109,504\nNon-trainable params: 0\nTotal mult-adds (M): 369.68\n==========================================================================================\nInput size (MB): 0.40\nForward/backward pass size (MB): 23.46\nParams size (MB): 0.44\nEstimated Total Size (MB): 24.30\n==========================================================================================\n\n\n\n\nEvaluation\n\nMNIST Digits Dataset\n\nIMG_DIM = (1, 28, 28)\nNOISE_DIM = 100\n\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.5], [0.5]),\n])\n\ndata = get_minst_dataset(transform)\ndataloader = DataLoader(data, **loader_kwargs)\n\ntorch.manual_seed(SEED)\n\n# benchmark_noise is used for the animation to show how output evolve on same vector\nbenchmark_noise = torch.randn(16*16, NOISE_DIM, device=device)\n\ngenerator = Generator(nz=NOISE_DIM, ngf=32, nc=IMG_DIM[0]).to(device)\ngenerator.apply(weights_init)\n\ndiscriminator = Discriminator(ndf=32, nc=IMG_DIM[0]).to(device)\ndiscriminator.apply(weights_init)\n\noptimizer_G = optim.AdamW(\n    generator.parameters(),\n    lr=LR,\n    betas=BETAS,\n    #weight_decay=1e-5,\n)\n\noptimizer_D = optim.AdamW(\n    discriminator.parameters(),\n    lr=LR,\n    betas=BETAS,\n    #weight_decay=1e-5,\n)\n\ncriterion = nn.BCELoss().to(device) # F.binary_cross_entropy_with_logits #nn.BCELoss().to(device)\n\n\nanimation = []\n\ng_losses, d_losses = [], []\nfor epoch in tqdm(range(N_EPOCHS), unit='epochs'):\n\n    generator.train()\n    discriminator.train()\n\n    for samples_real, _ in dataloader:\n        g_loss, d_loss = train_step(\n            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device\n        )\n\n        g_losses.append(g_loss)\n        d_losses.append(d_loss)\n\n    generator.eval()\n    with torch.inference_mode():\n        images = generator(benchmark_noise)\n        images = images.detach().cpu()\n    \n        images = make_grid(images, nrow=16, normalize=True)\n    \n        animation.append(images)\n\n100%|██████████| 100/100 [09:44&lt;00:00,  5.84s/epochs]\n\n\n\n\n\n\n\nGenerator and Discriminator loss evolution over epochs using DCGAN on the MNIST digit dataset.\n\n\n\n\n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\nMNIST Fashion Dataset\n\nIMG_DIM = (1, 28, 28)\nNOISE_DIM = 100\n\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.5], [0.5]),\n])\n\ndata = get_mnist_fashion_dataset(transform)\ndataloader = DataLoader(data, **loader_kwargs)\n\ntorch.manual_seed(SEED)\n\n# benchmark_noise is used for the animation to show how output evolve on same vector\nbenchmark_noise = torch.randn(16*16, NOISE_DIM, device=device)\n\ngenerator = Generator(nz=NOISE_DIM, ngf=32, nc=IMG_DIM[0]).to(device)\ngenerator.apply(weights_init)\n\ndiscriminator = Discriminator(ndf=32, nc=IMG_DIM[0]).to(device)\ndiscriminator.apply(weights_init)\n\noptimizer_G = optim.AdamW(\n    generator.parameters(),\n    lr=LR,\n    betas=BETAS,\n    #weight_decay=1e-4,\n)\n\noptimizer_D = optim.AdamW(\n    discriminator.parameters(),\n    lr=LR,\n    betas=BETAS,\n    #weight_decay=1e-4,\n)\n\ncriterion = nn.BCELoss().to(device) # F.binary_cross_entropy_with_logits #nn.BCELoss().to(device)\n\n\nanimation = []\n\ng_losses, d_losses = [], []\nfor epoch in tqdm(range(N_EPOCHS), unit='epochs'):\n\n    generator.train()\n    discriminator.train()\n\n    for samples_real, _ in dataloader:\n        g_loss, d_loss = train_step(\n            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device\n        )\n\n        g_losses.append(g_loss)\n        d_losses.append(d_loss)\n\n    generator.eval()\n    with torch.inference_mode():\n        images = generator(benchmark_noise)\n        images = images.detach().cpu()\n    \n        images = make_grid(images, nrow=16, normalize=True)\n    \n        animation.append(images)\n\n100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [10:09&lt;00:00,  6.09s/epochs]\n\n\n\n\n\n\n\nGenerator and Discriminator loss evolution over epochs using DCGAN on the MNIST fashion dataset.\n\n\n\n\n\n\n\n \n Your browser does not support the video tag."
  },
  {
    "objectID": "posts/simple-generative-advsersarial-networks/index.html#conclusion",
    "href": "posts/simple-generative-advsersarial-networks/index.html#conclusion",
    "title": "Generative Adversarial Networks",
    "section": "Conclusion",
    "text": "Conclusion\nWhile the equations lend the Vanilla GAN a formal framework, it’s essential to grasp the underlying concept: a perpetual duel between two entities striving to outwit one another. The mathematics merely outlines the game’s boundaries. As the Generator refines its craftsmanship, the Discriminator sharpens its discernment, leading to a dynamic balance. This iterative process encapsulates the beauty of GANs, where creativity meets critical evaluation, demonstrating the power and versatility of adversarial learning in AI."
  },
  {
    "objectID": "snippets.html",
    "href": "snippets.html",
    "title": "Code snippets",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Generative Adversarial Networks\n\n\n\n\n\n\nGregor Cerar\n\n\nOct 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Style Transfer\n\n\n\n\n\n\nGregor Cerar\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gregor Cerar, PhD",
    "section": "",
    "text": "Hi 🖐️! I’m Gregor (Greg for short). I’m currently a postdoc at SensorLab, Jozef Stefan Institute, and a data scientist at Comsensus. Most of my work revolves around AI applications in smart infrastructure and time series data. My main research interest is in applications of self-supervised learning in wireless communications.\nI’m an introvert who usually stays away from the spotlight, deeply enjoys the quiet corners of research and writing elegant code. I work mostly in Python. I enjoy hiking, playing chess and “Uno!” card game."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Generative Adversarial Networks\n\n\n5 min\n\n\n\n\n\n\nOct 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Style Transfer\n\n\n8 min\n\n\n\n\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/neural-style-transfer/index.html",
    "href": "posts/neural-style-transfer/index.html",
    "title": "Neural Style Transfer",
    "section": "",
    "text": "Neural Style Transfer (NST) is a deep learning method that combines the content of one image with the style of another, like giving your photo a Van Gogh-esque makeover.\nUsing convolutional neural networks, NST examines both images’ features and creates a new image that merges the content’s structure with the style’s attributes. This technique became a hit due to its novel outcomes, leading to its adoption in various apps and platforms, highlighting deep learning’s prowess in image transformation.\nIntroduced in 2015 by L. Gatys and team in “A Neural Algorithm of Artistic Style,” this method transfers art styles between images. This guide revisits and tweaks their approach, presenting several examples."
  },
  {
    "objectID": "posts/neural-style-transfer/index.html#prerequisites",
    "href": "posts/neural-style-transfer/index.html#prerequisites",
    "title": "Neural Style Transfer",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore we get started, we need to install NumPy, matplotlib, PyTorch deep learning framework, and finally, torchvision library.\n\nfrom pathlib import Path\nfrom typing import List, Union\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport torch\nfrom PIL import Image\nfrom torch import Tensor, nn, optim\nfrom torch.nn import functional as F\nfrom torchvision import models, transforms as T\nfrom torchvision.transforms import functional as VF\nfrom torchvision.utils import make_grid\n\nfrom tqdm import tqdm\n\n# Random seed for reproducability\nSEED = 42\n\n# Size of the output image\nIMG_SIZE = 512\n\nWhile neural networks are capable of running on CPUs, using a compute accelerator, like a GPU, is recommended for enhanced performance. For my experiments, I utilized the NVIDIA RTX 3090. Speed gains can be further achieved by harnessing tensor cores and employing the bfloat16 data type found in the Ampere architecture.\n\nAMP_ENABLED = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    # Allow CuDNN internal benchmarking for architecture-specific optimizations\n    torch.backends.cudnn.benchmark = True\n\n    if torch.cuda.is_bf16_supported():\n        AMP_ENABLED = True\n        torch.set_float32_matmul_precision('medium')"
  },
  {
    "objectID": "posts/neural-style-transfer/index.html#implementation",
    "href": "posts/neural-style-transfer/index.html#implementation",
    "title": "Neural Style Transfer",
    "section": "Implementation",
    "text": "Implementation\n\n\n\nFigure 1: The Neural Style Transfer framework introduced by Gatys et al. distinguishes style and content features from designated layers.\n\n\nIn this section, we delve into the implementation of Neural Style Transfer. The steps are as follows:\n\nPrepare the content, style, and target images.\nInitialize with a pre-trained VGG neural network, keeping its weights fixed.\nIntroduce three unique loss metrics.\nAdjust the neural network to extract features during forward-backward passes, applying gradient modifications to the target image.\nIterate through this process.\n\n\n# Weights for different features (Were these used by Gatys et al.?)\nSTYLE_LAYERS_DEFAULT = {\n    \"conv1_1\": 0.75,\n    \"conv2_1\": 0.5,\n    \"conv3_1\": 0.2,\n    \"conv4_1\": 0.2,\n    \"conv5_1\": 0.2,\n}\n\nCONTENT_LAYERS_DEFAULT = (\"conv5_2\",)\n\nCONTENT_WEIGHT = 8  # \"alpha\" in the literature (default: 8)\nSTYLE_WEIGHT = 70  # \"beta\" in the literature (default: 70)\nTV_WEIGHT = 10  # \"gamma\" in the literature (default: 10)\n\n\nLEARNING_RATE = 0.004\nN_EPOCHS = 7_000\n\n\nLoss metrics\nTo effectively implement Neural Style Transfer, we need to quantify how well the generated image matches both the content and style of our source images. This is done using loss metrics. Let’s delve into the specifics of these metrics and how they drive the NST process.\n\nContent loss metric\nContent loss is calculated by Euclidean distance (i.e., mean squared error) between the respective intermediate higher-level feature representation \\(F^l\\) and \\(P^l\\) of original input image \\(\\vec{x}\\) and the content image \\(\\vec{p}\\) at layer \\(l\\).\nHence a given input image \\(\\vec{x}\\) is encoded in each layer of the CNN by the filter responses to that image. A layer with \\(N_l\\) distinct filters has \\(N_l\\) feature maps of size \\(M_l\\), where \\(M_l\\) is the height times width of the feature map. So the response in a layer \\(l\\) can be stored in a matrix \\(F^l \\in \\mathcal{R}^{N_l \\times M_l}\\) where \\(F_{ij}^{l}\\) is the activation of the \\(i^{th}\\) filter at position \\(j\\) in layer \\(l\\).\n\\[\n\\mathcal{L}_{content}(\\vec{p}, \\vec{x}, l) = \\frac{1}{2}\\sum_{i,j} (F^{l}_{ij} - P^{l}_{ij})^2\n\\]\n\ndef content_loss_func(target_features, content_features):\n    \"\"\"Calculate content loss metric for give layers.\"\"\"\n\n    content_loss = 0.0\n    for layer in content_features:\n        target_feature = target_features[layer]\n        content_feature = content_features[layer]\n\n        content_layer_loss = F.mse_loss(target_feature, content_feature)\n        content_loss += content_layer_loss\n\n    return content_loss\n\n\n\nStyle loss\nThe style loss is a bit more involved than the content loss. For Neural Style Transfer, it’s commonly computed by comparing the Gram matrices of the feature maps from the style image and the generated image.\nFirst, let’s understand the Gram matrix. Given the feature map \\(F\\) of size \\(C \\times (H \\times W)\\), where \\(C\\) is the number of channels and \\(H \\times W\\) are the spatial dimensions, the Gram matrix \\(G\\) is of size \\(C \\times C\\) and is computed as:\n\\[\nG^l_{ij} = \\sum_k F^l_{ik} F^l_{jk}\n\\]\nwhere \\(G_{ij}\\) is the inner product between vectorized feature maps \\(i\\) and \\(j\\). This results in a matrix that captures the correlation between different feature maps, and thus, the style of information.\n\ndef gram_matrix(input: Tensor) -&gt; Tensor:\n    (b, c, h, w) = input.size()\n\n    # reshape into (C x (H x W))\n    features = input.view(b * c, h * w)\n\n    # compute the gram product\n    gram = torch.mm(features, features.t())\n\n    return gram\n\nThe style loss between the Gram matrix of the generated image \\(G\\) and that of style image \\(A\\) (at specific layer) is:\n\\[\nE_l = \\frac{1}{4 N^{2}_{l} M^{2}_{l}} \\sum_{i,j}(G^l_{ij} - A^l_{ij})^2\n\\]\nWhere \\(E_l\\) is the style loss for layer \\(l\\), \\(N_l\\) and \\(M_l\\) are the numbers of channels and height times width in the feature representation of layer \\(l\\) respectively. \\(G_{ij}^l\\) and \\(A_{ij}^l\\) are the gram matrices of the intermediate representation of the style image \\(\\vec{a}\\) and the input base image \\(\\vec{x}\\) respectively.\nThe total style loss is:\n\\[\n\\mathcal{L}_{style}(\\vec{a}, \\vec{x}) = \\sum_{l=0}^{L} w_l E_l\n\\]\n\ndef style_loss_func(target_features, style_features, precomputed_style_grams):\n    style_loss = 0.0\n    for layer in style_features:\n        target_feature = target_features[layer]\n        target_gram = gram_matrix(target_feature)\n\n        style_gram = precomputed_style_grams[layer]\n\n        _, c, h, w = target_feature.shape\n\n        weight = STYLE_LAYERS_DEFAULT[layer]\n        layer_style_loss = weight * F.mse_loss(target_gram, style_gram) / (c * h * w)\n        style_loss += layer_style_loss\n\n    return style_loss\n\n\n\nTotal Variation Loss\nTotal Variation (TV) loss, also known as Total Variation Regularization, is commonly added to the Neural Style Transfer objective to encourage spatial smoothness in the generated image. Without it, the output might exhibit noise or oscillations, particularly in regions where the content and style objectives don’t offer much guidance.\nGiven an image \\(\\vec{x}\\) of size \\(H \\times W \\times C\\) (height, width, channels), the Total Variation loss is defined as the sum of the absolute differences between neighboring pixel values:\n\\[\n\\mathcal{L}_{TV}(\\vec{x}) = \\sum_{i,j} ((x_{i,j+1} - x_{i,j})^2 + (x_{i+1,j} - x_{i,j})^2)\n\\]\nwhere \\(x_{i,j}\\) is the pixel value at position \\((i,j)\\).\nIn simple terms, this loss penalizes abrupt changes in pixel values from one to its neighbors. By minimizing this loss, the generated image becomes smoother, reducing artifacts and unwanted noise. When combined with content and style losses, the TV loss ensures that the resulting image not only captures the content and style of the source images but also looks visually coherent and smooth.\n\ndef total_variance_loss_func(target: Tensor):\n    tv_loss = F.l1_loss(target[:, :, :, :-1], target[:, :, :, 1:]) \\\n            + F.l1_loss(target[:, :, :-1, :], target[:, :, 1:, :])\n\n    return tv_loss\n\n\n\nTotal Loss\nThe total loss is a combination of multiple loss components, each of which targets a specific aspect of the image generation process. The primary components typically are:\n\nContent Loss: Ensures the generated image resembles the content of the content image.\nStyle Loss: Ensures the generated image captures the stylistic features of the style image.\nTotal Variation Loss: Encourages spatial smoothness in the generated image, reducing artifacts and noise.\n\nGiven the above components, the total loss \\(\\mathcal{L}_{total}\\) for Neural Style Transfer can be formulated as:\n\\[\n\\mathcal{L}_{total}(\\vec{p},\\vec{a},\\vec{x}) = \\alpha\\mathcal{L}_{content}(\\vec{p},\\vec{x}) + \\beta\\mathcal{L}_{style}(\\vec{a},\\vec{x}) + \\gamma\\mathcal{L}_{TV}(\\vec{x})\n\\]\n\\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\) are weight factors that determine the relative importance of the content, style, and the total variation losses, respectively. By adjusting these weights, one can control the balance between content preservation, style transfer intensity, and the smoothness of the generated image. The NST algorithm aims to adjust the generated image to minimize this total loss.\n\n\n\nInput preparation\nHere we specify path to content and style images:\n\ncontent_path = \"./bridge.jpg\"\nstyle_path = \"./colorful-whirlpool.jpg\"\n\n\n\nNeural Style Transfer Process\nFor feature extraction, we’ll leverage VGG19, pre-trained on ImageNet. It’s essential to note that we set the model to evaluation and no_grad mode. This ensures we only use VGG19 for extracting features without altering its weights. For optimal performance, we also transfer the neural network (NN) to a chosen device, ideally a GPU.\nAn intriguing choice by Gatys et al. was to modify VGG-19, replacing max pooling with average pooling, aiming for visually superior results. However, a challenge arises: our NN was originally trained with MaxPool2d layers. Substituting them can affect activations due to reduced output values. To counteract this, we’ve introduced a custom ScaledAvgPool2d.\n\n# We will use frozen pretrained VGG neural network for feature extraction\n# In the original paper, authors have used VGG19 (without batch normalization)\nmodel = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n\n# Authors in the original paper suggested use of AvgPool instead of MaxPool for more pleasing \n# results. However changing the pooling also affects activation, so the input needs to be\n# scaled (can't find the original source).\nclass ScaledAvgPool2d(torch.nn.Module):\n    def __init__(self, kernel_size, stride, padding=0, scale_factor=2.0):\n        super().__init__()\n        self.avgpool = torch.nn.AvgPool2d(kernel_size, stride, padding)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        return self.avgpool(x) * self.scale_factor\n\n# Replace max pooling layers with custom avg pooling layers\n#for i, layer in enumerate(model):\n#   if isinstance(layer, torch.nn.MaxPool2d):\n#       model[i] = ScaledAvgPool2d(kernel_size=2, stride=2, padding=0)\n\nmodel = model.eval().requires_grad_(False).to(device)\n\nThe pretrained VGG model used normalized ImageNet samples for optimal performance. For effective style transfer, we’ll follow suit to improve feature extraction. Though images may appear altered post-normalization, they’ll be reverted to their original state after NST. Next, we’ll transform the content and style images by:\n\nLoading them from storage.\nResizing while maintaining aspect ratio.\nConverting to tensors.\nNormalizing using ImageNet weights.\n\n\n# ImageNet normalization weights\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\ntransform = T.Compose([\n    # Smaller edge of the image will be matched to `IMG_SIZE`\n    T.Resize(IMG_SIZE),\n    T.ToTensor(),\n    T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n])\n\ndef load_image(path: Union[str, Path]) -&gt; Tensor:\n    image = Image.open(path).convert(\"RGB\")\n    image = transform(image).unsqueeze(0)\n    return image\n\nThe following code will prepares content \\(\\vec{p}\\), style \\(\\vec{a}\\), and target \\(\\vec{x}\\) images. The target image is a clone of the content image and we enable computation of gradients on it.\n\n# The \"style\" image from which we obtain style\nstyle = load_image(style_path).to(device)\n\n# The \"content\" image on which we apply style\ncontent = load_image(content_path).to(device)\n\n# The \"target\" image to store outcome\ntarget = content.clone().requires_grad_(True).to(device)\n#target = torch.rand_like(content).requires_grad_(True).to(device)\n\nThe function below retrieves feature maps from designated layers. As shown in Figure Figure 1:\n\nContent feature map comes from relu5_2.\nStyle feature maps are sourced from relu1_1, relu2_1, relu3_1, relu4_1, and relu5_1.\n\n\ndef get_features(image: Tensor, model: nn.Module, layers=None):\n    if layers is None:\n        layers = tuple(STYLE_LAYERS_DEFAULT) + CONTENT_LAYERS_DEFAULT\n\n    features = {}\n    block_num = 1\n    conv_num = 0\n\n    x = image\n\n    for layer in model:\n        x = layer(x)\n\n        if isinstance(layer, nn.Conv2d):\n            # produce layer name to find matching convolutions from the paper\n            # and store their output for further processing.\n            conv_num += 1\n            name = f\"conv{block_num}_{conv_num}\"\n            if name in layers:\n                features[name] = x\n\n        elif isinstance(layer, (nn.MaxPool2d, nn.AvgPool2d, ScaledAvgPool2d)):\n            # In VGG, each block ends with max/avg pooling layer.\n            block_num += 1\n            conv_num = 0\n\n        elif isinstance(layer, (nn.BatchNorm2d, nn.ReLU)):\n            pass\n\n        else:\n            raise Exception(f\"Unknown layer: {layer}\")\n\n    return features\n\nSince content and style image never change, we can precompute their feature maps and grams to speed up the NST process.\n\n# Precompute content features, style features, and style gram matrices.\ncontent_features = get_features(content, model, CONTENT_LAYERS_DEFAULT)\nstyle_features = get_features(style, model, STYLE_LAYERS_DEFAULT)\n\nstyle_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n\nNext, we will use Adam optimizer, where we specify that only target image (\\(\\vec{x}\\)) is considered for optimization.\n\noptimizer = optim.Adam([target], lr=LEARNING_RATE)\n\nThe final step of NST is to gradually transfer style using everything that we’ve learned in this blog post. We extract feature maps, compute total loss, perform step using gradient descent, and repeat the process N_EPOCHS times. Gradient changes will apply only on target image.\nTo notably enhance NST speed, I utilized mixed precision with the unique bfloat16, found in newer hardware. Traditional half-precision float16 doesn’t yield the same results. I’ve tested it.\n\npbar = tqdm(range(N_EPOCHS))\n\nfor step in pbar:\n    optimizer.zero_grad(set_to_none=True)\n\n    with torch.autocast('cuda', dtype=torch.bfloat16, enabled=AMP_ENABLED):\n        target_features = get_features(target, model)\n\n        content_loss = CONTENT_WEIGHT * content_loss_func(target_features, content_features)\n        style_loss = STYLE_WEIGHT * style_loss_func(target_features, style_features, style_grams)\n        tv_loss = TV_WEIGHT * total_variance_loss_func(target)\n\n        total_loss = content_loss + style_loss + tv_loss\n\n    total_loss.backward(retain_graph=True)  # do we need `retain_graph=True`?\n\n    optimizer.step()\n\n    pbar.set_postfix_str(\n        f\"total_loss={total_loss.item():.2f} \"\n        f\"content_loss={content_loss.item():.2f} \"\n        f\"style_loss={style_loss.item():.2f} \"\n        f\"tv_loss={tv_loss.item():.2f} \"\n    )\n\n100%|██████████| 7000/7000 [02:04&lt;00:00, 56.40it/s, total_loss=31.13 content_loss=5.47 style_loss=20.16 tv_loss=5.50 ]     \n\n\nAs mentioned before, images need to be denormalized to correct colors. After that we compare content, style and target images side-by-side.\n\nclass InverseNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, x: Tensor) -&gt; Tensor:\n        for t, m, s in zip(x, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return x\n\n\nclass Clip(object):\n    def __init__(self, vmin: float = 0.0, vmax: float = 1.0):\n        self.vmin = vmin\n        self.vmax = vmax\n\n    def __call__(self, x):\n        return torch.clamp(x, self.vmin, self.vmax)\n\n\ninv_transform_preview = T.Compose([\n    InverseNormalize(IMAGENET_MEAN, IMAGENET_STD),\n    T.Resize(IMG_SIZE, antialias=True),\n    T.CenterCrop((IMG_SIZE, IMG_SIZE)),\n    Clip(),\n])\n\nimgs = [\n    inv_transform_preview(i.detach().squeeze().cpu())\n    for i in (content, style, target)\n]\n\ngrid = make_grid(imgs)\n\ndef show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n\n    _, axs = plt.subplots(ncols=len(imgs), figsize=(21, 7), squeeze=False, dpi=92, tight_layout=True)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = VF.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\n\nshow(grid)"
  },
  {
    "objectID": "posts/neural-style-transfer/index.html#conclusions",
    "href": "posts/neural-style-transfer/index.html#conclusions",
    "title": "Neural Style Transfer",
    "section": "Conclusions",
    "text": "Conclusions\nNeural Style Transfer (NST) beautifully merges art and tech, allowing one image’s content to adopt the style of another. Through our NST exploration:\n\nWe utilized VGG19 for its prowess in feature extraction.\nWe learned the value of average pooling and the need to scale activations.\nIterative processing let us transfer style, with the use of bfloat16 significantly speeding things up.\n\nWhile NST shines in art, its core ideas have broader uses, from design to augmented reality. This post is a primer, but there’s a vast horizon to delve into. Dive in and enjoy the art of style transfer!"
  },
  {
    "objectID": "posts/neural-style-transfer/index.html#acknowledgements",
    "href": "posts/neural-style-transfer/index.html#acknowledgements",
    "title": "Neural Style Transfer",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI’d like to acknowledge the following artworks:\n\n“Gray Bridge and Trees” by Martin Damboldt\n“Walking in the Rain” by Leonid Afremov\n“The Starry Night” by Vincent van Gogh\n\nFor a complete list of acknowledgements, please visit my GitHub repository:\n\ngcerar/pytorch-neural-style-transfer"
  },
  {
    "objectID": "posts/neural-style-transfer/index.html#appendix",
    "href": "posts/neural-style-transfer/index.html#appendix",
    "title": "Neural Style Transfer",
    "section": "Appendix",
    "text": "Appendix\nA few cherry-picked examples of style transfer:"
  }
]