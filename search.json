[
  {
    "objectID": "posts/2023-10-10-vanilla-GANs/index.html",
    "href": "posts/2023-10-10-vanilla-GANs/index.html",
    "title": "Generative Adversarial Networks",
    "section": "",
    "text": "Generative Adversarial Networks (GANs) are an innovative class of unsupervised neural networks that have revolutionized the field of artificial intelligence. They were first introduced in Generative Adversarial Networks (Goodfellow et al. 2014) and consist of two separate neural networks: the generator (creates data) and the discriminator (evaluates data authenticity). The generator aims to fool the discriminator by producing realistic data, while the discriminator tries to differentiate real from fake. Over iterations, the generator’s data becomes more convincing.\nAs an analogy, consider two kids, one drawing counterfeit money (“Generator”) and another assessing its realism (“Discriminator”). Over time, the counterfeit drawings become increasingly convincing."
  },
  {
    "objectID": "posts/2023-10-10-vanilla-GANs/index.html#objective-function",
    "href": "posts/2023-10-10-vanilla-GANs/index.html#objective-function",
    "title": "Generative Adversarial Networks",
    "section": "Objective Function",
    "text": "Objective Function\nThe interaction between the Generator and the Discriminator can be quantified by their objective or loss functions:\n\nDiscriminator’s Objective: For real data \\(x\\), \\(D\\) wants \\(D(x)\\) near \\(1\\). For generated data \\(G(z)\\), it targets \\(D(G(z))\\) close to \\(0\\). Its objective is:\n\n\\[\n\\mathcal{L}(D) = \\log(D(x)) + \\log(1 - D(G(z))).\n\\]\n\nGenerator’s Objective: \\(G\\) aims for \\(D(G(z))\\) to approach \\(1\\), given by:\n\n\\[\n\\mathcal{L}(G) = \\log⁡(1 − D(G(z)))\n\\]\nBoth \\(G\\) and \\(D\\) continuously improve to outperform each other in this game.\n\nMinimax Game in GANs\nVanilla GANs are structured around the minimax game from game theory:\n\\[\n\\min_{G}\\max_{D} \\mathcal{L}(D, G) = \\log(D(x)) + \\log(1 - D(G(z)))\n\\]\nIn essence:\n\nDiscriminator: Maximizes its capacity to differentiate real data from generated.\nGenerator: Minimizes the discriminator’s success rate by producing superior forgeries.\n\nThe iterative competition refines both, targeting a proficient Generator and a perceptive Discriminator."
  },
  {
    "objectID": "posts/2023-10-10-vanilla-GANs/index.html#prepare-environment",
    "href": "posts/2023-10-10-vanilla-GANs/index.html#prepare-environment",
    "title": "Generative Adversarial Networks",
    "section": "Prepare Environment",
    "text": "Prepare Environment\nIn the upcoming sections, we’ll do the following steps to prepare the development environment:\n\nImport necessary libraries, primarily PyTorch and Matplotlib.\nDefine constants, including project path and seed, for consistency.\nDetermine the computational device (e.g., GPU).\nProvide a weight initialization helper function.\n\n\nfrom typing import List, Tuple, Callable, Sequence\nfrom pathlib import Path\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n%config InlineBackend.figure_formats = {'retina', 'png'}\n\nimport torch\nfrom torch import Tensor, nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import ConcatDataset, DataLoader, Dataset\nfrom torchvision import transforms as T\nfrom torchvision.utils import make_grid\n\nfrom tqdm import tqdm\n\nfrom torchinfo import summary\n\nSEED = 42\n\nPROJECT_PATH = Path('.').resolve()\nFIGURE_PATH = PROJECT_PATH / 'figures'\nDATASET_PATH = Path.home() / 'datasets'\n\n\n# Disable functionalities for speed-up\ntorch.autograd.set_detect_anomaly(False)\ntorch.autograd.profiler.profile(False)\ntorch.autograd.profiler.emit_nvtx(False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    # Allow CuDNN internal benchmarking for architecture-specific optimizations\n    torch.backends.cudnn.benchmark = True\n\n\ndef weights_init(net: nn.Module) -&gt; None:\n    for m in net.modules():\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n            nn.init.normal_(m.weight, 0.0, 0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)\n\n        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n            nn.init.normal_(m.weight, 1.0, 0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)\n\n        elif isinstance(m, nn.Linear):\n            nn.init.normal_(m.weight, 0, 0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)\n\n\nGenerator\nThe Generator in GANs acts as an artist, crafting data.\n\nInput: Takes random noise, typically from a standard normal distribution.\nArchitecture: Uses dense layers, progressively increasing data dimensions.\nOutput: Reshapes data to desired format (e.g., image). Often uses ‘tanh’ for activation.\nObjective: Generate data indistinguishable from real by the Discriminator.\n\n\nclass Generator(nn.Module):\n    def __init__(self, out_dim:Sequence[int], nz:int=100, ngf:int=256, alpha:float=0.2):\n        \"\"\"\n        :param out_dim: output image dimension / shape\n        :param nz: size of the latent z vector $z$\n        :param ngf: size of feature maps (units in the hidden layers) in the generator\n        :param alpha: negative slope of leaky ReLU activation\n        \"\"\"\n        super().__init__()\n        self.out_dim = out_dim\n        self.model = nn.Sequential(\n            nn.Linear(nz, ngf),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Linear(ngf, ngf*2),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Linear(ngf*2, ngf*4),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Linear(ngf*4, int(np.prod(self.out_dim))),\n            nn.Tanh(),\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = self.model(x)\n        x = torch.reshape(x, (x.size(0), *self.out_dim))\n        return x\n\nsummary(Generator(out_dim=(1, 28, 28)), input_size=[128, 100])\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nGenerator                                [128, 1, 28, 28]          --\n├─Sequential: 1-1                        [128, 784]                --\n│    └─Linear: 2-1                       [128, 256]                25,856\n│    └─LeakyReLU: 2-2                    [128, 256]                --\n│    └─Linear: 2-3                       [128, 512]                131,584\n│    └─LeakyReLU: 2-4                    [128, 512]                --\n│    └─Linear: 2-5                       [128, 1024]               525,312\n│    └─LeakyReLU: 2-6                    [128, 1024]               --\n│    └─Linear: 2-7                       [128, 784]                803,600\n│    └─Tanh: 2-8                         [128, 784]                --\n==========================================================================================\nTotal params: 1,486,352\nTrainable params: 1,486,352\nNon-trainable params: 0\nTotal mult-adds (M): 190.25\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 2.64\nParams size (MB): 5.95\nEstimated Total Size (MB): 8.63\n==========================================================================================\n\n\n\n\nDiscriminator\nThe Discriminator is GAN’s evaluator, distinguishing real from fake data.\n\nInput: Takes either real data samples or those from the Generator.\nArchitecture: Employs dense layers for binary classification of the input.\nOutput: Uses a sigmoid activation, yielding a score between 0-1, reflecting the data’s authenticity.\nObjective: Recognize real data and identify fake data from the Generator.\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_dim:Sequence[int], ndf:int=128, alpha:float=0.2):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(input_dim)), ndf*4),\n            nn.LeakyReLU(alpha, inplace=True),\n            nn.Dropout(0.3),\n\n            nn.Linear(ndf*4, ndf*2),\n            nn.LeakyReLU(alpha, inplace=True),\n            nn.Dropout(0.3),\n\n            nn.Linear(ndf*2, ndf),\n            nn.LeakyReLU(alpha, inplace=True),\n            nn.Dropout(0.3),\n\n            nn.Linear(ndf, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = torch.reshape(x, (x.size(0), -1))\n        return self.model(x)\n\nsummary(Discriminator(input_dim=(1, 28, 28)), input_size=[128, 1, 28, 28])\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nDiscriminator                            [128, 1]                  --\n├─Sequential: 1-1                        [128, 1]                  --\n│    └─Linear: 2-1                       [128, 512]                401,920\n│    └─LeakyReLU: 2-2                    [128, 512]                --\n│    └─Dropout: 2-3                      [128, 512]                --\n│    └─Linear: 2-4                       [128, 256]                131,328\n│    └─LeakyReLU: 2-5                    [128, 256]                --\n│    └─Dropout: 2-6                      [128, 256]                --\n│    └─Linear: 2-7                       [128, 128]                32,896\n│    └─LeakyReLU: 2-8                    [128, 128]                --\n│    └─Dropout: 2-9                      [128, 128]                --\n│    └─Linear: 2-10                      [128, 1]                  129\n│    └─Sigmoid: 2-11                     [128, 1]                  --\n==========================================================================================\nTotal params: 566,273\nTrainable params: 566,273\nNon-trainable params: 0\nTotal mult-adds (M): 72.48\n==========================================================================================\nInput size (MB): 0.40\nForward/backward pass size (MB): 0.92\nParams size (MB): 2.27\nEstimated Total Size (MB): 3.59\n=========================================================================================="
  },
  {
    "objectID": "posts/2023-10-10-vanilla-GANs/index.html#traning-loop",
    "href": "posts/2023-10-10-vanilla-GANs/index.html#traning-loop",
    "title": "Generative Adversarial Networks",
    "section": "Traning Loop",
    "text": "Traning Loop\nThe training process is iterative:\n\nUpdate Discriminator: With the Generator static, improve the Discriminator’s detection of real vs. fake.\nUpdate Generator: With a static Discriminator, enhance the Generator’s ability to deceive.\n\nTraining continues until the Generator produces almost authentic data. Equilibrium is reached when the Discriminator sees every input as equally likely real or fake, assigning a probability of \\(\\frac{1}{2}\\).\n\n\n\n\n\n\nNote\n\n\n\nUsing .eval() and .train() modes initially seemed promising for faster training. However, they affected layers like BatchNorm2d and Dropout, making the GAN diverge. Also, switching between eval and train modes is not free of charge.\n\n\n\ndef train_step(\n    generator: nn.Module,\n    discriminator: nn.Module,\n    optim_G: optim.Optimizer,\n    optim_D: optim.Optimizer,\n    criterion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    real_data: torch.Tensor,\n    noise_dim: int,\n    device: torch.device,\n) -&gt; tuple[float, float]:\n    batch_size = real_data.size(0)\n    \n    real_data = real_data.to(device)\n    \n    # Labels for real and fake data\n    real_labels = torch.ones(batch_size, 1, device=device)\n    fake_labels = torch.zeros(batch_size, 1, device=device)\n    \n    ### Train Discriminator\n\n    # Real data\n    output_real = discriminator(real_data)\n    loss_D_real = criterion(output_real, real_labels)\n    \n    # Fake data\n    noise = torch.randn(batch_size, noise_dim, dtype=torch.float, device=device)\n    fake_data = generator(noise)\n    output_fake = discriminator(fake_data.detach())\n    loss_D_fake = criterion(output_fake, fake_labels)\n    \n    # Backprop and optimize for discriminator\n    loss_D = (loss_D_real + loss_D_fake) / 2.0\n    discriminator.zero_grad()\n    loss_D.backward()\n    optim_D.step()\n    \n    ### Train Generator\n\n    # Recompute fake data’s discriminator scores\n    output_fake = discriminator(fake_data)\n    loss_G = criterion(output_fake, real_labels)\n    \n    # Backprop and optimize for generator\n    generator.zero_grad()\n    loss_G.backward()\n    optim_G.step()\n    \n    return loss_G.item(), loss_D.item()"
  },
  {
    "objectID": "posts/2023-10-10-vanilla-GANs/index.html#evaluation",
    "href": "posts/2023-10-10-vanilla-GANs/index.html#evaluation",
    "title": "Generative Adversarial Networks",
    "section": "Evaluation",
    "text": "Evaluation\nBefore evaluation, we configured the learning rate (LR), optimizer’s \\(\\beta\\) parameters, batch size, and data loader settings for all experiments. We used the MNIST digits and MNIST fashion datasets for assessment.\n\nOPTIMIZER_LR = 0.0002\nL2_NORM = 1e-5\nOPTIMIZER_BETAS = (0.5, 0.999)\nN_EPOCHS = 100\nBATCH_SIZE = 128\n\n\nloader_kwargs = {\n    'num_workers': 8,\n    'pin_memory': True,\n    'shuffle': True,\n    'batch_size': BATCH_SIZE,\n    'prefetch_factor': 16,\n    'pin_memory_device': device.type,\n    'persistent_workers': False,\n}\n\n\nMNIST Digits Dataset\nThe MNIST (Modified National Institute of Standards and Technology) dataset is a well-known collection of handwritten digits, extensively used in the fields of machine learning and computer vision for training and testing purposes. Its simplicity and size make it a popular choice for introductory courses and experiments in image recognition.\nIn total, the dataset contains 70,000 grayscale images of handwritten digits (from 0 to 9). Each image is 28x28 pixels.\n\ndef get_minst_dataset(transform=None) -&gt; Dataset:\n    from torchvision.datasets import MNIST\n    root = str(DATASET_PATH)\n    trainset = MNIST(root=root, train=True, download=True, transform=transform)\n    testset = MNIST(root=root, train=False, download=True, transform=transform)\n    # Combine train and test dataset for more samples.\n    dataset = ConcatDataset([trainset, testset])\n    return dataset\n\n\nIMG_DIM = (1, 28, 28)\nNOISE_DIM = 100\n\n\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize(0.5, 0.5)\n])\n\ndata = get_minst_dataset(transform=transform)\ndataloader = DataLoader(data, **loader_kwargs)\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# benchmark_noise is used for the animation to show how output evolve on the same vector\nbenchmark_noise = torch.randn(16*16, NOISE_DIM, device=device)\n\ngenerator = Generator(out_dim=IMG_DIM, nz=NOISE_DIM).to(device)\ngenerator.apply(weights_init)\n\ndiscriminator = Discriminator(input_dim=IMG_DIM).to(device)\ndiscriminator.apply(weights_init)\n\noptimizer_G = optim.AdamW(\n    generator.parameters(),\n    lr=OPTIMIZER_LR,\n    betas=OPTIMIZER_BETAS,\n    weight_decay=L2_NORM,\n)\n\noptimizer_D = optim.AdamW(\n    discriminator.parameters(),\n    lr=OPTIMIZER_LR,\n    betas=OPTIMIZER_BETAS,\n    weight_decay=L2_NORM,\n)\n\ncriterion = nn.BCELoss().to(device)\n\n\nanimation = []\n\ng_losses, d_losses = [], []\nfor _ in tqdm(range(N_EPOCHS), unit='epochs'):\n    generator.train()\n    discriminator.train()\n\n    for samples_real, _ in dataloader:\n        g_loss, d_loss = train_step(\n            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device\n        )\n\n        g_losses.append(g_loss)\n        d_losses.append(d_loss)\n\n    generator.eval()\n    with torch.inference_mode():\n        images = generator(benchmark_noise)\n        images = images.detach().cpu()\n\n        images = make_grid(images, nrow=16, normalize=True)\n        animation.append(images)\n\n100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:43&lt;00:00,  2.23s/epochs]\n\n\n\n\n\n\n\n\nGenerator and Discriminator loss evolution over epochs using Vanilla GAN on the MNIST digit dataset.\n\n\n\n\n\n\nMNIST Fashion Dataset\nThe Fashion MNIST dataset is a collection of grayscale images of 10 different categories of clothing items, designed as a more challenging alternative to the classic MNIST dataset of handwritten digits. Each image in the dataset is 28x28 pixels. The 10 categories include items like t-shirts/tops, trousers, pullovers, dresses, coats, sandals, and more. With 70,000 images, Fashion MNIST is commonly used for benchmarking machine learning algorithms, especially in image classification tasks.\n\nIMG_DIM = (1, 28, 28)\nNOISE_DIM = 100\n\n\ndef get_mnist_fashion_dataset(transform=None):\n    from torchvision.datasets import FashionMNIST\n    root = str(DATASET_PATH)\n    trainset = FashionMNIST(root=root, train=True, download=True, transform=transform)\n    testset = FashionMNIST(root=root, train=False, download=True, transform=transform)\n    # Combine train and test dataset for more samples.\n    dataset = ConcatDataset([trainset, testset])\n    return dataset\n\n\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize(0.5, 0.5)\n])\n\ndata = get_mnist_fashion_dataset(transform=transform)\ndataloader = DataLoader(data, **loader_kwargs)\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# benchmark_noise is used for the animation to show how output evolve on same vector\nbenchmark_noise = torch.randn(16*16, NOISE_DIM, device=device)\n\ngenerator = Generator(out_dim=IMG_DIM, nz=NOISE_DIM).to(device)\ngenerator.apply(weights_init)\n\ndiscriminator = Discriminator(input_dim=IMG_DIM).to(device)\ndiscriminator.apply(weights_init)\n\noptimizer_G = optim.AdamW(\n    generator.parameters(),\n    lr=OPTIMIZER_LR,\n    betas=OPTIMIZER_BETAS,\n    weight_decay=L2_NORM,\n)\n\noptimizer_D = optim.AdamW(\n    discriminator.parameters(),\n    lr=OPTIMIZER_LR,\n    betas=OPTIMIZER_BETAS,\n    weight_decay=L2_NORM,\n)\n\ncriterion = nn.BCELoss().to(device)\n\n\nanimation = []\n\ng_losses, d_losses = [], []\nfor _ in tqdm(range(N_EPOCHS), unit='epochs'):\n\n    generator.train()\n    discriminator.train()\n\n    for samples_real, _ in dataloader:\n        g_loss, d_loss = train_step(\n            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device\n        )\n\n        g_losses.append(g_loss)\n        d_losses.append(d_loss)\n\n    generator.eval()\n    with torch.inference_mode():\n        images = generator(benchmark_noise)\n        images = images.detach().cpu()\n\n        images = make_grid(images, nrow=16, normalize=True)\n\n        animation.append(images)\n\n100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:49&lt;00:00,  2.30s/epochs]\n\n\n\n\n\n\n\n\nGenerator and Discriminator loss evolution over epochs using Vanilla GAN on the MNIST fashion dataset."
  },
  {
    "objectID": "posts/2023-10-10-vanilla-GANs/index.html#setting-up-dcgans",
    "href": "posts/2023-10-10-vanilla-GANs/index.html#setting-up-dcgans",
    "title": "Generative Adversarial Networks",
    "section": "Setting Up DCGANs",
    "text": "Setting Up DCGANs\n\nGenerator\n\nclass Generator(nn.Module):\n    def __init__(self, nz:int=100, ngf:int=32, nc:int=1):\n        \"\"\"\n        :param nz: size of the latent z vector\n        :param ngf: size of feature maps in generator\n        :param nc: number of channels in the training images.\n        \"\"\"\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf*4, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf*4),\n            nn.ReLU(inplace=True),\n\n            nn.ConvTranspose2d(ngf*4, ngf*2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf*2),\n            nn.ReLU(inplace=True),\n\n            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(inplace=True),\n\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = torch.reshape(x, (x.size(0), -1, 1, 1))\n        return self.layers(x)\n\nsummary(Generator(), input_size=(128, 100))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nGenerator                                [128, 1, 28, 28]          --\n├─Sequential: 1-1                        [128, 1, 28, 28]          --\n│    └─ConvTranspose2d: 2-1              [128, 128, 4, 4]          204,800\n│    └─BatchNorm2d: 2-2                  [128, 128, 4, 4]          256\n│    └─ReLU: 2-3                         [128, 128, 4, 4]          --\n│    └─ConvTranspose2d: 2-4              [128, 64, 7, 7]           73,728\n│    └─BatchNorm2d: 2-5                  [128, 64, 7, 7]           128\n│    └─ReLU: 2-6                         [128, 64, 7, 7]           --\n│    └─ConvTranspose2d: 2-7              [128, 32, 14, 14]         32,768\n│    └─BatchNorm2d: 2-8                  [128, 32, 14, 14]         64\n│    └─ReLU: 2-9                         [128, 32, 14, 14]         --\n│    └─ConvTranspose2d: 2-10             [128, 1, 28, 28]          512\n│    └─Tanh: 2-11                        [128, 1, 28, 28]          --\n==========================================================================================\nTotal params: 312,256\nTrainable params: 312,256\nNon-trainable params: 0\nTotal mult-adds (G): 1.76\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 24.26\nParams size (MB): 1.25\nEstimated Total Size (MB): 25.56\n==========================================================================================\n\n\n\n\nDiscriminator\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf:int=32, nc:int=1, alpha:float=0.2):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf*2),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Conv2d(ndf*2, ndf*4, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf*4),\n            nn.LeakyReLU(alpha, inplace=True),\n\n            nn.Conv2d(ndf*4, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = self.layers(x)\n        x = torch.reshape(x, (x.size(0), -1))\n        return x\n    \n\nsummary(Discriminator(), input_size=(BATCH_SIZE, 1, 28, 28))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nDiscriminator                            [128, 1]                  --\n├─Sequential: 1-1                        [128, 1, 1, 1]            --\n│    └─Conv2d: 2-1                       [128, 32, 14, 14]         512\n│    └─BatchNorm2d: 2-2                  [128, 32, 14, 14]         64\n│    └─LeakyReLU: 2-3                    [128, 32, 14, 14]         --\n│    └─Conv2d: 2-4                       [128, 64, 7, 7]           32,768\n│    └─BatchNorm2d: 2-5                  [128, 64, 7, 7]           128\n│    └─LeakyReLU: 2-6                    [128, 64, 7, 7]           --\n│    └─Conv2d: 2-7                       [128, 128, 4, 4]          73,728\n│    └─BatchNorm2d: 2-8                  [128, 128, 4, 4]          256\n│    └─LeakyReLU: 2-9                    [128, 128, 4, 4]          --\n│    └─Conv2d: 2-10                      [128, 1, 1, 1]            2,048\n│    └─Sigmoid: 2-11                     [128, 1, 1, 1]            --\n==========================================================================================\nTotal params: 109,504\nTrainable params: 109,504\nNon-trainable params: 0\nTotal mult-adds (M): 369.68\n==========================================================================================\nInput size (MB): 0.40\nForward/backward pass size (MB): 23.46\nParams size (MB): 0.44\nEstimated Total Size (MB): 24.30\n=========================================================================================="
  },
  {
    "objectID": "posts/2023-10-10-vanilla-GANs/index.html#evaluation-1",
    "href": "posts/2023-10-10-vanilla-GANs/index.html#evaluation-1",
    "title": "Generative Adversarial Networks",
    "section": "Evaluation",
    "text": "Evaluation\n\nMNIST Digits Dataset\n\nIMG_DIM = (1, 28, 28)\nNOISE_DIM = 128\n\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize(0.5, 0.5),\n])\n\ndata = get_minst_dataset(transform)\ndataloader = DataLoader(data, **loader_kwargs)\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# benchmark_noise is used for the animation to show how output evolve on same vector\nbenchmark_noise = torch.randn(16*16, NOISE_DIM, device=device)\n\ngenerator = Generator(nz=NOISE_DIM, ngf=32, nc=IMG_DIM[0]).to(device)\ngenerator.apply(weights_init)\n\ndiscriminator = Discriminator(ndf=32, nc=IMG_DIM[0]).to(device)\ndiscriminator.apply(weights_init)\n\noptimizer_G = optim.AdamW(\n    generator.parameters(),\n    lr=OPTIMIZER_LR,\n    betas=OPTIMIZER_BETAS,\n    weight_decay=L2_NORM,\n)\n\noptimizer_D = optim.AdamW(\n    discriminator.parameters(),\n    lr=OPTIMIZER_LR,\n    betas=OPTIMIZER_BETAS,\n    weight_decay=L2_NORM,\n)\n\ncriterion = nn.BCELoss().to(device)\n\n\nanimation = []\n\ng_losses, d_losses = [], []\nfor epoch in tqdm(range(N_EPOCHS), unit='epochs'):\n\n    generator.train()\n    discriminator.train()\n\n    for samples_real, _ in dataloader:\n        g_loss, d_loss = train_step(\n            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device\n        )\n\n        g_losses.append(g_loss)\n        d_losses.append(d_loss)\n\n    generator.eval()\n    with torch.inference_mode():\n        images = generator(benchmark_noise)\n        images = images.detach().cpu()\n    \n        images = make_grid(images, nrow=16, normalize=True)\n    \n        animation.append(images)\n\n100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [04:54&lt;00:00,  2.94s/epochs]\n\n\n\n\n\n\n\n\nGenerator and Discriminator loss evolution over epochs using DCGAN on the MNIST digit dataset.\n\n\n\n\n\n\nMNIST Fashion Dataset\n\nIMG_DIM = (1, 28, 28)\nNOISE_DIM = 128\n\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize(0.5, 0.5),\n])\n\ndata = get_mnist_fashion_dataset(transform)\ndataloader = DataLoader(data, **loader_kwargs)\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# benchmark_noise is used for the animation to show how output evolve on same vector\nbenchmark_noise = torch.randn(16*16, NOISE_DIM, device=device)\n\ngenerator = Generator(nz=NOISE_DIM, ngf=32, nc=IMG_DIM[0]).to(device)\ngenerator.apply(weights_init)\n\ndiscriminator = Discriminator(ndf=32, nc=IMG_DIM[0]).to(device)\ndiscriminator.apply(weights_init)\n\noptimizer_G = optim.AdamW(\n    generator.parameters(),\n    lr=OPTIMIZER_LR,\n    betas=OPTIMIZER_BETAS,\n    weight_decay=L2_NORM,\n)\n\noptimizer_D = optim.AdamW(\n    discriminator.parameters(),\n    lr=OPTIMIZER_LR,\n    betas=OPTIMIZER_BETAS,\n    weight_decay=L2_NORM\n)\n\ncriterion = nn.BCELoss().to(device) # F.binary_cross_entropy_with_logits #nn.BCELoss().to(device)\n\n\nanimation = []\n\ng_losses, d_losses = [], []\nfor epoch in tqdm(range(N_EPOCHS), unit='epochs'):\n\n    generator.train()\n    discriminator.train()\n\n    for samples_real, _ in dataloader:\n        g_loss, d_loss = train_step(\n            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device\n        )\n\n        g_losses.append(g_loss)\n        d_losses.append(d_loss)\n\n    generator.eval()\n    with torch.inference_mode():\n        images = generator(benchmark_noise)\n        images = images.detach().cpu()\n    \n        images = make_grid(images, nrow=16, normalize=True)\n    \n        animation.append(images)\n\n100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [04:55&lt;00:00,  2.95s/epochs]\n\n\n\n\n\n\n\n\nGenerator and Discriminator loss evolution over epochs using DCGAN on the MNIST fashion dataset."
  },
  {
    "objectID": "snippets.html",
    "href": "snippets.html",
    "title": "Code snippets",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/2023-11-20-research-compute-infrastructure/index.html",
    "href": "posts/2023-11-20-research-compute-infrastructure/index.html",
    "title": "Research Compute Infrastructure",
    "section": "",
    "text": "Recent technological advances have transformed education, elevating the quality of teaching and learning. Jupyter Notebooks emerge as a leading tool for interactive computing, programming, and data analysis (Perkel 2018; Mendez et al. 2019; Granger et al. 2021). However, hardware limitations became a significant hurdle when handling larger research projects. While public cloud services are an option, they come with notable drawbacks. In response, we developed a private cloud solution for our lab using Kubernetes. This solution addresses cost and security concerns while ensuring adaptability. Through this technology, we have enabled efficient app management, scalability, and resource flexibility.\n\n\nA Jupyter Notebook is an open document format based on JSON1. Notebooks are organized into a sequence of cells, with each cell containing code, descriptive text, equations, and rich outputs (e.g., text displays, tables, audio, images, and animations). Tools like JupyterLab provide a platform for interactive code execution, data analysis, and documentation, all within a single interface, culminating in a Jupyter Notebook. These notebooks support various programming languages (e.g., Python, R, Scala, C++) and allow users to write and execute code cells iteratively (using REPL2 or WETL3 approaches), offering immediate visibility of intermediate results. This facilitates the creation of narrative-driven data analyses, educational materials, and interactive presentations. Due to their versatility and interactivity, Jupyter Notebooks are a robust teaching tool for learning, conducting data science, and computer research.\nBecause of these remarkable features, our research lab decided to incorporate Jupyter Notebooks into our research lab’s educational and research processes. By doing this, we encourage students and researchers to document their projects and easily share their research results.\n\n\n\nHowever, for large-scale projects involving hefty data processing on personal computers, using Jupyter Notebooks becomes a significant challenge. We frequently run into hardware limitations like storage space, RAM, processing power, and access to computational accelerators, which can hinder or even halt our progress. These projects are typically in the early stages of research, analysis, or prototyping, so intensive optimizations are impractical because they can slow down experimental development. Two potential solutions emerge: running Jupyter Notebooks on the grid, HPC infrastructure, or cloud services.\nHPC infrastructure, like SLING in Slovenia or EuroHPC on a European level, offers immense computational power. However, given that HPCs are significant investments, queue management solutions like SLURM are employed in the HPC world to optimize their use. Computation tasks must be pre-packaged with metadata, code, and input data. These tasks then join a waiting list. This approach is not aligned well with data-driven research, which aims for interactive programming and quick feedback, limiting the full utilization of Jupyter Notebooks. Hence, cloud services become a more common choice for these notebooks.\nPublic cloud platforms like Google Colab and Kaggle have popularized Jupyter Notebook usage. Users can access the service anytime without queues, edit notebooks, and utilize cloud computing resources, all via a browser. Both services are freely accessible in a limited version. However, due to high user demand, these platforms sometimes limit computational resources, affecting service quality. Alternatives include custom paid services in the public cloud (e.g., AWS, Azure, GCP, Alibaba Cloud), tailoring infrastructure to customer needs. However, public cloud services have drawbacks, including high rental costs, unpredictable market-affected expenses, and security concerns when handling sensitive data.\nPrivate clouds are an alternative to the public cloud, addressing cost and security challenges. They are crucial for research labs and companies dealing with sensitive data or requiring high adaptability. It grants organizations more transparency and cost control based on their needs and capabilities. Despite the initial technical knowledge and infrastructure investment requirements, private clouds offer enhanced security, control, and flexibility, leading to more predictable costs in the long run.\nSeveral technologies are available to set up a private cloud, including commercial options (e.g., VMware vSphere, Red Hat OpenShift, IBM Cloud Private) and open-source solutions (e.g., OpenStack, Eucalyptus, Kubernetes). Among the open-source options, Kubernetes is the most popular.\nKubernetes (abbreviated as K8s) is an open-source platform designed for the automation, management, and deployment of applications within containers. Its advanced orchestration features allow for efficient application management, automatic scaling, monitoring of their performance, and high availability. It can simplify the development and maintenance of complex cloud-based applications.\nContrary to Docker and docker-compose, which primarily focus on building, storing, and running individual containers, Kubernetes offers a much more comprehensive platform for managing containers across expansive environments that span multiple computing nodes. While Docker provides easy creation and operation of individual containers, and docker-compose allows defining multiple containers as application units, Kubernetes facilitates the management of entire clusters of these application units throughout their life cycle, which includes automatic deployment, dynamic adjustments based on load, recovery in case of errors, and more advanced service and network management.\nIn our research laboratory, due to the growing computational demands prevalent in data science and the desire to retain the recognizable workflow present in Jupyter notebooks, we have developed our private cloud solution based on Kubernetes technology.\nThe following sections will present a private cloud setup featuring Jupyter notebooks built on top of open-source solutions. The user experience closely resembles that of existing paid cloud services. The private cloud must meet the following requirements:\n\nSystem Scalability: The cloud should allow for easily adding computing nodes to the cluster without disrupting the operational system, supporting larger research projects or teaching groups.\nEfficient Resource Management: The system must enable precise allocation of resources to users. In this context, an administrator can define a balance between a lax and strict resource allocation policy.\nEnhanced Collaboration Experience: The system should allow for straightforward sharing of Jupyter notebooks among users, promoting collaboration on joint projects and idea exchange between researchers and students.\nNo Waiting Queues: The system should eliminate waiting queues, offering users immediate access to computational resources to the best of their capacity."
  },
  {
    "objectID": "posts/2023-11-20-research-compute-infrastructure/index.html#jupyter-notebooks",
    "href": "posts/2023-11-20-research-compute-infrastructure/index.html#jupyter-notebooks",
    "title": "Research Compute Infrastructure",
    "section": "",
    "text": "A Jupyter Notebook is an open document format based on JSON1. Notebooks are organized into a sequence of cells, with each cell containing code, descriptive text, equations, and rich outputs (e.g., text displays, tables, audio, images, and animations). Tools like JupyterLab provide a platform for interactive code execution, data analysis, and documentation, all within a single interface, culminating in a Jupyter Notebook. These notebooks support various programming languages (e.g., Python, R, Scala, C++) and allow users to write and execute code cells iteratively (using REPL2 or WETL3 approaches), offering immediate visibility of intermediate results. This facilitates the creation of narrative-driven data analyses, educational materials, and interactive presentations. Due to their versatility and interactivity, Jupyter Notebooks are a robust teaching tool for learning, conducting data science, and computer research.\nBecause of these remarkable features, our research lab decided to incorporate Jupyter Notebooks into our research lab’s educational and research processes. By doing this, we encourage students and researchers to document their projects and easily share their research results."
  },
  {
    "objectID": "posts/2023-11-20-research-compute-infrastructure/index.html#scalability",
    "href": "posts/2023-11-20-research-compute-infrastructure/index.html#scalability",
    "title": "Research Compute Infrastructure",
    "section": "",
    "text": "However, for large-scale projects involving hefty data processing on personal computers, using Jupyter Notebooks becomes a significant challenge. We frequently run into hardware limitations like storage space, RAM, processing power, and access to computational accelerators, which can hinder or even halt our progress. These projects are typically in the early stages of research, analysis, or prototyping, so intensive optimizations are impractical because they can slow down experimental development. Two potential solutions emerge: running Jupyter Notebooks on the grid, HPC infrastructure, or cloud services.\nHPC infrastructure, like SLING in Slovenia or EuroHPC on a European level, offers immense computational power. However, given that HPCs are significant investments, queue management solutions like SLURM are employed in the HPC world to optimize their use. Computation tasks must be pre-packaged with metadata, code, and input data. These tasks then join a waiting list. This approach is not aligned well with data-driven research, which aims for interactive programming and quick feedback, limiting the full utilization of Jupyter Notebooks. Hence, cloud services become a more common choice for these notebooks.\nPublic cloud platforms like Google Colab and Kaggle have popularized Jupyter Notebook usage. Users can access the service anytime without queues, edit notebooks, and utilize cloud computing resources, all via a browser. Both services are freely accessible in a limited version. However, due to high user demand, these platforms sometimes limit computational resources, affecting service quality. Alternatives include custom paid services in the public cloud (e.g., AWS, Azure, GCP, Alibaba Cloud), tailoring infrastructure to customer needs. However, public cloud services have drawbacks, including high rental costs, unpredictable market-affected expenses, and security concerns when handling sensitive data.\nPrivate clouds are an alternative to the public cloud, addressing cost and security challenges. They are crucial for research labs and companies dealing with sensitive data or requiring high adaptability. It grants organizations more transparency and cost control based on their needs and capabilities. Despite the initial technical knowledge and infrastructure investment requirements, private clouds offer enhanced security, control, and flexibility, leading to more predictable costs in the long run.\nSeveral technologies are available to set up a private cloud, including commercial options (e.g., VMware vSphere, Red Hat OpenShift, IBM Cloud Private) and open-source solutions (e.g., OpenStack, Eucalyptus, Kubernetes). Among the open-source options, Kubernetes is the most popular.\nKubernetes (abbreviated as K8s) is an open-source platform designed for the automation, management, and deployment of applications within containers. Its advanced orchestration features allow for efficient application management, automatic scaling, monitoring of their performance, and high availability. It can simplify the development and maintenance of complex cloud-based applications.\nContrary to Docker and docker-compose, which primarily focus on building, storing, and running individual containers, Kubernetes offers a much more comprehensive platform for managing containers across expansive environments that span multiple computing nodes. While Docker provides easy creation and operation of individual containers, and docker-compose allows defining multiple containers as application units, Kubernetes facilitates the management of entire clusters of these application units throughout their life cycle, which includes automatic deployment, dynamic adjustments based on load, recovery in case of errors, and more advanced service and network management.\nIn our research laboratory, due to the growing computational demands prevalent in data science and the desire to retain the recognizable workflow present in Jupyter notebooks, we have developed our private cloud solution based on Kubernetes technology.\nThe following sections will present a private cloud setup featuring Jupyter notebooks built on top of open-source solutions. The user experience closely resembles that of existing paid cloud services. The private cloud must meet the following requirements:\n\nSystem Scalability: The cloud should allow for easily adding computing nodes to the cluster without disrupting the operational system, supporting larger research projects or teaching groups.\nEfficient Resource Management: The system must enable precise allocation of resources to users. In this context, an administrator can define a balance between a lax and strict resource allocation policy.\nEnhanced Collaboration Experience: The system should allow for straightforward sharing of Jupyter notebooks among users, promoting collaboration on joint projects and idea exchange between researchers and students.\nNo Waiting Queues: The system should eliminate waiting queues, offering users immediate access to computational resources to the best of their capacity."
  },
  {
    "objectID": "posts/2023-11-20-research-compute-infrastructure/index.html#turnkey-solution",
    "href": "posts/2023-11-20-research-compute-infrastructure/index.html#turnkey-solution",
    "title": "Research Compute Infrastructure",
    "section": "Turnkey Solution?",
    "text": "Turnkey Solution?\nWhen planning the private cloud, we initially explored turnkey solutions, including NVIDIA DeepOps. Despite its advantages, we built our custom solution for the following reasons. While DeepOps is an excellent turnkey solution with maintained source code on GitHub and offers commercial support, initial setup requires configuration file adjustments, including Ansible scripts for automated (re)configuration of installed Linux distribution. Its complexity discouraged us, further investing our time into tinkering with it.\nOne of our biggest concerns was the intricate solution that tries to be versatile and “simple”. However, this inevitably leads to hiding functionalities and, in case of issues, jumping around documentation of multiple unrelated internally used tools. Despite proclaimed simplicity, troubleshooting or upgrade problems require manual intervention, where a thorough understanding of Linux, DeepOps, its internal tooling, and their interactions is necessary for system control. Therefore, we decided to start with a minimalist solution and, over time, plan to expand the system to understand the infrastructure’s operation better."
  },
  {
    "objectID": "posts/2023-11-20-research-compute-infrastructure/index.html#foundation-infrastructure",
    "href": "posts/2023-11-20-research-compute-infrastructure/index.html#foundation-infrastructure",
    "title": "Research Compute Infrastructure",
    "section": "Foundation Infrastructure",
    "text": "Foundation Infrastructure\nIn this section, we discuss the foundation infrastructure of our private cloud solution. We’ll go through these building blocks, including the selection of container management tools and resource sharing, which are vital for the operation of the Kubernetes platform.\nOperating System: We chose Ubuntu Server based on the Debian Linux distribution for our system. The advantage of widely used Debian-based Linux distributions is the abundance of available knowledge resources and support, making problem-solving more accessible. Among alternatives, like declarative binary reproducible NixOS and RHEL-based distributions, we also considered the Talos distribution specialized for Kubernetes. However, we preferred to stick with Ubuntu Server due to the Talos project’s novelty and associated risks.\nContainer Management: For container management, we selected ContainerD, also used in the DeepOps solution and officially supported by NVIDIA. It is an open-source tool that implements the CRI interface for communication between the operating system and Kubernetes for efficient and reliable container management.\nData Storage: For data storage, we chose ZFS, which resides on one of the nodes. Although solutions like HDFS, Gluster, Lustre, or Ceph are far more common in the HPC world, they require dedicated infrastructure and tools to offer features offered by ZFS out-of-the-box. Features include checkpoints, data deduplication, compression, a COW (copy-on-write) system to prevent data loss during writing, immunity to silent bit-rot, the ability to use disks as redundancy for mechanical failures, and the use of fast SSD devices as a cache. It also allows easy manual intervention in the event of incidents. However, at the time of writing, ZFS does not stretch across multiple nodes, posing a risk of cluster failure in case of a data-storing node’s malfunction (single point of failure). There is an ongoing effort to implement ZFS’ distributed RAID (dRAID) [src].\nTo access ZFS storage from Kubernetes, we used the NFS server, which is part of the Linux kernel. We chose NFS because it is one of the few methods that allow multiple containers to bind to the same mounting point (see table).\nSystem Management: For remote management and node configuration, we use Ansible maintained by Red Hat. We selected it due to its prevalence in other significant open-source projects and positive experiences from past projects."
  },
  {
    "objectID": "posts/2023-11-20-research-compute-infrastructure/index.html#kubernetes",
    "href": "posts/2023-11-20-research-compute-infrastructure/index.html#kubernetes",
    "title": "Research Compute Infrastructure",
    "section": "Kubernetes",
    "text": "Kubernetes\nIn Kubernetes, everything operates as a service. These services provide various functionalities that enhance Kubernetes capabilities, such as storage access, CPU and GPU allocation, traffic management, and connecting services within a mesh network.\nTo support specific functionalities, appropriate services (much like operating system drivers) must be installed. These specialized services, often called “operators” in Kubernetes terminology [src], are essential. They not only deploy and manage functionalities but also respond to issues. Operators enhance Kubernetes by interfacing with standardized and version-controlled APIs.\nPut simply, operators can be viewed as one or more Docker containers. They function as an intermediary layer, bridging the gap between the underlying operating system and Kubernetes APIs.\n\nInternal Services\nIn Kubernetes, internal services are not intended for end users but are crucial for the system’s operation. These services operate in the background, ensuring vital functionalities that enable the stable operation and management of the container environment. In this subsection, we will introduce key services within Kubernetes and explain their role in our infrastructure. We will describe each service’s primary functionality and examine alternatives we explored in making our decision.\nKubernetes Distribution: When choosing a Kubernetes distribution, we examined three options: Canonical MicroK8s, Red Hat OpenShift, and the basic “vanilla” Kubernetes distribution. “Vanilla” Kubernetes represents the unaltered version directly available in Google’s repository, without pre-installed applications or plugins. We went for the vanilla version as it provides flexibility and freedom of choice of the extensions.\nMicroK8s is an excellent solution for quick experimentation and setting up the system on smaller devices with limited resources (e.g., Raspberry Pi). However, it has many pre-installed applications and uses Canonical’s Snap packaging system, which can complicate adjusting configuration files and accessing external services, such as the NFS server.\nWe ruled out OpenShift due to the complexity of managing security profiles that, for our use case, were excessive, requiring substantial effort to implement these profiles for each service. Therefore, we opted for the basic “vanilla” Kubernetes distribution, offering more flexible and straightforward customization tailored to our needs.\nKubernetes Package Deployment: To describe the implementation of services in Kubernetes, a straightforward approach is to write YAML configuration file(s) (also called manifesto), which are then forwarded to Kubernetes via the command line. However, some services can be quite complex, leading developers to create service packages, making services more general-purpose and customizable through parameters. The most widespread packaging system is Helm, allowing for more portable and adaptable service packages. Helm uses YAML files as templates (much like forms), which are then filled out based on the provided parameters and sent to Kubernetes.\nNetwork Operator: Kubernetes services must be interconnected to communicate with other services. We opted for the open-source Tigera Calico operator to manage interconnections. Given its prevalence and functionalities, we found it the most suitable solution.\nCalico and Flannel are the most common solutions for network operators. Flannel is more minimalistic and operates as a network switch (layer 2) using technologies like Open vSwitch or VXLAN. In contrast, Calico routes traffic like a network router (layer 3). Especially in cases of multi-cluster (i.e., multiple physical locations) or hybrid cloud services, Calico emerges as a better choice.\nStorage Operator: For effective storage management within the Kubernetes system, we used csi-driver-nfs. It allows us to use the already established NFS servers. With it, we ensure uninterrupted access to persistent storage for any service within our private cloud.\nThe csi-driver-nfs proved most suitable since we already had an NFS server on one of the nodes. It allows us straightforward and centralized storage management for all services within Kubernetes. Centralization brings about numerous advantages, yet also challenges. Among the latter is the system’s vulnerability during a potential outage of the node storing the data. Nonetheless, centralization facilitates easier troubleshooting and backup execution.\nBare-Metal Ingress Load-Balancer: To ensure balanced ingress (of incoming) traffic among entry points in our Kubernetes cluster, we decided to utilize the MetalLB solution. After thorough research, we could not find any other alternative. Most of the online documentation (e.g., tutorials, blogs) focuses on setting up infrastructure on public clouds such as AWS or Azure and using solutions tailored to the demands of public cloud providers. However, since our infrastructure is based on our hardware (i.e., bare-metal), we opted for MetalLB, which has proven reliable and effective in routing traffic among our Kubernetes cluster’s entry points.\nIngress Operator: While a network operator manages interconnection between services within Kubernetes, the ingress operator manages access to services from the outside world. For security reasons, direct access to the internal network is prohibited. While it is possible to enter the internal network through a proxy (i.e., kubectl proxy), that’s meant only for debugging purposes. The ingress operator is designed to resolve domain names and route traffic to the correct container and port, which we described in the service’s YAML manifesto. Using domain name resolution has several advantages. Regardless of the service’s internal IP address, the ingress operator will always correctly direct traffic. The ingress operator can act as a load balancer when there is a high-traffic load, balancing traffic between multiple copies of service.\nAmong the most common solutions for ingress traffic management are NGINX and Traefik Ingress Operator. We chose NGINX, but the operators’ interface is standardized, so there are almost no differences between the solutions. Regardless of the selected solution, once a new service is deployed, the operator will follow the service’s manifesto and automatically route traffic to the appropriate container.\nGPU Operator: For efficient management of access to computing accelerators, we decided to use the official NVIDIA GPU-Operator suite of services. This suite provides two distinct installation options for NVIDIA drivers. The first option leverages host drivers, while the second involves drivers packaged within containers. Initially, we opted for the first option, wanting to enable the use of accelerators outside the Kubernetes framework. However, due to issues with conflicting driver versions, we decided to utilize the drivers provided by the GPU-Operator.\n\n\nUser Services\nIn this section, we introduce the selected services available to end users of our private cloud, enabling efficient execution and management of their research and educational projects.\nJupyterHub represents one of the key services in our private cloud, providing users with easy access to computing resources, data, and Jupyter notebooks for research and teaching purposes. To implement JupyterHub, we use the Z2JH (Zero-to-JupyterHub) implementation, developed by a team of researchers at the University of Berkeley in collaboration with the Jupyter community. This solution facilitates quick setup and maintenance.\nEvery individual user is granted access to an isolated container instance via their username and password or OAuth provider, such as GitHub, Google, or OAuth0. An isolated instance offers a stripped-down Linux environment with limited internet access and without admin permissions. Kubernetes then ensures access to shared data resources, common directories, and the use of computational accelerators.\nThe JupyterHub user interface is similar to Google Colab or Kaggle services. Upon entering the isolated instance, JupyterLab is already running, and the user also has access to the Linux terminal. Additional tools and software packages can be installed using pip, conda, or mamba commands.\nGrafana is a key service in our private cloud, facilitating a straightforward display of the current workload of the compute cluster and the availability of computational accelerators. This data visualization platform allows users to present information clearly and transparently, aiding them in making decisions regarding resource usage and optimizing their tasks. Utilizing Grafana ensures efficient and transparent resource monitoring, enhancing user experience. Data collection (Prometheus) and visualization (Grafana) are deployed by kube-prometheus-stack."
  },
  {
    "objectID": "posts/2023-11-20-research-compute-infrastructure/index.html#hardware",
    "href": "posts/2023-11-20-research-compute-infrastructure/index.html#hardware",
    "title": "Research Compute Infrastructure",
    "section": "Hardware",
    "text": "Hardware\n\n\nTable 2: Hardware specifications of Computing node.\n\n\nHardware\nSpecifications\n\n\n\n\nChasis\nSupermicro A+ Server 4124GS-TNR, 4U size, up to PCI-E 8 GPUs\n\n\nCPU\n2x AMD EPYC 75F3 (32C/64T, up to 4GHz, 256MB L3 cache)\n\n\nMemory\n1TB (16x64GB) REG ECC DDR4, 3200MHz\n\n\nSystem\n2x 2TB SSD NVMe, software RAID1 (mirror)\n\n\nStorage\n6x 8TB SSD SATA, software RAIDZ1 (1 disk redundacy)\n\n\nGPU\n2x NVIDIA A100 80GB PCI-E\n\n\n\n\nWhen we bought the hardware in early 2022, we chose third-generation AMD EPYC processors. Specifically, we went for the F-series, which has higher base and turbo frequencies — up to 4GHz — at the cost of fewer cores. We picked a CPU with the highest available TDP of 280W. We installed server-grade registered error-correcting memory at the highest frequency supported by the processor and populated all eight channels on both processors. Sixteen sticks of RAM in total. Although we considered solutions from Intel, AMD EPYC processors had better price-to-performance ratios.\nFrom the perspective of numerical performance, our significant concern was Intel-optimized libraries, such as Intel MKL, often found in numerical tools. The library has a “bug” that causes non-Intel processors to utilize a slower SSE instead of more advanced AVX vectorization instructions [src]. OpenBLAS is a good alternative but requires some effort to install it. See Anaconda no-mkl package.\nWe chose two NVMe drives configured in the mirror configuration (RAID1) for the system drive. We selected six 8TB SSD SATA drives configured in ZFS RAIDZ1 for data storage, which has one drive redundancy. We also chose two A100 GPUs as accelerators.\nNVIDIA A100 GPUs come in two form factors: PCI-E and SXM4. The SXM4 proprietary form factor has a higher TDP and high-bandwidth NVLink interconnections between every GPU through NVSwitch hardware. The downside of SXM4 is that it will only support Ampere generation GPUs and require a special motherboard. The PCI-E variant has a lower TDP, and NVLink can only be across two GPUs. However, we decided against vendor lock-in, limiting ourselves to one brand and generation, and went with the PCI-E variant.\nWe considered the most likely workflow scenarios. We expected most communication to be CPU-to-GPU, with GPUs sliced into several instances. Once GPU slicing is enabled, the NVIDIA drivers disable NVLink, rendering NVLink connections useless. We can change GPU slicing strategy freely.\n\n\n\nFigure 2: The computing node on my desk underwent final checks before being installed in the server rack."
  },
  {
    "objectID": "posts/2023-11-20-research-compute-infrastructure/index.html#user-experience",
    "href": "posts/2023-11-20-research-compute-infrastructure/index.html#user-experience",
    "title": "Research Compute Infrastructure",
    "section": "User Experience",
    "text": "User Experience\nAfter deploying the hardware and software stack, we conducted a month-long live test to stabilize the configuration. During this period, users were informed that we might reboot the system or make significant changes without responsibility for any potential data loss, though we aimed to minimize such occurrences.\nWe made two key decisions about resource allocation. Users can utilize all available memory and CPU cores. When CPU demand is high, Kubernetes and the operating system manage the scheduling of tasks. In cases of high memory usage, the job consuming the most memory is terminated to protect other running tasks.\nFeedback from students and researchers was overwhelmingly positive, highlighting the high speed, numerous cores, ample memory, and dedicated GPU access without interference.\nDuring the testing phase, “testers” identified several issues, which were promptly addressed. These included adding a shared folder with datasets and Jupyter notebooks, shared package cache, and better persistence of running tasks in JupyterLab.\n\nJupyterHub\nJupyterHub has become a crucial component of our research infrastructure, enhancing our workflow significantly. Its smooth integration was largely due to the interface and functionality of JupyterHub, which closely resemble the tools our researchers and students were familiar with. This similarity played a key role in its quick adoption and high user satisfaction.\n\n\n\nFigure 3: JupyterHub offers a list of predefined containers, where some of them offer a GPU instance.\n\n\nUpon logging into JupyterHub, users are presented with a list of predefined containers (as shown in Figure 3). Our recent update includes several options:\n\nA basic minimal working environment environment.\nA comprehensive data science environment equipped with multiple packages and support for Python, R, and Julia.\nA selection of containers offering GPU instances.\n\nThe development environment greets users with a layout similar to modern IDEs, featuring a file explorer on the left and code editor tabs on the right (see Figure 4).\n\n\n\nFigure 4: JupyterLab workspace with familiar layout: a file explorer on the left and code editor tabs on the right.\n\n\n\n\nGrafana\nFor transparent insight into infrastructure availability, the user has read-only access to the Grafana dashboard (Figure~\\(\\ref{fig:grafana:dashboard}\\)). Here, they can observe most of the infrastructure metrics.\nFor transparent insight into infrastructure availability, the user has read-only access to the Grafana dashboard. Dashboard visualizes computing resource utilization including metrics like total and per-container CPU usage, memory usage per container, GPU utilization, temperature readings, and storage I/O (see Figure 5).\n\n\n\nFigure 5: Visualization of computing cluster utilization showing total and per-container CPU utilization, per-container memory utilization, GPU slices utilization, temperatures, and storage I/O."
  },
  {
    "objectID": "posts/2023-11-20-research-compute-infrastructure/index.html#footnotes",
    "href": "posts/2023-11-20-research-compute-infrastructure/index.html#footnotes",
    "title": "Research Compute Infrastructure",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJSON: JavaScript Object Notation↩︎\nREPL: read–eval–print loop↩︎\nWETL: write-eval-think-loop↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gregor Cerar, PhD",
    "section": "",
    "text": "Hi 🖐️! I’m Gregor (Greg for short). I’m currently a postdoc at SensorLab, Jožef Stefan Institute, and concurrently a data scientist at Comsensus.\nMost of my work revolves around AI applications in smart infrastructure and time series data. My main research interest is in applications of self-supervised learning in wireless communications.\nOn a personal note, I’m an introvert who usually stays away from the spotlight and deeply enjoys the quiet corners of research and writing elegant code, mainly in Python. Beyond the screens and algorithms, I enjoy hiking, playing chess, and the “Uno!” card game."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nResearch Compute Infrastructure\n\n\n\n\n\n\n\ncompute\n\n\ninfrastructure\n\n\n\n\nOur research lab needed a more scalable approach to sharing computing resources among researchers. To address this, we built a scalable computing infrastructure on Kubernetes. Researchers can request resources from the pool and use a familiar workflow with Jupyter notebooks. Resources are returned to the pool once a task is complete or idle for too long.\n\n\n\n\n\n\nNov 20, 2023\n\n\n23 min\n\n\n\n\n\n\n  \n\n\n\n\nGenerative Adversarial Networks\n\n\n\n\n\n\n\npytorch\n\n\nGAN\n\n\n\n\nGenerative Adversarial Networks (GANs) represent an innovative class of unsupervised neural networks that have revolutionized the field of artificial intelligence. Eager to learn how they work, I’ve implemented foundational “vanilla” GAN and its more complex counterpart, the Deep Convolutional GAN (DCGAN), from scratch. I’ve put them on a test run on MNIST Digits and Fashion toy datasets.\n\n\n\n\n\n\nOct 10, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nNeural Style Transfer\n\n\n\n\n\n\n\npytorch\n\n\nNST\n\n\n\n\nNeural Style Transfer (NST) was a breakthrough deep learning approach that can transfer artistic style from one image to another. Eager to learn how it works, I’ve implemented the original approach from scratch.\n\n\n\n\n\n\nSep 15, 2023\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Research Compute Infrastructure\n\n\n23 min\n\n\nOur research lab needed a more scalable approach to sharing computing resources among researchers. To address this, we built a scalable computing infrastructure on…\n\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Adversarial Networks\n\n\n5 min\n\n\nGenerative Adversarial Networks (GANs) represent an innovative class of unsupervised neural networks that have revolutionized the field of artificial intelligence. Eager to…\n\n\n\nOct 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Style Transfer\n\n\n8 min\n\n\nNeural Style Transfer (NST) was a breakthrough deep learning approach that can transfer artistic style from one image to another. Eager to learn how it works, I’ve…\n\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-09-15-neural-style-transfer/index.html",
    "href": "posts/2023-09-15-neural-style-transfer/index.html",
    "title": "Neural Style Transfer",
    "section": "",
    "text": "Neural Style Transfer (NST) is a deep learning technique that combines the content of one image with the style of another, like giving your photo a Van Gogh-esque makeover.\nUsing convolutional neural networks, NST examines both images’ features and creates a new image that merges the content’s structure with the style’s attributes. This technique became a hit due to its novel outcomes, leading to its adoption in various apps and platforms and highlighting deep learning’s prowess in image transformation.\nIntroduced initially in “A Neural Algorithm of Artistic Style” (Gatys, Ecker, and Bethge 2015), this method transfers art styles between images. Eager to learn how it works, I’ve implemented the original approach from scratch and presented a few cherry-picked transformed examples."
  },
  {
    "objectID": "posts/2023-09-15-neural-style-transfer/index.html#loss-metrics",
    "href": "posts/2023-09-15-neural-style-transfer/index.html#loss-metrics",
    "title": "Neural Style Transfer",
    "section": "Loss metrics",
    "text": "Loss metrics\nTo effectively implement Neural Style Transfer, we need to quantify how well the generated image matches both the content and style of our source images. This is done using loss metrics. Let’s delve into the specifics of these metrics and how they drive the NST process.\n\nContent loss metric\nContent loss is calculated through Euclidean distance (i.e., mean squared error) between the respective intermediate higher-level feature representation \\(F^l\\) and \\(P^l\\) of original input image \\(\\vec{x}\\) and the content image \\(\\vec{p}\\) at layer \\(l\\).\nHence, a given input image \\(\\vec{x}\\) is encoded in each layer of the CNN by the filter responses to that image. A layer with \\(N_l\\) distinct filters has \\(N_l\\) feature maps of size \\(M_l\\), where \\(M_l\\) is the height times the width of the feature map. So the response in a layer \\(l\\) can be stored in a matrix \\(F^l \\in \\mathcal{R}^{N_l \\times M_l}\\) where \\(F_{ij}^{l}\\) is the activation of the \\(i^{th}\\) filter at position \\(j\\) in layer \\(l\\).\n\\[\n\\mathcal{L}_{content}(\\vec{p}, \\vec{x}, l) = \\frac{1}{2}\\sum_{i,j} (F^{l}_{ij} - P^{l}_{ij})^2\n\\]\n\ndef content_loss_func(target_features, precomputed_content_features):\n    \"\"\"Calculate content loss metric for give layers.\"\"\"\n\n    content_loss = 0.0\n    for layer in precomputed_content_features:\n        target_feature = target_features[layer]\n        content_feature = precomputed_content_features[layer]\n\n        content_layer_loss = F.mse_loss(target_feature, content_feature)\n        content_loss += content_layer_loss\n\n    return content_loss\n\n\n\nStyle loss\nThe style loss is more convolved than the content loss. We compute it by comparing the Gram matrices of the feature maps from the style image and the generated image.\nFirst, let’s understand the Gram matrix. Given the feature map \\(F\\) of size \\(C \\times (H \\times W)\\), where \\(C\\) is the number of channels and \\(H \\times W\\) are the spatial dimensions, the Gram matrix \\(G\\) is of size \\(C \\times C\\) and is computed as\n\\[\nG^l_{ij} = \\sum_k F^l_{ik} F^l_{jk}\n\\]\nwhere \\(G_{ij}\\) is the inner product between vectorized feature maps \\(i\\) and \\(j\\). This results in a matrix that captures the correlation between different feature maps and, thus, the style information.\n\ndef gram_matrix(input: Tensor) -&gt; Tensor:\n    (b, c, h, w) = input.size()\n\n    # reshape into (C x (H x W))\n    features = input.view(b * c, h * w)\n\n    # compute the gram product\n    gram = torch.mm(features, features.t())\n\n    return gram\n\nThe style loss between the Gram matrix of the generated image \\(G\\) and that of style image \\(A\\) (at a specific layer \\(l\\)) is:\n\\[\nE_l = \\frac{1}{4 N^{2}_{l} M^{2}_{l}} \\sum_{i,j}(G^l_{ij} - A^l_{ij})^2\n\\]\nWhere \\(E_l\\) is the style loss for layer \\(l\\), \\(N_l\\) and \\(M_l\\) are the numbers of channels and height times width in the feature representation of layer \\(l\\), respectively. \\(G_{ij}^l\\) and \\(A_{ij}^l\\) are the gram matrices of the intermediate representation of the style image \\(\\vec{a}\\) and the input base image \\(\\vec{x}\\) respectively.\nThe total style loss is:\n\\[\n\\mathcal{L}_{style}(\\vec{a}, \\vec{x}) = \\sum_{l=0}^{L} w_l E_l\n\\]\n\ndef style_loss_func(target_features, style_features, precomputed_style_grams):\n    style_loss = 0.0\n    for layer in style_features:\n        target_feature = target_features[layer]\n        target_gram = gram_matrix(target_feature)\n\n        style_gram = precomputed_style_grams[layer]\n\n        _, c, h, w = target_feature.shape\n\n        weight = STYLE_LAYERS_DEFAULT[layer]\n        layer_style_loss = weight * F.mse_loss(target_gram, style_gram) / (c * h * w)\n        style_loss += layer_style_loss\n\n    return style_loss\n\n\n\nTotal Variation Loss\nTotal Variation (TV) loss, also known as Total Variation Regularization, is commonly added to the Neural Style Transfer objective to encourage spatial smoothness in the generated image. Without it, the output might exhibit noise or oscillations, particularly in regions where the content and style objectives don’t offer much guidance.\nGiven an image \\(\\vec{x}\\) of size \\(H \\times W \\times C\\) (height, width, channels), the Total Variation loss is defined as the sum of the absolute differences between neighboring pixel values:\n\\[\n\\mathcal{L}_{TV}(\\vec{x}) = \\sum_{i,j} ((x_{i,j+1} - x_{i,j})^2 + (x_{i+1,j} - x_{i,j})^2)\n\\]\nwhere \\(x_{i,j}\\) is the pixel value at position \\((i,j)\\).\nIn simple terms, this loss penalizes abrupt changes in pixel values from one to its neighbors. By minimizing this loss, the generated image becomes smoother, reducing artifacts and unwanted noise. When combined with content and style losses, the TV loss ensures that the resulting image not only captures the content and style of the source images but also looks visually coherent and smooth.\n\ndef total_variance_loss_func(target: Tensor):\n    tv_loss = F.l1_loss(target[:, :, :, :-1], target[:, :, :, 1:]) \\\n            + F.l1_loss(target[:, :, :-1, :], target[:, :, 1:, :])\n\n    return tv_loss\n\n\n\nTotal Loss\nThe total loss combines three loss metric components, each targeting a specific aspect of the image generation process. Let’s recap the components:\n\nContent Loss: Ensures the generated image resembles the content image’s content.\nStyle Loss: Ensures the generated image captures the stylistic features of the style image.\nTotal Variation Loss: Encourages spatial smoothness in the generated image, reducing artifacts and noise.\n\nGiven the above components, the total loss \\(\\mathcal{L}_{total}\\) for Neural Style Transfer can be formulated as:\n\\[\n\\mathcal{L}_{total}(\\vec{p},\\vec{a},\\vec{x}) = \\alpha\\mathcal{L}_{content}(\\vec{p},\\vec{x}) + \\beta\\mathcal{L}_{style}(\\vec{a},\\vec{x}) + \\gamma\\mathcal{L}_{TV}(\\vec{x})\n\\]\n\\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\) are weight factors that determine the relative importance of the content, style, and the total variation losses, respectively. By adjusting these weights, one can control the balance between content preservation, style transfer intensity, and the smoothness of the generated image. The algorithm aims to adjust the generated image to minimize the total loss."
  },
  {
    "objectID": "posts/2023-09-15-neural-style-transfer/index.html#input-preparation",
    "href": "posts/2023-09-15-neural-style-transfer/index.html#input-preparation",
    "title": "Neural Style Transfer",
    "section": "Input preparation",
    "text": "Input preparation\nHere we specify path to content and style images:\n\ncontent_path = \"./bridge.jpg\"\nstyle_path = \"./walking-in-the-rain.jpg\""
  },
  {
    "objectID": "posts/2023-09-15-neural-style-transfer/index.html#neural-style-transfer-process",
    "href": "posts/2023-09-15-neural-style-transfer/index.html#neural-style-transfer-process",
    "title": "Neural Style Transfer",
    "section": "Neural Style Transfer Process",
    "text": "Neural Style Transfer Process\nFor feature extraction, we’ll leverage VGG19, pre-trained on ImageNet, same as the original authors. Note that we set the model to evaluation mode, ensuring we only use VGG19 to extract features without altering its weights. We also transfer the neural network (NN) to a chosen device, ideally a GPU, for optimal performance.\n\n\n\n\n\n\nNote\n\n\n\nAn intriguing choice by Gatys et al. was to modify VGG-19, replacing max pooling with average pooling, aiming for visually superior results. However, a challenge arises: our NN was initially trained with MaxPool2d layers. Substituting them can affect activations due to reduced output values. To counteract this, we’ve introduced a custom ScaledAvgPool2d.\n\n\n\n# We will use a frozen pre-trained VGG neural network for feature extraction.\n# In the original paper, authors have used VGG19 (without batch normalization)\nmodel = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n\n# Authors in the original paper suggested using AvgPool instead of MaxPool\n# for more pleasing results. However, changing the pooling also affects\n# activation, so the input needs to be scaled (can't find the original source).\nclass ScaledAvgPool2d(nn.Module):\n    def __init__(self, kernel_size, stride, padding=0, scale_factor=2.0):\n        super().__init__()\n        self.avgpool = torch.nn.AvgPool2d(kernel_size, stride, padding)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        return self.avgpool(x) * self.scale_factor\n\n# (OPTIONAL) Replace max-pooling layers with custom average pooling layers\n#for i, layer in enumerate(model):\n#   if isinstance(layer, torch.nn.MaxPool2d):\n#       model[i] = ScaledAvgPool2d(kernel_size=2, stride=2, padding=0)\n\nmodel = model.eval().requires_grad_(False).to(device)\n\nThe pretrained VGG model used normalized ImageNet samples for better performance. For effective style transfer, we’ll follow suit to improve feature extraction. Though images will appear altered post-normalization, they are reverted to their original state after the NST process. Next, we’ll transform the content and style images by:\n\nLoading them from storage.\nResizing while maintaining aspect ratio.\nConverting to tensors.\nNormalizing using ImageNet weights.\n\n\n# ImageNet normalization weights per channel\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\ntransform = T.Compose([\n    # Shorter edge of the image will be matched to `IMG_SIZE`\n    T.Resize(IMG_SIZE),\n    T.ToTensor(),\n    T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n])\n\ndef load_image(path: Union[str, Path]) -&gt; Tensor:\n    image = Image.open(path).convert(\"RGB\")\n\n    # Transform images into tensors\n    image = transform(image)\n\n    # Add dimension to imitate batch size equal to 1: (C,H,W) -&gt; (B,C,H,W)\n    image = image.unsqueeze(0)\n    return image\n\nThe following code will prepares content \\(\\vec{p}\\), style \\(\\vec{a}\\), and target \\(\\vec{x}\\) images. The target image is a clone of the content image and we enable computation of gradients on it.\n\n# The \"style\" image from which we obtain style\nstyle = load_image(style_path).to(device)\n\n# The \"content\" image on which we apply style\ncontent = load_image(content_path).to(device)\n\n# The \"target\" image to store the outcome\ntarget = content.clone().requires_grad_(True).to(device)\n\nThe function below retrieves feature maps from designated layers. As shown in Figure 1:\n\nContent feature map comes from relu5_2.\nStyle feature maps are sourced from relu1_1, relu2_1, relu3_1, relu4_1, and relu5_1.\n\n\ndef get_features(image: Tensor, model: nn.Module, layers=None):\n    if layers is None:\n        layers = tuple(STYLE_LAYERS_DEFAULT) + CONTENT_LAYERS_DEFAULT\n\n    features = {}\n    block_num = 1\n    conv_num = 0\n\n    x = image\n\n    for layer in model:\n        x = layer(x)\n\n        if isinstance(layer, nn.Conv2d):\n            # produce layer name to find matching convolutions from the paper\n            # and store their output for further processing.\n            conv_num += 1\n            name = f\"conv{block_num}_{conv_num}\"\n            if name in layers:\n                features[name] = x\n\n        elif isinstance(layer, (nn.MaxPool2d, nn.AvgPool2d, ScaledAvgPool2d)):\n            # In VGG, each block ends with max/avg pooling layer.\n            block_num += 1\n            conv_num = 0\n\n        elif isinstance(layer, (nn.BatchNorm2d, nn.ReLU)):\n            pass\n\n        else:\n            raise Exception(f\"Unknown layer: {layer}\")\n\n    return features\n\nSince content and style images never change, we can precompute their feature maps and grams to speed up the NST process.\n\n# Precompute content features, style features, and style gram matrices.\ncontent_features = get_features(content, model, CONTENT_LAYERS_DEFAULT)\nstyle_features = get_features(style, model, STYLE_LAYERS_DEFAULT)\n\nstyle_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n\nNext, we will use Adam optimizer, where we specify that only target image \\(\\vec{x}\\) is considered for optimization.\n\noptimizer = optim.Adam([target], lr=LEARNING_RATE)\n\nThe final step of NST is to transfer style using everything we’ve implemented. We extract feature maps, compute total loss, perform steps using gradient descent, and repeat the process N_EPOCHS times. Gradient changes will apply only to the target image.\nTo notably enhance NST speed, I utilized mixed precision with the unique bfloat16 found in newer hardware. Traditional half-precision float16 doesn’t yield the same results. I’ve tested it. Probably because of the issue with gradient scaling.\n\npbar = tqdm(range(N_EPOCHS))\n\nfor step in pbar:\n    with torch.autocast('cuda', dtype=torch.bfloat16, enabled=AMP_ENABLED):\n        target_features = get_features(target, model)\n\n        content_loss = CONTENT_WEIGHT * content_loss_func(target_features, content_features)\n        style_loss = STYLE_WEIGHT * style_loss_func(target_features, style_features, style_grams)\n        tv_loss = TV_WEIGHT * total_variance_loss_func(target)\n\n        total_loss = content_loss + style_loss + tv_loss\n\n    optimizer.zero_grad(set_to_none=True)\n    total_loss.backward()  # do I need to `retain_graph=True`?\n\n    optimizer.step()\n\n    pbar.set_postfix_str(\n        f\"total_loss={total_loss.item():.2f} \"\n        f\"content_loss={content_loss.item():.2f} \"\n        f\"style_loss={style_loss.item():.2f} \"\n        f\"tv_loss={tv_loss.item():.2f} \"\n    )\n\n100%|██████████| 5000/5000 [01:30&lt;00:00, 55.36it/s, total_loss=43.90 content_loss=8.71 style_loss=29.08 tv_loss=6.11 ]     \n\n\nAs mentioned before, images need to be denormalized (i.e. reverted back) to correct colors. After that we compare content, style and target images side-by-side.\n\nclass InverseNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, x: Tensor) -&gt; Tensor:\n        for t, m, s in zip(x, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return x\n\n\nclass Clip(object):\n    def __init__(self, vmin: float = 0.0, vmax: float = 1.0):\n        self.vmin = vmin\n        self.vmax = vmax\n\n    def __call__(self, x):\n        return torch.clamp(x, self.vmin, self.vmax)\n\n\ninv_transform_preview = T.Compose([\n    InverseNormalize(IMAGENET_MEAN, IMAGENET_STD),\n    T.Resize(IMG_SIZE, antialias=True),\n    T.CenterCrop((IMG_SIZE, IMG_SIZE)),\n    Clip(),\n])\n\nimgs = [\n    inv_transform_preview(i.detach().squeeze().cpu())\n    for i in (content, style, target)\n]\n\ngrid = make_grid(imgs)\n\ndef show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n\n    fig, axs = plt.subplots(ncols=len(imgs), figsize=(21, 7), squeeze=False, dpi=92, tight_layout=True, frameon=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = VF.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\n\nshow(grid)\n\n\n\n\nSuccessfuly applied neural style transfer. The content image (left), the style image (center), and final target image (right)."
  },
  {
    "objectID": "posts/2023-09-15-neural-style-transfer/index.html#examples",
    "href": "posts/2023-09-15-neural-style-transfer/index.html#examples",
    "title": "Neural Style Transfer",
    "section": "Examples",
    "text": "Examples\nA few cherry-picked examples of style transfer:"
  }
]