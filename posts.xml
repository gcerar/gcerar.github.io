<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Greg&#39;s blog</title>
<link>gcerar.github.io/posts.html</link>
<atom:link href="gcerar.github.io/posts.xml" rel="self" type="application/rss+xml"/>
<description>Greg&#39;s personal blog</description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Mon, 20 Nov 2023 00:00:00 GMT</lastBuildDate>
<item>
  <title>Research Compute Infrastructure</title>
  <dc:creator>Gregor Cerar</dc:creator>
  <link>gcerar.github.io/posts/2023-11-20-research-compute-infrastructure/index.html</link>
  <description><![CDATA[ 



<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Recent technological advances have transformed education, elevating the quality of teaching and learning. Jupyter Notebooks emerge as a leading tool for interactive computing, programming, and data analysis <span class="citation" data-cites="perkel2018jupyter mendez2019toward granger2021storytelling">(Perkel 2018; Mendez et al. 2019; Granger et al. 2021)</span>. However, hardware limitations became a significant hurdle when handling larger research projects. While public cloud services are an option, they come with notable drawbacks. In response, we developed a private cloud solution for our lab using Kubernetes. This solution addresses cost and security concerns while ensuring adaptability. Through this technology, we have enabled efficient app management, scalability, and resource flexibility.</p>
<section id="jupyter-notebooks" class="level2">
<h2 class="anchored" data-anchor-id="jupyter-notebooks">Jupyter Notebooks</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Project_Jupyter">Jupyter Notebook</a> is an open document format based on JSON<sup>1</sup>. Notebooks are organized into a sequence of cells, with each cell containing code, descriptive text, equations, and rich outputs (<em>e.g.</em>, text displays, tables, audio, images, and animations). Tools like JupyterLab provide a platform for interactive code execution, data analysis, and documentation, all within a single interface, culminating in a Jupyter Notebook. These notebooks support various programming languages (<em>e.g.</em>, Python, R, Scala, C++) and allow users to write and execute code cells iteratively (using REPL<sup>2</sup> or WETL<sup>3</sup> approaches), offering immediate visibility of intermediate results. This facilitates the creation of narrative-driven data analyses, educational materials, and interactive presentations. Due to their versatility and interactivity, Jupyter Notebooks are a robust teaching tool for learning, conducting data science, and computer research.</p>
<p>Because of these remarkable features, our research lab decided to incorporate Jupyter Notebooks into our research lab’s educational and research processes. By doing this, we encourage students and researchers to document their projects and easily share their research results.</p>
</section>
<section id="scalability" class="level2">
<h2 class="anchored" data-anchor-id="scalability">Scalability</h2>
<p>However, for large-scale projects involving hefty data processing on personal computers, using Jupyter Notebooks becomes a significant challenge. We frequently run into hardware limitations like storage space, RAM, processing power, and access to computational accelerators, which can hinder or even halt our progress. These projects are typically in the early stages of research, analysis, or prototyping, so intensive optimizations are impractical because they can slow down experimental development. Two potential solutions emerge: running Jupyter Notebooks on the grid, HPC infrastructure, or cloud services.</p>
<p>HPC infrastructure, like <a href="https://sling.si">SLING</a> in Slovenia or <a href="https://eurohpc-ju.europa.eu">EuroHPC</a> on a European level, offers immense computational power. However, given that HPCs are significant investments, queue management solutions like SLURM are employed in the HPC world to optimize their use. Computation tasks must be pre-packaged with metadata, code, and input data. These tasks then join a waiting list. This approach is not aligned well with data-driven research, which aims for interactive programming and quick feedback, limiting the full utilization of Jupyter Notebooks. Hence, cloud services become a more common choice for these notebooks.</p>
<p>Public cloud platforms like Google <a href="https://colab.research.google.com">Colab</a> and <a href="https://www.kaggle.com">Kaggle</a> have popularized Jupyter Notebook usage. Users can access the service anytime without queues, edit notebooks, and utilize cloud computing resources, all via a browser. Both services are freely accessible in a limited version. However, due to high user demand, these platforms sometimes limit computational resources, affecting service quality. Alternatives include custom paid services in the public cloud (e.g., <a href="https://aws.amazon.com">AWS</a>, <a href="https://azure.microsoft.com/en-us">Azure</a>, <a href="https://cloud.google.com">GCP</a>, <a href="https://alibabacloud.com">Alibaba Cloud</a>), tailoring infrastructure to customer needs. However, public cloud services have drawbacks, including high rental costs, unpredictable market-affected expenses, and security concerns when handling sensitive data.</p>
<p>Private clouds are an alternative to the public cloud, addressing cost and security challenges. They are crucial for research labs and companies dealing with sensitive data or requiring high adaptability. It grants organizations more transparency and cost control based on their needs and capabilities. Despite the initial technical knowledge and infrastructure investment requirements, private clouds offer enhanced security, control, and flexibility, leading to more predictable costs in the long run.</p>
<p>Several technologies are available to set up a private cloud, including commercial options (<em>e.g.,</em> VMware <a href="https://www.vmware.com/products/vsphere.html">vSphere</a>, Red Hat <a href="https://www.redhat.com/en/technologies/cloud-computing/openshift">OpenShift</a>, IBM <a href="https://www.ibm.com/docs/en/cloud-private/3.2.x?topic=started-cloud-private-overview">Cloud Private</a>) and open-source solutions (<em>e.g.,</em> <a href="https://www.openstack.org">OpenStack</a>, <a href="https://www.eucalyptus.cloud">Eucalyptus</a>, <a href="https://kubernetes.io">Kubernetes</a>). Among the open-source options, Kubernetes is <a href="https://trends.google.com/trends/explore?cat=5&amp;q=%2Fg%2F11b7lxp79d,%2Fm%2F0cm87w_,%2Fm%2F0cnx0mm&amp;hl=en-US">the most popular</a>.</p>
<p><a href="https://kubernetes.io">Kubernetes</a> (abbreviated as K8s) is an open-source platform designed for the automation, management, and deployment of applications within containers. Its advanced orchestration features allow for efficient application management, automatic scaling, monitoring of their performance, and high availability. It can simplify the development and maintenance of complex cloud-based applications.</p>
<p>Contrary to <a href="https://www.docker.com/">Docker</a> and <a href="https://docs.docker.com/compose/">docker-compose</a>, which primarily focus on building, storing, and running individual containers, Kubernetes offers a much more comprehensive platform for managing containers across expansive environments that span multiple computing nodes. While Docker provides easy creation and operation of individual containers, and docker-compose allows defining multiple containers as application units, Kubernetes facilitates the management of entire clusters of these application units throughout their life cycle, which includes automatic deployment, dynamic adjustments based on load, recovery in case of errors, and more advanced service and network management.</p>
<p>In our research laboratory, due to the growing computational demands prevalent in data science and the desire to retain the recognizable workflow present in Jupyter notebooks, we have developed our private cloud solution based on Kubernetes technology.</p>
<p>The following sections will present a private cloud setup featuring Jupyter notebooks built on top of open-source solutions. The user experience closely resembles that of existing paid cloud services. The private cloud must meet the following requirements:</p>
<ul>
<li><p><strong>System Scalability:</strong> The cloud should allow for easily adding computing nodes to the cluster without disrupting the operational system, supporting larger research projects or teaching groups.</p></li>
<li><p><strong>Efficient Resource Management:</strong> The system must enable precise allocation of resources to users. In this context, an administrator can define a balance between a lax and strict resource allocation policy.</p></li>
<li><p><strong>Enhanced Collaboration Experience:</strong> The system should allow for straightforward sharing of Jupyter notebooks among users, promoting collaboration on joint projects and idea exchange between researchers and students.</p></li>
<li><p><strong>No Waiting Queues:</strong> The system should eliminate waiting queues, offering users immediate access to computational resources to the best of their capacity.</p></li>
</ul>
</section>
</section>
<section id="architecture" class="level1">
<h1>Architecture</h1>
<p>We decided to base our private cloud on the Kubernetes platform to meet system scalability and resource management requirements, aiming to enhance the functionality, accessibility, and sharing of Jupyter notebooks <span class="citation" data-cites="bussonnier2018jupyterhub">(Bussonnier 2018)</span> within the Kubernetes private cloud. In this section, will delve deeper into the system’s architecture that integrates services and elaborate on the design decisions. Subsequently, we describe the individual services within our infrastructure.</p>
<div id="tbl-stack" class="anchored">
<table class="table-hover table">
<caption>Table&nbsp;1: Services and Used Technologies.</caption>
<thead>
<tr class="header">
<th>Service</th>
<th>Solutions (<strong>Used in bold</strong>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Turnkey solution?</td>
<td><strong>Custom</strong>, NVIDIA DeepOps</td>
</tr>
<tr class="even">
<td><strong>Basic Infrastructure</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>Operating System</td>
<td><strong>Ubuntu</strong>, RHEL, NixOS, Talos</td>
</tr>
<tr class="even">
<td>Data Storage</td>
<td><strong>ZFS</strong>, GlusterFS, Lustre, CEPH, iSCSI</td>
</tr>
<tr class="odd">
<td>System Management</td>
<td><strong>Ansible</strong>, Terraform, Puppet, Chef</td>
</tr>
<tr class="even">
<td><strong>Internal Services</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>K8s Distribution</td>
<td><strong>vanilla</strong>, MicroK8s, OpenShift, Rancher</td>
</tr>
<tr class="even">
<td>K8s Installation</td>
<td><strong>Helm</strong>, Kustomize</td>
</tr>
<tr class="odd">
<td>Network Manager</td>
<td><strong>Calico</strong>, Canal, Flannel, Weave</td>
</tr>
<tr class="even">
<td>Data Manager</td>
<td><strong>csi-driver-nfs</strong>, Rook, OpenEBS</td>
</tr>
<tr class="odd">
<td>Traffic Balancing</td>
<td><strong>MetalLB</strong>, cloud provider specific</td>
</tr>
<tr class="even">
<td>Traffic Manager</td>
<td><strong>Nginx</strong>, Traefik</td>
</tr>
<tr class="odd">
<td>GPU Manager</td>
<td><strong>NVIDIA GPU-Operator</strong></td>
</tr>
<tr class="even">
<td><strong>Services for Users</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>JupyterHub Manager</td>
<td><strong>Z2JH (Zero-to-JupyterHub)</strong></td>
</tr>
<tr class="even">
<td>Metrics and Monitoring</td>
<td><strong>kube-prometheus-stack</strong>, InfluxDB</td>
</tr>
</tbody>
</table>
</div>
<p>Table&nbsp;1, in its first column, lists all the services required for system operation. The second column lists the open-source solutions that can provide these services. Bolded services indicate those selected and used in our private cloud. We made our choices based on specific criteria. We first surveyed technologies and solutions utilized in related projects. We further narrowed our selection to open-source solutions tested in private clouds on native infrastructure. A significant factor in our decision-making was also an insight into the popularity of the projects, gauged by the number of stars in repositories, the number of forks of the project, and the level of development activity on GitHub/GitLab. In our decision-making process, we didn’t follow a single empirical metric but took multiple factors into account to ensure a comprehensive assessment of solutions.</p>
<div id="fig-architecture" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-11-20-research-compute-infrastructure/figures/over10k-arch.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: A three-tier logical infrastructure diagram. At the bottom is the foundational infrastructure, followed by internal Kubernetes services in the middle, and on top are the services exposed to users.</figcaption>
</figure>
</div>
<p>The diagram in Figure&nbsp;1 provides a high-level representation of our private cloud and its infrastructure across three levels. The first level comprises heterogeneous computing nodes, forming the foundational infrastructure. Each node operates its operating system, running a portion of the Kubernetes platform. The second level encompasses internal Kubernetes services, which are essential for operation and never directly accessed by users. The final third level includes the services availed by end-users.</p>
<section id="turnkey-solution" class="level2">
<h2 class="anchored" data-anchor-id="turnkey-solution">Turnkey Solution?</h2>
<p>When planning the private cloud, we initially explored turnkey solutions, including NVIDIA <a href="https://github.com/NVIDIA/deepops">DeepOps</a>. Despite its advantages, we built our custom solution for the following reasons. While DeepOps is an excellent turnkey solution with maintained source code on GitHub and offers commercial support, initial setup requires configuration file adjustments, including Ansible scripts for automated (re)configuration of installed Linux distribution. Its complexity discouraged us, further investing our time into tinkering with it.</p>
<p>One of our biggest concerns was the intricate solution that tries to be versatile and “simple”. However, this inevitably leads to hiding functionalities and, in case of issues, jumping around documentation of multiple unrelated internally used tools. Despite proclaimed simplicity, troubleshooting or upgrade problems require manual intervention, where a thorough understanding of Linux, DeepOps, its internal tooling, and their interactions is necessary for system control. Therefore, we decided to start with a minimalist solution and, over time, plan to expand the system to understand the infrastructure’s operation better.</p>
</section>
<section id="foundation-infrastructure" class="level2">
<h2 class="anchored" data-anchor-id="foundation-infrastructure">Foundation Infrastructure</h2>
<p>In this section, we discuss the foundation infrastructure of our private cloud solution. We’ll go through these building blocks, including the selection of container management tools and resource sharing, which are vital for the operation of the Kubernetes platform.</p>
<p><strong>Operating System:</strong> We chose Ubuntu Server based on the Debian Linux distribution for our system. The advantage of widely used Debian-based Linux distributions is the abundance of available knowledge resources and support, making problem-solving more accessible. Among alternatives, like declarative binary reproducible <a href="https://nixos.org">NixOS</a> and RHEL-based distributions, we also considered the <a href="https://www.talos.dev">Talos</a> distribution specialized for Kubernetes. However, we preferred to stick with Ubuntu Server due to the Talos project’s novelty and associated risks.</p>
<p><strong>Container Management:</strong> For container management, we selected <a href="https://containerd.io">ContainerD</a>, also used in the DeepOps solution and officially supported by NVIDIA. It is an open-source tool that implements the CRI interface for communication between the operating system and Kubernetes for efficient and reliable container management.</p>
<p><strong>Data Storage:</strong> For data storage, we chose <a href="https://en.wikipedia.org/wiki/OpenZFS">ZFS</a>, which resides on one of the nodes. Although solutions like <a href="https://en.wikipedia.org/wiki/Apache_Hadoop#HDFS">HDFS</a>, <a href="https://www.gluster.org/">Gluster</a>, <a href="https://www.lustre.org/">Lustre</a>, or <a href="https://ceph.io/en/">Ceph</a> are far more common in the HPC world, they require dedicated infrastructure and tools to offer features offered by ZFS out-of-the-box. Features include checkpoints, data deduplication, compression, a COW (copy-on-write) system to prevent data loss during writing, immunity to silent bit-rot, the ability to use disks as redundancy for mechanical failures, and the use of fast SSD devices as a cache. It also allows easy manual intervention in the event of incidents. However, at the time of writing, ZFS does not stretch across multiple nodes, posing a risk of cluster failure in case of a data-storing node’s malfunction (single point of failure). There is an ongoing effort to implement ZFS’ distributed RAID (dRAID) [<a href="https://github.com/openzfs/zfs/commit/b2255edcc0099e62ad46a3dd9d64537663c6aee3">src</a>].</p>
<p>To access ZFS storage from Kubernetes, we used the NFS server, which is part of the Linux kernel. We chose <a href="https://en.wikipedia.org/wiki/Network_File_System">NFS</a> because it is one of the few methods that allow multiple containers to bind to the same mounting point (see <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">table</a>).</p>
<p><strong>System Management:</strong> For remote management and node configuration, we use <a href="https://www.ansible.com/">Ansible</a> maintained by Red Hat. We selected it due to its prevalence in other significant open-source projects and positive experiences from past projects.</p>
</section>
<section id="kubernetes" class="level2">
<h2 class="anchored" data-anchor-id="kubernetes">Kubernetes</h2>
<p>In Kubernetes, everything operates as a service. These services provide various functionalities that enhance Kubernetes capabilities, such as storage access, CPU and GPU allocation, traffic management, and connecting services within a mesh network.</p>
<p>To support specific functionalities, appropriate services (much like operating system drivers) must be installed. These specialized services, often called “operators” in Kubernetes terminology [<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">src</a>], are essential. They not only deploy and manage functionalities but also respond to issues. Operators enhance Kubernetes by interfacing with standardized and version-controlled APIs.</p>
<p>Put simply, operators can be viewed as one or more Docker containers. They function as an intermediary layer, bridging the gap between the underlying operating system and Kubernetes APIs.</p>
<section id="internal-services" class="level3">
<h3 class="anchored" data-anchor-id="internal-services">Internal Services</h3>
<p>In Kubernetes, internal services are not intended for end users but are crucial for the system’s operation. These services operate in the background, ensuring vital functionalities that enable the stable operation and management of the container environment. In this subsection, we will introduce key services within Kubernetes and explain their role in our infrastructure. We will describe each service’s primary functionality and examine alternatives we explored in making our decision.</p>
<p><strong>Kubernetes Distribution:</strong> When choosing a Kubernetes distribution, we examined three options: Canonical <a href="https://microk8s.io">MicroK8s</a>, Red Hat OpenShift, and the basic “vanilla” Kubernetes distribution. “Vanilla” Kubernetes represents the unaltered version directly available in Google’s repository, without pre-installed applications or plugins. We went for the vanilla version as it provides flexibility and freedom of choice of the extensions.</p>
<p><a href="https://microk8s.io">MicroK8s</a> is an excellent solution for quick experimentation and setting up the system on smaller devices with limited resources (<em>e.g.,</em> Raspberry Pi). However, it has many pre-installed applications and uses Canonical’s Snap packaging system, which can complicate adjusting configuration files and accessing external services, such as the NFS server.</p>
<p>We ruled out <a href="https://en.wikipedia.org/wiki/OpenShift">OpenShift</a> due to the complexity of managing security profiles that, for our use case, were excessive, requiring substantial effort to implement these profiles for each service. Therefore, we opted for the basic “vanilla” Kubernetes distribution, offering more flexible and straightforward customization tailored to our needs.</p>
<p><strong>Kubernetes Package Deployment:</strong> To describe the implementation of services in Kubernetes, a straightforward approach is to write YAML configuration file(s) (also called manifesto), which are then forwarded to Kubernetes via the command line. However, some services can be quite complex, leading developers to create service packages, making services more general-purpose and customizable through parameters. The most widespread packaging system is <a href="https://helm.sh/">Helm</a>, allowing for more portable and adaptable service packages. Helm uses YAML files as templates (much like forms), which are then filled out based on the provided parameters and sent to Kubernetes.</p>
<p><strong>Network Operator:</strong> Kubernetes services must be interconnected to communicate with other services. We opted for the open-source Tigera <a href="https://github.com/tigera/operator">Calico</a> operator to manage interconnections. Given its prevalence and functionalities, we found it the most suitable solution.</p>
<p>Calico and <a href="https://github.com/flannel-io/flannel">Flannel</a> are the most common solutions for network operators. Flannel is more minimalistic and operates as a network switch (layer 2) using technologies like <a href="https://www.openvswitch.org/">Open vSwitch</a> or <a href="https://en.wikipedia.org/wiki/Virtual_Extensible_LAN">VXLAN</a>. In contrast, Calico routes traffic like a network router (layer 3). Especially in cases of multi-cluster (<em>i.e.,</em> multiple physical locations) or hybrid cloud services, Calico emerges as a better choice.</p>
<p><strong>Storage Operator:</strong> For effective storage management within the Kubernetes system, we used <a href="https://github.com/kubernetes-csi/csi-driver-nfs">csi-driver-nfs</a>. It allows us to use the already established NFS servers. With it, we ensure uninterrupted access to persistent storage for any service within our private cloud.</p>
<p>The <em>csi-driver-nfs</em> proved most suitable since we already had an NFS server on one of the nodes. It allows us straightforward and centralized storage management for all services within Kubernetes. Centralization brings about numerous advantages, yet also challenges. Among the latter is the system’s vulnerability during a potential outage of the node storing the data. Nonetheless, centralization facilitates easier troubleshooting and backup execution.</p>
<p><strong>Bare-Metal Ingress Load-Balancer:</strong> To ensure balanced ingress (of incoming) traffic among entry points in our Kubernetes cluster, we decided to utilize the <a href="https://metallb.universe.tf">MetalLB</a> solution. After thorough research, we could not find any other alternative. Most of the online documentation (<em>e.g.,</em> tutorials, blogs) focuses on setting up infrastructure on public clouds such as AWS or Azure and using solutions tailored to the demands of public cloud providers. However, since our infrastructure is based on our hardware (<em>i.e.,</em> bare-metal), we opted for MetalLB, which has proven reliable and effective in routing traffic among our Kubernetes cluster’s entry points.</p>
<p><strong>Ingress Operator:</strong> While a network operator manages interconnection between services within Kubernetes, the ingress operator manages access to services from the outside world. For security reasons, direct access to the internal network is prohibited. While it is possible to enter the internal network through a proxy (<em>i.e.,</em> <code>kubectl proxy</code>), that’s meant only for debugging purposes. The ingress operator is designed to resolve domain names and route traffic to the correct container and port, which we described in the service’s YAML manifesto. Using domain name resolution has several advantages. Regardless of the service’s internal IP address, the ingress operator will always correctly direct traffic. The ingress operator can act as a load balancer when there is a high-traffic load, balancing traffic between multiple copies of service.</p>
<p>Among the most common solutions for ingress traffic management are <a href="https://github.com/nginxinc/nginx-ingress-helm-operator">NGINX</a> and <a href="https://doc.traefik.io/traefik/providers/kubernetes-ingress/">Traefik</a> Ingress Operator. We chose NGINX, but the operators’ interface is standardized, so there are almost no differences between the solutions. Regardless of the selected solution, once a new service is deployed, the operator will follow the service’s manifesto and automatically route traffic to the appropriate container.</p>
<p><strong>GPU Operator:</strong> For efficient management of access to computing accelerators, we decided to use the official NVIDIA <a href="https://github.com/NVIDIA/gpu-operator">GPU-Operator</a> suite of services. This suite provides two distinct installation options for NVIDIA drivers. The first option leverages host drivers, while the second involves drivers packaged within containers. Initially, we opted for the first option, wanting to enable the use of accelerators outside the Kubernetes framework. However, due to issues with conflicting driver versions, we decided to utilize the drivers provided by the GPU-Operator.</p>
</section>
<section id="user-services" class="level3">
<h3 class="anchored" data-anchor-id="user-services">User Services</h3>
<p>In this section, we introduce the selected services available to end users of our private cloud, enabling efficient execution and management of their research and educational projects.</p>
<p><strong>JupyterHub</strong> represents one of the key services in our private cloud, providing users with easy access to computing resources, data, and Jupyter notebooks for research and teaching purposes. To implement JupyterHub, we use the <a href="https://z2jh.jupyter.org/en/stable/">Z2JH</a> (Zero-to-JupyterHub) implementation, developed by a team of researchers at the University of Berkeley in collaboration with the Jupyter community. This solution facilitates quick setup and maintenance.</p>
<p>Every individual user is granted access to an isolated container instance via their username and password or <a href="https://en.wikipedia.org/wiki/OAuth">OAuth</a> provider, such as GitHub, Google, or OAuth0. An isolated instance offers a stripped-down Linux environment with limited internet access and without admin permissions. Kubernetes then ensures access to shared data resources, common directories, and the use of computational accelerators.</p>
<p>The JupyterHub user interface is similar to Google Colab or Kaggle services. Upon entering the isolated instance, JupyterLab is already running, and the user also has access to the Linux terminal. Additional tools and software packages can be installed using <a href="https://pip.pypa.io/en/stable/">pip</a>, <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/packages.html">conda</a>, or <a href="https://github.com/mamba-org/mamba">mamba</a> commands.</p>
<p><strong>Grafana</strong> is a key service in our private cloud, facilitating a straightforward display of the current workload of the compute cluster and the availability of computational accelerators. This data visualization platform allows users to present information clearly and transparently, aiding them in making decisions regarding resource usage and optimizing their tasks. Utilizing Grafana ensures efficient and transparent resource monitoring, enhancing user experience. Data collection (Prometheus) and visualization (Grafana) are deployed by <a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack">kube-prometheus-stack</a>.</p>
</section>
</section>
</section>
<section id="deployment" class="level1">
<h1>Deployment</h1>
<p>In this section, we’ll present how we deployed our computing infrastructure. First, I’ll summarize the hardware decisions, caveats, and finally, the user experience with some screenshots.</p>
<section id="hardware" class="level2">
<h2 class="anchored" data-anchor-id="hardware">Hardware</h2>
<div id="tbl-hardware" class="anchored">
<table class="table-hover table">
<caption>Table&nbsp;2: Hardware specifications of Computing node.</caption>
<thead>
<tr class="header">
<th>Hardware</th>
<th>Specifications</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Chasis</td>
<td>Supermicro A+ Server <a href="https://www.supermicro.com/en/aplus/system/4u/4124/as-4124gs-tnr.cfm">4124GS-TNR</a>, 4U size, up to PCI-E 8 GPUs</td>
</tr>
<tr class="even">
<td>CPU</td>
<td>2x AMD <a href="https://www.amd.com/en/products/cpu/amd-epyc-75f3">EPYC 75F3</a> (32C/64T, up to 4GHz, 256MB L3 cache)</td>
</tr>
<tr class="odd">
<td>Memory</td>
<td>1TB (16x64GB) REG ECC DDR4, 3200MHz</td>
</tr>
<tr class="even">
<td>System</td>
<td>2x 2TB SSD NVMe, software RAID1 (mirror)</td>
</tr>
<tr class="odd">
<td>Storage</td>
<td>6x 8TB SSD SATA, software RAIDZ1 (1 disk redundacy)</td>
</tr>
<tr class="even">
<td>GPU</td>
<td>2x NVIDIA A100 80GB PCI-E</td>
</tr>
</tbody>
</table>
</div>
<p>When we bought the hardware in early 2022, we chose third-generation AMD EPYC processors. Specifically, we went for the F-series, which has higher base and turbo frequencies — up to 4GHz — at the cost of fewer cores. We picked a CPU with the highest available TDP of 280W. We installed server-grade registered error-correcting memory at the highest frequency supported by the processor and populated all eight channels on both processors. Sixteen sticks of RAM in total. Although we considered solutions from Intel, AMD EPYC processors had better price-to-performance ratios.</p>
<p>From the perspective of numerical performance, our significant concern was Intel-optimized libraries, such as Intel MKL, often found in numerical tools. The library has a “bug” that causes non-Intel processors to utilize a slower SSE instead of more advanced AVX vectorization instructions [<a href="https://danieldk.eu/Posts/2020-08-31-MKL-Zen.html">src</a>]. <a href="https://www.openblas.net/">OpenBLAS</a> is a good alternative but requires some effort to install it. See Anaconda <a href="https://github.com/conda-forge/nomkl-feedstock">no-mkl</a> package.</p>
<p>We chose two NVMe drives configured in the mirror configuration (RAID1) for the system drive. We selected six 8TB SSD SATA drives configured in ZFS RAIDZ1 for data storage, which has one drive redundancy. We also chose two A100 GPUs as accelerators.</p>
<p>NVIDIA A100 GPUs come in two form factors: PCI-E and SXM4. The SXM4 proprietary form factor has a higher TDP and high-bandwidth NVLink interconnections between every GPU through NVSwitch hardware. The downside of SXM4 is that it will only support Ampere generation GPUs and require a special motherboard. The PCI-E variant has a lower TDP, and NVLink can only be across two GPUs. However, we decided against vendor lock-in, limiting ourselves to one brand and generation, and went with the PCI-E variant.</p>
<p>We considered the most likely workflow scenarios. We expected most communication to be CPU-to-GPU, with GPUs sliced into several instances. Once GPU slicing is enabled, the NVIDIA drivers disable NVLink, rendering NVLink connections useless. We can change GPU slicing strategy freely.</p>
<div id="fig-hardware" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-11-20-research-compute-infrastructure/figures/hardware-top-view-resize.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: The computing node on my desk underwent final checks before being installed in the server rack.</figcaption>
</figure>
</div>
</section>
<section id="user-experience" class="level2">
<h2 class="anchored" data-anchor-id="user-experience">User Experience</h2>
<p>After deploying the hardware and software stack, we conducted a month-long live test to stabilize the configuration. During this period, users were informed that we might reboot the system or make significant changes without responsibility for any potential data loss, though we aimed to minimize such occurrences.</p>
<p>We made two key decisions about resource allocation. Users can utilize all available memory and CPU cores. When CPU demand is high, Kubernetes and the operating system manage the scheduling of tasks. In cases of high memory usage, the job consuming the most memory is terminated to protect other running tasks.</p>
<p>Feedback from students and researchers was overwhelmingly positive, highlighting the high speed, numerous cores, ample memory, and dedicated GPU access without interference.</p>
<p>During the testing phase, “testers” identified several issues, which were promptly addressed. These included adding a shared folder with datasets and Jupyter notebooks, shared package cache, and better persistence of running tasks in JupyterLab.</p>
<section id="jupyterhub" class="level3">
<h3 class="anchored" data-anchor-id="jupyterhub">JupyterHub</h3>
<p>JupyterHub has become a crucial component of our research infrastructure, enhancing our workflow significantly. Its smooth integration was largely due to the interface and functionality of JupyterHub, which closely resemble the tools our researchers and students were familiar with. This similarity played a key role in its quick adoption and high user satisfaction.</p>
<div id="fig-jhub-profiles" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-11-20-research-compute-infrastructure/figures/jhub-profiles.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: JupyterHub offers a list of predefined containers, where some of them offer a GPU instance.</figcaption>
</figure>
</div>
<p>Upon logging into JupyterHub, users are presented with a list of predefined containers (as shown in Figure&nbsp;3). Our recent update includes several options:</p>
<ul>
<li>A basic minimal working environment environment.</li>
<li>A comprehensive data science environment equipped with multiple packages and support for Python, R, and Julia.</li>
<li>A selection of containers offering GPU instances.</li>
</ul>
<p>The development environment greets users with a layout similar to modern IDEs, featuring a file explorer on the left and code editor tabs on the right (see Figure&nbsp;4).</p>
<div id="fig-jhub-workspace" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-11-20-research-compute-infrastructure/figures/jhub-workspace.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: JupyterLab workspace with familiar layout: a file explorer on the left and code editor tabs on the right.</figcaption>
</figure>
</div>
</section>
<section id="grafana" class="level3">
<h3 class="anchored" data-anchor-id="grafana">Grafana</h3>
<p>For transparent insight into infrastructure availability, the user has read-only access to the Grafana dashboard (Figure~<img src="https://latex.codecogs.com/png.latex?%5Cref%7Bfig:grafana:dashboard%7D">). Here, they can observe most of the infrastructure metrics.</p>
<p>For transparent insight into infrastructure availability, the user has read-only access to the Grafana dashboard. Dashboard visualizes computing resource utilization including metrics like total and per-container CPU usage, memory usage per container, GPU utilization, temperature readings, and storage I/O (see Figure&nbsp;5).</p>
<div id="fig-grafana-dashboard" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-11-20-research-compute-infrastructure/figures/grafana-censored.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Visualization of computing cluster utilization showing total and per-container CPU utilization, per-container memory utilization, GPU slices utilization, temperatures, and storage I/O.</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>This article introduced our private cloud solution based on Kubernetes technology. This solution offers a scalable environment for using Jupyter notebooks, an effective educational tool for data-driven narrative analysis, creating learning materials, and interactive presentations. Additionally, the system allows for the concurrent sharing of computing resources, significantly enhancing the utilization of our entire infrastructure.</p>
<p>The JupyterHub service on the Kubernetes platform facilitates easy access to the work environment and ensures user isolation, allowing for uninterrupted work and research. Users benefit from storage space and shared folders for file sharing, promoting collaboration and teamwork. Users also have access to computational accelerators when available.</p>
<p>We discuss our solution’s key components, architecture, and design decisions, revealing the technology choices that led to efficient operation and an exceptional user experience. Our focus has been on open-source platforms that have proven reliable and effective in our environment. As the core platform, Kubernetes enables scalable container management and high availability, while JupyterHub provides easy access to services and simplifies user management.</p>
<p>We plan to enhance our solution with additional services and technologies to improve user experience and increase our cloud’s performance. We remain open to new technologies and approaches that contribute to the better functioning of our private cloud solution for research and education. Data from Prometheus will be crucial for analyzing infrastructure utilization and understanding the extent of user competition for computing resources.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bussonnier2018jupyterhub" class="csl-entry">
Bussonnier, Matthias. 2018. <span>“<span class="nocase">Jupyter and HPC: Current state and future roadmap</span>.”</span> <em>Exascale Computing Project</em>. <a href="https://www.exascaleproject.org/event/jupyter/">https://www.exascaleproject.org/event/jupyter/</a>.
</div>
<div id="ref-granger2021storytelling" class="csl-entry">
Granger, Brian E. et al. 2021. <span>“<span class="nocase">Jupyter: Thinking and Storytelling With Code and Data</span>.”</span> <em>Computing in Science &amp; Engineering</em> 23 (2): 7–14. <a href="https://doi.org/10.1109/MCSE.2021.3059263">https://doi.org/10.1109/MCSE.2021.3059263</a>.
</div>
<div id="ref-mendez2019toward" class="csl-entry">
Mendez, Kevin M et al. 2019. <span>“<span class="nocase">Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing</span>.”</span> <em>Metabolomics</em> 15 (10): 1–16.
</div>
<div id="ref-perkel2018jupyter" class="csl-entry">
Perkel, Jeffrey M. 2018. <span>“<span class="nocase">Why Jupyter is data scientists’ computational notebook of choice</span>.”</span> <em>Nature</em> 563 (7732): 145–47.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>JSON: JavaScript Object Notation↩︎</p></li>
<li id="fn2"><p>REPL: read–eval–print loop↩︎</p></li>
<li id="fn3"><p>WETL: write-eval-think-loop↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>compute</category>
  <category>infrastructure</category>
  <guid>gcerar.github.io/posts/2023-11-20-research-compute-infrastructure/index.html</guid>
  <pubDate>Mon, 20 Nov 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Generative Adversarial Networks</title>
  <dc:creator>Gregor Cerar</dc:creator>
  <link>gcerar.github.io/posts/2023-10-10-vanilla-GANs/index.html</link>
  <description><![CDATA[ 



<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Generative Adversarial Networks (GANs) are an innovative class of unsupervised neural networks that have revolutionized the field of artificial intelligence. They were first introduced in <a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a> <span class="citation" data-cites="goodfellow2014generative">(Goodfellow et al. 2014)</span> and consist of two separate neural networks: the <strong>generator</strong> (creates data) and the <strong>discriminator</strong> (evaluates data authenticity). The generator aims to fool the discriminator by producing realistic data, while the discriminator tries to differentiate real from fake. Over iterations, the generator’s data becomes more convincing.</p>
<p>As an analogy, consider two kids, one drawing counterfeit money (“Generator”) and another assessing its realism (“Discriminator”). Over time, the counterfeit drawings become increasingly convincing.</p>
</section>
<section id="vanilla-gan" class="level1 page-columns page-full">
<h1>Vanilla GAN</h1>
<p>The most fundamental variant of GAN is the “vanilla” GAN, where “vanilla” signifies the model in its original and most straightforward form rather than a flavor. To better understand its mechanism, I’ve illustrated its structure on Figure&nbsp;1.</p>
<div id="fig-architecture" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-10-10-vanilla-GANs/figures/GAN-architecture.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: GAN architecture</figcaption>
</figure>
</div>
<ul>
<li><strong>Generator</strong> <img src="https://latex.codecogs.com/png.latex?G(z;%20w_g)"> takes random noise <img src="https://latex.codecogs.com/png.latex?z"> as input and produces fabricated data <img src="https://latex.codecogs.com/png.latex?x_f">.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?z"> represents the input vector, a noise vector from the Gaussian distribution.</li>
<li><img src="https://latex.codecogs.com/png.latex?w_g"> denotes generator neural network weights.</li>
<li><img src="https://latex.codecogs.com/png.latex?x_f"> is a fabricated data sample meant for the discriminator.</li>
</ul></li>
<li><strong>Discriminator</strong> <img src="https://latex.codecogs.com/png.latex?D(x;%20w_d)"> differentiates between real and generated data.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?x"> represents input vectors, which come from either a real dataset (<img src="https://latex.codecogs.com/png.latex?x_r%20%5Csim%20p_%5Ctextrm%7Bdata%7D(x)">) or from the set of fabricated samples (<img src="https://latex.codecogs.com/png.latex?x_f%20=%20G(z%20%5Csim%20p_z(z);%20w_g)">).</li>
<li><img src="https://latex.codecogs.com/png.latex?w_d"> denodes discriminator neural network weights.</li>
</ul></li>
</ul>
<section id="objective-function" class="level2">
<h2 class="anchored" data-anchor-id="objective-function">Objective Function</h2>
<p>The interaction between the Generator and the Discriminator can be quantified by their objective or loss functions:</p>
<ul>
<li><strong>Discriminator’s Objective:</strong> For real data <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?D"> wants <img src="https://latex.codecogs.com/png.latex?D(x)"> near <img src="https://latex.codecogs.com/png.latex?1">. For generated data <img src="https://latex.codecogs.com/png.latex?G(z)">, it targets <img src="https://latex.codecogs.com/png.latex?D(G(z))"> close to <img src="https://latex.codecogs.com/png.latex?0">. Its objective is:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D(D)%20=%20%5Clog(D(x))%20+%20%5Clog(1%20-%20D(G(z))).%0A"></p>
<ul>
<li><strong>Generator’s Objective:</strong> <img src="https://latex.codecogs.com/png.latex?G"> aims for <img src="https://latex.codecogs.com/png.latex?D(G(z))"> to approach <img src="https://latex.codecogs.com/png.latex?1">, given by:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D(G)%20=%20%5Clog%E2%81%A1(1%20%E2%88%92%20D(G(z)))%0A"></p>
<p>Both <img src="https://latex.codecogs.com/png.latex?G"> and <img src="https://latex.codecogs.com/png.latex?D"> continuously improve to outperform each other in this game.</p>
<section id="minimax-game-in-gans" class="level3">
<h3 class="anchored" data-anchor-id="minimax-game-in-gans">Minimax Game in GANs</h3>
<p>Vanilla GANs are structured around the minimax game from game theory:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmin_%7BG%7D%5Cmax_%7BD%7D%20%5Cmathcal%7BL%7D(D,%20G)%20=%20%5Clog(D(x))%20+%20%5Clog(1%20-%20D(G(z)))%0A"></p>
<p>In essence:</p>
<ul>
<li><strong>Discriminator:</strong> Maximizes its capacity to differentiate real data from generated.</li>
<li><strong>Generator:</strong> Minimizes the discriminator’s success rate by producing superior forgeries.</li>
</ul>
<p>The iterative competition refines both, targeting a proficient Generator and a perceptive Discriminator.</p>
</section>
</section>
<section id="prepare-environment" class="level2">
<h2 class="anchored" data-anchor-id="prepare-environment">Prepare Environment</h2>
<p>In the upcoming sections, we’ll do the following steps to prepare the development environment:</p>
<ul>
<li>Import necessary libraries, primarily PyTorch and Matplotlib.</li>
<li>Define constants, including project path and seed, for consistency.</li>
<li>Determine the computational device (e.g., GPU).</li>
<li>Provide a weight initialization helper function.</li>
</ul>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List, Tuple, Callable, Sequence</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pathlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Path</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> matplotlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-7"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>config InlineBackend.figure_formats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retina'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'png'</span>}</span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Tensor, nn, optim</span>
<span id="cb1-11"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> functional <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> F</span>
<span id="cb1-12"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.utils.data <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ConcatDataset, DataLoader, Dataset</span>
<span id="cb1-13"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> transforms <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> T</span>
<span id="cb1-14"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision.utils <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_grid</span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> tqdm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tqdm</span>
<span id="cb1-17"></span>
<span id="cb1-18"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchinfo <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> summary</span>
<span id="cb1-19"></span>
<span id="cb1-20">SEED <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span></span>
<span id="cb1-21"></span>
<span id="cb1-22">PROJECT_PATH <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>).resolve()</span>
<span id="cb1-23">FIGURE_PATH <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PROJECT_PATH <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'figures'</span></span>
<span id="cb1-24">DATASET_PATH <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path.home() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'datasets'</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Disable functionalities for speed-up</span></span>
<span id="cb2-2">torch.autograd.set_detect_anomaly(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb2-3">torch.autograd.profiler.profile(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb2-4">torch.autograd.profiler.emit_nvtx(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb2-5"></span>
<span id="cb2-6">device <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.device(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cpu"</span>)</span>
<span id="cb2-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> torch.cuda.is_available():</span>
<span id="cb2-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Allow CuDNN internal benchmarking for architecture-specific optimizations</span></span>
<span id="cb2-9">    torch.backends.cudnn.benchmark <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> weights_init(net: nn.Module) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> m <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> net.modules():</span>
<span id="cb3-3">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(m, (nn.Conv2d, nn.ConvTranspose2d)):</span>
<span id="cb3-4">            nn.init.normal_(m.weight, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02</span>)</span>
<span id="cb3-5">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> m.bias <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-6">                nn.init.constant_(m.bias, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>)</span>
<span id="cb3-7"></span>
<span id="cb3-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">elif</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(m, (nn.BatchNorm1d, nn.BatchNorm2d)):</span>
<span id="cb3-9">            nn.init.normal_(m.weight, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02</span>)</span>
<span id="cb3-10">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> m.bias <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-11">                nn.init.constant_(m.bias, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>)</span>
<span id="cb3-12"></span>
<span id="cb3-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">elif</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(m, nn.Linear):</span>
<span id="cb3-14">            nn.init.normal_(m.weight, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02</span>)</span>
<span id="cb3-15">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> m.bias <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-16">                nn.init.constant_(m.bias, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>)</span></code></pre></div>
</div>
<section id="generator" class="level3">
<h3 class="anchored" data-anchor-id="generator">Generator</h3>
<p>The Generator in GANs acts as an artist, crafting data.</p>
<ul>
<li><strong>Input:</strong> Takes random noise, typically from a standard normal distribution.</li>
<li><strong>Architecture:</strong> Uses dense layers, progressively increasing data dimensions.</li>
<li><strong>Output:</strong> Reshapes data to desired format (e.g., image). Often uses ‘tanh’ for activation.</li>
<li><strong>Objective:</strong> Generate data indistinguishable from real by the Discriminator.</li>
</ul>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Generator(nn.Module):</span>
<span id="cb4-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, out_dim:Sequence[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>], nz:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, ngf:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>, alpha:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>):</span>
<span id="cb4-3">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb4-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        :param out_dim: output image dimension / shape</span></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        :param nz: size of the latent z vector $z$</span></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        :param ngf: size of feature maps (units in the hidden layers) in the generator</span></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        :param alpha: negative slope of leaky ReLU activation</span></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb4-9">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb4-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.out_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> out_dim</span>
<span id="cb4-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb4-12">            nn.Linear(nz, ngf),</span>
<span id="cb4-13">            nn.LeakyReLU(alpha, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb4-14"></span>
<span id="cb4-15">            nn.Linear(ngf, ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb4-16">            nn.LeakyReLU(alpha, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb4-17"></span>
<span id="cb4-18">            nn.Linear(ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb4-19">            nn.LeakyReLU(alpha, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb4-20"></span>
<span id="cb4-21">            nn.Linear(ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(np.prod(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.out_dim))),</span>
<span id="cb4-22">            nn.Tanh(),</span>
<span id="cb4-23">        )</span>
<span id="cb4-24"></span>
<span id="cb4-25">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb4-26">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model(x)</span>
<span id="cb4-27">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.reshape(x, (x.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.out_dim))</span>
<span id="cb4-28">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> x</span>
<span id="cb4-29"></span>
<span id="cb4-30">summary(Generator(out_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)), input_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Generator                                [128, 1, 28, 28]          --
├─Sequential: 1-1                        [128, 784]                --
│    └─Linear: 2-1                       [128, 256]                25,856
│    └─LeakyReLU: 2-2                    [128, 256]                --
│    └─Linear: 2-3                       [128, 512]                131,584
│    └─LeakyReLU: 2-4                    [128, 512]                --
│    └─Linear: 2-5                       [128, 1024]               525,312
│    └─LeakyReLU: 2-6                    [128, 1024]               --
│    └─Linear: 2-7                       [128, 784]                803,600
│    └─Tanh: 2-8                         [128, 784]                --
==========================================================================================
Total params: 1,486,352
Trainable params: 1,486,352
Non-trainable params: 0
Total mult-adds (M): 190.25
==========================================================================================
Input size (MB): 0.05
Forward/backward pass size (MB): 2.64
Params size (MB): 5.95
Estimated Total Size (MB): 8.63
==========================================================================================</code></pre>
</div>
</div>
</section>
<section id="discriminator" class="level3">
<h3 class="anchored" data-anchor-id="discriminator">Discriminator</h3>
<p>The Discriminator is GAN’s evaluator, distinguishing real from fake data.</p>
<ul>
<li><strong>Input:</strong> Takes either real data samples or those from the Generator.</li>
<li><strong>Architecture:</strong> Employs dense layers for binary classification of the input.</li>
<li><strong>Output:</strong> Uses a sigmoid activation, yielding a score between 0-1, reflecting the data’s authenticity.</li>
<li><strong>Objective:</strong> Recognize real data and identify fake data from the Generator.</li>
</ul>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Discriminator(nn.Module):</span>
<span id="cb6-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, input_dim:Sequence[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>], ndf:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, alpha:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>):</span>
<span id="cb6-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb6-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb6-5">            nn.Linear(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(np.prod(input_dim)), ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb6-6">            nn.LeakyReLU(alpha, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb6-7">            nn.Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>),</span>
<span id="cb6-8"></span>
<span id="cb6-9">            nn.Linear(ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb6-10">            nn.LeakyReLU(alpha, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb6-11">            nn.Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>),</span>
<span id="cb6-12"></span>
<span id="cb6-13">            nn.Linear(ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, ndf),</span>
<span id="cb6-14">            nn.LeakyReLU(alpha, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb6-15">            nn.Dropout(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>),</span>
<span id="cb6-16"></span>
<span id="cb6-17">            nn.Linear(ndf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb6-18">            nn.Sigmoid(),</span>
<span id="cb6-19">        )</span>
<span id="cb6-20"></span>
<span id="cb6-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb6-22">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.reshape(x, (x.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb6-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model(x)</span>
<span id="cb6-24"></span>
<span id="cb6-25">summary(Discriminator(input_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)), input_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Discriminator                            [128, 1]                  --
├─Sequential: 1-1                        [128, 1]                  --
│    └─Linear: 2-1                       [128, 512]                401,920
│    └─LeakyReLU: 2-2                    [128, 512]                --
│    └─Dropout: 2-3                      [128, 512]                --
│    └─Linear: 2-4                       [128, 256]                131,328
│    └─LeakyReLU: 2-5                    [128, 256]                --
│    └─Dropout: 2-6                      [128, 256]                --
│    └─Linear: 2-7                       [128, 128]                32,896
│    └─LeakyReLU: 2-8                    [128, 128]                --
│    └─Dropout: 2-9                      [128, 128]                --
│    └─Linear: 2-10                      [128, 1]                  129
│    └─Sigmoid: 2-11                     [128, 1]                  --
==========================================================================================
Total params: 566,273
Trainable params: 566,273
Non-trainable params: 0
Total mult-adds (M): 72.48
==========================================================================================
Input size (MB): 0.40
Forward/backward pass size (MB): 0.92
Params size (MB): 2.27
Estimated Total Size (MB): 3.59
==========================================================================================</code></pre>
</div>
</div>
</section>
</section>
<section id="traning-loop" class="level2">
<h2 class="anchored" data-anchor-id="traning-loop">Traning Loop</h2>
<p>The training process is iterative:</p>
<ul>
<li><strong>Update Discriminator:</strong> With the Generator static, improve the Discriminator’s detection of real vs.&nbsp;fake.</li>
<li><strong>Update Generator:</strong> With a static Discriminator, enhance the Generator’s ability to deceive.</li>
</ul>
<p>Training continues until the Generator produces almost authentic data. Equilibrium is reached when the Discriminator sees every input as equally likely real or fake, assigning a probability of <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D">.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Using <code>.eval()</code> and <code>.train()</code> modes initially seemed promising for faster training. However, they affected layers like <code>BatchNorm2d</code> and <code>Dropout</code>, making the GAN diverge. Also, switching between eval and train modes is not free of charge.</p>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> train_step(</span>
<span id="cb8-2">    generator: nn.Module,</span>
<span id="cb8-3">    discriminator: nn.Module,</span>
<span id="cb8-4">    optim_G: optim.Optimizer,</span>
<span id="cb8-5">    optim_D: optim.Optimizer,</span>
<span id="cb8-6">    criterion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],</span>
<span id="cb8-7">    real_data: torch.Tensor,</span>
<span id="cb8-8">    noise_dim: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>,</span>
<span id="cb8-9">    device: torch.device,</span>
<span id="cb8-10">) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>]:</span>
<span id="cb8-11">    batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> real_data.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb8-12">    </span>
<span id="cb8-13">    real_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> real_data.to(device)</span>
<span id="cb8-14">    </span>
<span id="cb8-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Labels for real and fake data</span></span>
<span id="cb8-16">    real_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.ones(batch_size, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb8-17">    fake_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros(batch_size, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb8-18">    </span>
<span id="cb8-19">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### Train Discriminator</span></span>
<span id="cb8-20"></span>
<span id="cb8-21">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Real data</span></span>
<span id="cb8-22">    output_real <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> discriminator(real_data)</span>
<span id="cb8-23">    loss_D_real <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> criterion(output_real, real_labels)</span>
<span id="cb8-24">    </span>
<span id="cb8-25">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Fake data</span></span>
<span id="cb8-26">    noise <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(batch_size, noise_dim, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb8-27">    fake_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generator(noise)</span>
<span id="cb8-28">    output_fake <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> discriminator(fake_data.detach())</span>
<span id="cb8-29">    loss_D_fake <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> criterion(output_fake, fake_labels)</span>
<span id="cb8-30">    </span>
<span id="cb8-31">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Backprop and optimize for discriminator</span></span>
<span id="cb8-32">    loss_D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (loss_D_real <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> loss_D_fake) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span></span>
<span id="cb8-33">    discriminator.zero_grad()</span>
<span id="cb8-34">    loss_D.backward()</span>
<span id="cb8-35">    optim_D.step()</span>
<span id="cb8-36">    </span>
<span id="cb8-37">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### Train Generator</span></span>
<span id="cb8-38"></span>
<span id="cb8-39">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Recompute fake data’s discriminator scores</span></span>
<span id="cb8-40">    output_fake <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> discriminator(fake_data)</span>
<span id="cb8-41">    loss_G <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> criterion(output_fake, real_labels)</span>
<span id="cb8-42">    </span>
<span id="cb8-43">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Backprop and optimize for generator</span></span>
<span id="cb8-44">    generator.zero_grad()</span>
<span id="cb8-45">    loss_G.backward()</span>
<span id="cb8-46">    optim_G.step()</span>
<span id="cb8-47">    </span>
<span id="cb8-48">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> loss_G.item(), loss_D.item()</span></code></pre></div>
</div>
</section>
<section id="evaluation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="evaluation">Evaluation</h2>
<p>Before evaluation, we configured the learning rate (LR), optimizer’s <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> parameters, batch size, and data loader settings for all experiments. We used the MNIST digits and MNIST fashion datasets for assessment.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">OPTIMIZER_LR <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0002</span></span>
<span id="cb9-2">L2_NORM <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-5</span></span>
<span id="cb9-3">OPTIMIZER_BETAS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.999</span>)</span>
<span id="cb9-4">N_EPOCHS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb9-5">BATCH_SIZE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">loader_kwargs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb10-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_workers'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>,</span>
<span id="cb10-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pin_memory'</span>: <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb10-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'shuffle'</span>: <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb10-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'batch_size'</span>: BATCH_SIZE,</span>
<span id="cb10-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'prefetch_factor'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>,</span>
<span id="cb10-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pin_memory_device'</span>: device.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>,</span>
<span id="cb10-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'persistent_workers'</span>: <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>,</span>
<span id="cb10-9">}</span></code></pre></div>
</div>
<section id="mnist-digits-dataset" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="mnist-digits-dataset">MNIST Digits Dataset</h3>
<p>The MNIST (Modified National Institute of Standards and Technology) dataset is a well-known collection of handwritten digits, extensively used in the fields of machine learning and computer vision for training and testing purposes. Its simplicity and size make it a popular choice for introductory courses and experiments in image recognition.</p>
<p>In total, the dataset contains 70,000 grayscale images of handwritten digits (from 0 to 9). Each image is 28x28 pixels.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_minst_dataset(transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Dataset:</span>
<span id="cb11-2">    <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> MNIST</span>
<span id="cb11-3">    root <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(DATASET_PATH)</span>
<span id="cb11-4">    trainset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MNIST(root<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>root, train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, download<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>transform)</span>
<span id="cb11-5">    testset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MNIST(root<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>root, train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, download<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>transform)</span>
<span id="cb11-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Combine train and test dataset for more samples.</span></span>
<span id="cb11-7">    dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ConcatDataset([trainset, testset])</span>
<span id="cb11-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> dataset</span></code></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">IMG_DIM <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)</span>
<span id="cb12-2">NOISE_DIM <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">transform <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> T.Compose([</span>
<span id="cb13-2">    T.ToTensor(),</span>
<span id="cb13-3">    T.Normalize(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb13-4">])</span>
<span id="cb13-5"></span>
<span id="cb13-6">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_minst_dataset(transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>transform)</span>
<span id="cb13-7">dataloader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataLoader(data, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>loader_kwargs)</span>
<span id="cb13-8"></span>
<span id="cb13-9">torch.manual_seed(SEED)</span>
<span id="cb13-10">torch.cuda.manual_seed_all(SEED)</span>
<span id="cb13-11"></span>
<span id="cb13-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># benchmark_noise is used for the animation to show how output evolve on the same vector</span></span>
<span id="cb13-13">benchmark_noise <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, NOISE_DIM, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb13-14"></span>
<span id="cb13-15">generator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Generator(out_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>IMG_DIM, nz<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>NOISE_DIM).to(device)</span>
<span id="cb13-16">generator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(weights_init)</span>
<span id="cb13-17"></span>
<span id="cb13-18">discriminator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Discriminator(input_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>IMG_DIM).to(device)</span>
<span id="cb13-19">discriminator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(weights_init)</span>
<span id="cb13-20"></span>
<span id="cb13-21">optimizer_G <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.AdamW(</span>
<span id="cb13-22">    generator.parameters(),</span>
<span id="cb13-23">    lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_LR,</span>
<span id="cb13-24">    betas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_BETAS,</span>
<span id="cb13-25">    weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>L2_NORM,</span>
<span id="cb13-26">)</span>
<span id="cb13-27"></span>
<span id="cb13-28">optimizer_D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.AdamW(</span>
<span id="cb13-29">    discriminator.parameters(),</span>
<span id="cb13-30">    lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_LR,</span>
<span id="cb13-31">    betas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_BETAS,</span>
<span id="cb13-32">    weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>L2_NORM,</span>
<span id="cb13-33">)</span>
<span id="cb13-34"></span>
<span id="cb13-35">criterion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.BCELoss().to(device)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">animation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb14-2"></span>
<span id="cb14-3">g_losses, d_losses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [], []</span>
<span id="cb14-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> tqdm(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(N_EPOCHS), unit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'epochs'</span>):</span>
<span id="cb14-5">    generator.train()</span>
<span id="cb14-6">    discriminator.train()</span>
<span id="cb14-7"></span>
<span id="cb14-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> samples_real, _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> dataloader:</span>
<span id="cb14-9">        g_loss, d_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_step(</span>
<span id="cb14-10">            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device</span>
<span id="cb14-11">        )</span>
<span id="cb14-12"></span>
<span id="cb14-13">        g_losses.append(g_loss)</span>
<span id="cb14-14">        d_losses.append(d_loss)</span>
<span id="cb14-15"></span>
<span id="cb14-16">    generator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb14-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> torch.inference_mode():</span>
<span id="cb14-18">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generator(benchmark_noise)</span>
<span id="cb14-19">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.detach().cpu()</span>
<span id="cb14-20"></span>
<span id="cb14-21">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_grid(images, nrow<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, normalize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb14-22">        animation.append(images)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:43&lt;00:00,  2.23s/epochs]</code></pre>
</div>
</div>
<div class="cell page-columns page-full" data-execution_count="15">

<div class="no-row-height column-margin column-container"><div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-10-10-vanilla-GANs/index_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Generator and Discriminator loss evolution over epochs using Vanilla GAN on the MNIST digit dataset.</figcaption>
</figure>
</div>
</div></div></div>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="./figures/gan-mnist.mp4"></video></div>
</section>
<section id="mnist-fashion-dataset" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="mnist-fashion-dataset">MNIST Fashion Dataset</h3>
<p>The Fashion MNIST dataset is a collection of grayscale images of 10 different categories of clothing items, designed as a more challenging alternative to the classic MNIST dataset of handwritten digits. Each image in the dataset is 28x28 pixels. The 10 categories include items like t-shirts/tops, trousers, pullovers, dresses, coats, sandals, and more. With 70,000 images, Fashion MNIST is commonly used for benchmarking machine learning algorithms, especially in image classification tasks.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">IMG_DIM <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)</span>
<span id="cb16-2">NOISE_DIM <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_mnist_fashion_dataset(transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb17-2">    <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> FashionMNIST</span>
<span id="cb17-3">    root <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(DATASET_PATH)</span>
<span id="cb17-4">    trainset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> FashionMNIST(root<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>root, train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, download<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>transform)</span>
<span id="cb17-5">    testset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> FashionMNIST(root<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>root, train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, download<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>transform)</span>
<span id="cb17-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Combine train and test dataset for more samples.</span></span>
<span id="cb17-7">    dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ConcatDataset([trainset, testset])</span>
<span id="cb17-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> dataset</span></code></pre></div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">transform <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> T.Compose([</span>
<span id="cb18-2">    T.ToTensor(),</span>
<span id="cb18-3">    T.Normalize(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb18-4">])</span>
<span id="cb18-5"></span>
<span id="cb18-6">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_mnist_fashion_dataset(transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>transform)</span>
<span id="cb18-7">dataloader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataLoader(data, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>loader_kwargs)</span>
<span id="cb18-8"></span>
<span id="cb18-9">torch.manual_seed(SEED)</span>
<span id="cb18-10">torch.cuda.manual_seed_all(SEED)</span>
<span id="cb18-11"></span>
<span id="cb18-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># benchmark_noise is used for the animation to show how output evolve on same vector</span></span>
<span id="cb18-13">benchmark_noise <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, NOISE_DIM, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb18-14"></span>
<span id="cb18-15">generator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Generator(out_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>IMG_DIM, nz<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>NOISE_DIM).to(device)</span>
<span id="cb18-16">generator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(weights_init)</span>
<span id="cb18-17"></span>
<span id="cb18-18">discriminator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Discriminator(input_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>IMG_DIM).to(device)</span>
<span id="cb18-19">discriminator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(weights_init)</span>
<span id="cb18-20"></span>
<span id="cb18-21">optimizer_G <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.AdamW(</span>
<span id="cb18-22">    generator.parameters(),</span>
<span id="cb18-23">    lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_LR,</span>
<span id="cb18-24">    betas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_BETAS,</span>
<span id="cb18-25">    weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>L2_NORM,</span>
<span id="cb18-26">)</span>
<span id="cb18-27"></span>
<span id="cb18-28">optimizer_D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.AdamW(</span>
<span id="cb18-29">    discriminator.parameters(),</span>
<span id="cb18-30">    lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_LR,</span>
<span id="cb18-31">    betas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_BETAS,</span>
<span id="cb18-32">    weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>L2_NORM,</span>
<span id="cb18-33">)</span>
<span id="cb18-34"></span>
<span id="cb18-35">criterion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.BCELoss().to(device)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">animation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb19-2"></span>
<span id="cb19-3">g_losses, d_losses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [], []</span>
<span id="cb19-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> tqdm(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(N_EPOCHS), unit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'epochs'</span>):</span>
<span id="cb19-5"></span>
<span id="cb19-6">    generator.train()</span>
<span id="cb19-7">    discriminator.train()</span>
<span id="cb19-8"></span>
<span id="cb19-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> samples_real, _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> dataloader:</span>
<span id="cb19-10">        g_loss, d_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_step(</span>
<span id="cb19-11">            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device</span>
<span id="cb19-12">        )</span>
<span id="cb19-13"></span>
<span id="cb19-14">        g_losses.append(g_loss)</span>
<span id="cb19-15">        d_losses.append(d_loss)</span>
<span id="cb19-16"></span>
<span id="cb19-17">    generator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb19-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> torch.inference_mode():</span>
<span id="cb19-19">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generator(benchmark_noise)</span>
<span id="cb19-20">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.detach().cpu()</span>
<span id="cb19-21"></span>
<span id="cb19-22">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_grid(images, nrow<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, normalize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb19-23"></span>
<span id="cb19-24">        animation.append(images)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:49&lt;00:00,  2.30s/epochs]</code></pre>
</div>
</div>
<div class="cell page-columns page-full" data-execution_count="22">

<div class="no-row-height column-margin column-container"><div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-10-10-vanilla-GANs/index_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Generator and Discriminator loss evolution over epochs using Vanilla GAN on the MNIST fashion dataset.</figcaption>
</figure>
</div>
</div></div></div>
<div class="quarto-video"><video id="video_shortcode_videojs_video2" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="./figures/gan-fashion.mp4"></video></div>
</section>
</section>
</section>
<section id="dcgan" class="level1 page-columns page-full">
<h1>DCGAN</h1>
<p>DCGAN, short for Deep Convolutional Generative Adversarial Network, differs from vanilla GAN by using convolutional layers. This design makes DCGAN better for image data. With specific architectural guidelines, DCGAN trains more consistently and generates clearer images than vanilla GANs across various hyperparameters.</p>
<section id="setting-up-dcgans" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-dcgans">Setting Up DCGANs</h2>
<section id="generator-1" class="level3">
<h3 class="anchored" data-anchor-id="generator-1">Generator</h3>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Generator(nn.Module):</span>
<span id="cb21-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, nz:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, ngf:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, nc:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb21-3">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb21-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        :param nz: size of the latent z vector</span></span>
<span id="cb21-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        :param ngf: size of feature maps in generator</span></span>
<span id="cb21-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        :param nc: number of channels in the training images.</span></span>
<span id="cb21-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb21-8">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb21-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb21-10">            nn.ConvTranspose2d(nz, ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>),</span>
<span id="cb21-11">            nn.BatchNorm2d(ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb21-12">            nn.ReLU(inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb21-13"></span>
<span id="cb21-14">            nn.ConvTranspose2d(ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>),</span>
<span id="cb21-15">            nn.BatchNorm2d(ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb21-16">            nn.ReLU(inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb21-17"></span>
<span id="cb21-18">            nn.ConvTranspose2d(ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, ngf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>),</span>
<span id="cb21-19">            nn.BatchNorm2d(ngf),</span>
<span id="cb21-20">            nn.ReLU(inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb21-21"></span>
<span id="cb21-22">            nn.ConvTranspose2d(ngf, nc, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>),</span>
<span id="cb21-23">            nn.Tanh(),</span>
<span id="cb21-24">        )</span>
<span id="cb21-25"></span>
<span id="cb21-26">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb21-27">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.reshape(x, (x.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb21-28">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers(x)</span>
<span id="cb21-29"></span>
<span id="cb21-30">summary(Generator(), input_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Generator                                [128, 1, 28, 28]          --
├─Sequential: 1-1                        [128, 1, 28, 28]          --
│    └─ConvTranspose2d: 2-1              [128, 128, 4, 4]          204,800
│    └─BatchNorm2d: 2-2                  [128, 128, 4, 4]          256
│    └─ReLU: 2-3                         [128, 128, 4, 4]          --
│    └─ConvTranspose2d: 2-4              [128, 64, 7, 7]           73,728
│    └─BatchNorm2d: 2-5                  [128, 64, 7, 7]           128
│    └─ReLU: 2-6                         [128, 64, 7, 7]           --
│    └─ConvTranspose2d: 2-7              [128, 32, 14, 14]         32,768
│    └─BatchNorm2d: 2-8                  [128, 32, 14, 14]         64
│    └─ReLU: 2-9                         [128, 32, 14, 14]         --
│    └─ConvTranspose2d: 2-10             [128, 1, 28, 28]          512
│    └─Tanh: 2-11                        [128, 1, 28, 28]          --
==========================================================================================
Total params: 312,256
Trainable params: 312,256
Non-trainable params: 0
Total mult-adds (G): 1.76
==========================================================================================
Input size (MB): 0.05
Forward/backward pass size (MB): 24.26
Params size (MB): 1.25
Estimated Total Size (MB): 25.56
==========================================================================================</code></pre>
</div>
</div>
</section>
<section id="discriminator-1" class="level3">
<h3 class="anchored" data-anchor-id="discriminator-1">Discriminator</h3>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Discriminator(nn.Module):</span>
<span id="cb23-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, ndf:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, nc:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, alpha:<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>):</span>
<span id="cb23-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb23-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb23-5">            nn.Conv2d(nc, ndf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>),</span>
<span id="cb23-6">            nn.BatchNorm2d(ndf),</span>
<span id="cb23-7">            nn.LeakyReLU(alpha, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb23-8"></span>
<span id="cb23-9">            nn.Conv2d(ndf, ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>),</span>
<span id="cb23-10">            nn.BatchNorm2d(ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb23-11">            nn.LeakyReLU(alpha, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb23-12"></span>
<span id="cb23-13">            nn.Conv2d(ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>),</span>
<span id="cb23-14">            nn.BatchNorm2d(ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb23-15">            nn.LeakyReLU(alpha, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb23-16"></span>
<span id="cb23-17">            nn.Conv2d(ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>),</span>
<span id="cb23-18">            nn.Sigmoid(),</span>
<span id="cb23-19">        )</span>
<span id="cb23-20"></span>
<span id="cb23-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb23-22">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers(x)</span>
<span id="cb23-23">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.reshape(x, (x.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb23-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> x</span>
<span id="cb23-25">    </span>
<span id="cb23-26"></span>
<span id="cb23-27">summary(Discriminator(), input_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(BATCH_SIZE, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Discriminator                            [128, 1]                  --
├─Sequential: 1-1                        [128, 1, 1, 1]            --
│    └─Conv2d: 2-1                       [128, 32, 14, 14]         512
│    └─BatchNorm2d: 2-2                  [128, 32, 14, 14]         64
│    └─LeakyReLU: 2-3                    [128, 32, 14, 14]         --
│    └─Conv2d: 2-4                       [128, 64, 7, 7]           32,768
│    └─BatchNorm2d: 2-5                  [128, 64, 7, 7]           128
│    └─LeakyReLU: 2-6                    [128, 64, 7, 7]           --
│    └─Conv2d: 2-7                       [128, 128, 4, 4]          73,728
│    └─BatchNorm2d: 2-8                  [128, 128, 4, 4]          256
│    └─LeakyReLU: 2-9                    [128, 128, 4, 4]          --
│    └─Conv2d: 2-10                      [128, 1, 1, 1]            2,048
│    └─Sigmoid: 2-11                     [128, 1, 1, 1]            --
==========================================================================================
Total params: 109,504
Trainable params: 109,504
Non-trainable params: 0
Total mult-adds (M): 369.68
==========================================================================================
Input size (MB): 0.40
Forward/backward pass size (MB): 23.46
Params size (MB): 0.44
Estimated Total Size (MB): 24.30
==========================================================================================</code></pre>
</div>
</div>
</section>
</section>
<section id="evaluation-1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="evaluation-1">Evaluation</h2>
<section id="mnist-digits-dataset-1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="mnist-digits-dataset-1">MNIST Digits Dataset</h3>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">IMG_DIM <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)</span>
<span id="cb25-2">NOISE_DIM <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span></span>
<span id="cb25-3"></span>
<span id="cb25-4">transform <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> T.Compose([</span>
<span id="cb25-5">    T.ToTensor(),</span>
<span id="cb25-6">    T.Normalize(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>),</span>
<span id="cb25-7">])</span>
<span id="cb25-8"></span>
<span id="cb25-9">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_minst_dataset(transform)</span>
<span id="cb25-10">dataloader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataLoader(data, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>loader_kwargs)</span>
<span id="cb25-11"></span>
<span id="cb25-12">torch.manual_seed(SEED)</span>
<span id="cb25-13">torch.cuda.manual_seed_all(SEED)</span>
<span id="cb25-14"></span>
<span id="cb25-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># benchmark_noise is used for the animation to show how output evolve on same vector</span></span>
<span id="cb25-16">benchmark_noise <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, NOISE_DIM, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb25-17"></span>
<span id="cb25-18">generator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Generator(nz<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>NOISE_DIM, ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, nc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>IMG_DIM[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]).to(device)</span>
<span id="cb25-19">generator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(weights_init)</span>
<span id="cb25-20"></span>
<span id="cb25-21">discriminator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Discriminator(ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, nc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>IMG_DIM[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]).to(device)</span>
<span id="cb25-22">discriminator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(weights_init)</span>
<span id="cb25-23"></span>
<span id="cb25-24">optimizer_G <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.AdamW(</span>
<span id="cb25-25">    generator.parameters(),</span>
<span id="cb25-26">    lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_LR,</span>
<span id="cb25-27">    betas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_BETAS,</span>
<span id="cb25-28">    weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>L2_NORM,</span>
<span id="cb25-29">)</span>
<span id="cb25-30"></span>
<span id="cb25-31">optimizer_D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.AdamW(</span>
<span id="cb25-32">    discriminator.parameters(),</span>
<span id="cb25-33">    lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_LR,</span>
<span id="cb25-34">    betas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_BETAS,</span>
<span id="cb25-35">    weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>L2_NORM,</span>
<span id="cb25-36">)</span>
<span id="cb25-37"></span>
<span id="cb25-38">criterion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.BCELoss().to(device)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">animation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb26-2"></span>
<span id="cb26-3">g_losses, d_losses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [], []</span>
<span id="cb26-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> tqdm(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(N_EPOCHS), unit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'epochs'</span>):</span>
<span id="cb26-5"></span>
<span id="cb26-6">    generator.train()</span>
<span id="cb26-7">    discriminator.train()</span>
<span id="cb26-8"></span>
<span id="cb26-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> samples_real, _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> dataloader:</span>
<span id="cb26-10">        g_loss, d_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_step(</span>
<span id="cb26-11">            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device</span>
<span id="cb26-12">        )</span>
<span id="cb26-13"></span>
<span id="cb26-14">        g_losses.append(g_loss)</span>
<span id="cb26-15">        d_losses.append(d_loss)</span>
<span id="cb26-16"></span>
<span id="cb26-17">    generator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb26-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> torch.inference_mode():</span>
<span id="cb26-19">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generator(benchmark_noise)</span>
<span id="cb26-20">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.detach().cpu()</span>
<span id="cb26-21">    </span>
<span id="cb26-22">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_grid(images, nrow<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, normalize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb26-23">    </span>
<span id="cb26-24">        animation.append(images)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [04:54&lt;00:00,  2.94s/epochs]</code></pre>
</div>
</div>
<div class="cell page-columns page-full" data-execution_count="28">

<div class="no-row-height column-margin column-container"><div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-10-10-vanilla-GANs/index_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Generator and Discriminator loss evolution over epochs using DCGAN on the MNIST digit dataset.</figcaption>
</figure>
</div>
</div></div></div>
<div class="quarto-video"><video id="video_shortcode_videojs_video3" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="./figures/dcgan-mnist.mp4"></video></div>
</section>
<section id="mnist-fashion-dataset-1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="mnist-fashion-dataset-1">MNIST Fashion Dataset</h3>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">IMG_DIM <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)</span>
<span id="cb28-2">NOISE_DIM <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span></span>
<span id="cb28-3"></span>
<span id="cb28-4">transform <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> T.Compose([</span>
<span id="cb28-5">    T.ToTensor(),</span>
<span id="cb28-6">    T.Normalize(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>),</span>
<span id="cb28-7">])</span>
<span id="cb28-8"></span>
<span id="cb28-9">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_mnist_fashion_dataset(transform)</span>
<span id="cb28-10">dataloader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataLoader(data, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>loader_kwargs)</span>
<span id="cb28-11"></span>
<span id="cb28-12">torch.manual_seed(SEED)</span>
<span id="cb28-13">torch.cuda.manual_seed_all(SEED)</span>
<span id="cb28-14"></span>
<span id="cb28-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># benchmark_noise is used for the animation to show how output evolve on same vector</span></span>
<span id="cb28-16">benchmark_noise <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, NOISE_DIM, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb28-17"></span>
<span id="cb28-18">generator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Generator(nz<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>NOISE_DIM, ngf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, nc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>IMG_DIM[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]).to(device)</span>
<span id="cb28-19">generator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(weights_init)</span>
<span id="cb28-20"></span>
<span id="cb28-21">discriminator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Discriminator(ndf<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, nc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>IMG_DIM[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]).to(device)</span>
<span id="cb28-22">discriminator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(weights_init)</span>
<span id="cb28-23"></span>
<span id="cb28-24">optimizer_G <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.AdamW(</span>
<span id="cb28-25">    generator.parameters(),</span>
<span id="cb28-26">    lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_LR,</span>
<span id="cb28-27">    betas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_BETAS,</span>
<span id="cb28-28">    weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>L2_NORM,</span>
<span id="cb28-29">)</span>
<span id="cb28-30"></span>
<span id="cb28-31">optimizer_D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.AdamW(</span>
<span id="cb28-32">    discriminator.parameters(),</span>
<span id="cb28-33">    lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_LR,</span>
<span id="cb28-34">    betas<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OPTIMIZER_BETAS,</span>
<span id="cb28-35">    weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>L2_NORM</span>
<span id="cb28-36">)</span>
<span id="cb28-37"></span>
<span id="cb28-38">criterion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.BCELoss().to(device) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># F.binary_cross_entropy_with_logits #nn.BCELoss().to(device)</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">animation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb29-2"></span>
<span id="cb29-3">g_losses, d_losses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [], []</span>
<span id="cb29-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> tqdm(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(N_EPOCHS), unit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'epochs'</span>):</span>
<span id="cb29-5"></span>
<span id="cb29-6">    generator.train()</span>
<span id="cb29-7">    discriminator.train()</span>
<span id="cb29-8"></span>
<span id="cb29-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> samples_real, _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> dataloader:</span>
<span id="cb29-10">        g_loss, d_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_step(</span>
<span id="cb29-11">            generator, discriminator, optimizer_G, optimizer_D, criterion, samples_real, NOISE_DIM, device</span>
<span id="cb29-12">        )</span>
<span id="cb29-13"></span>
<span id="cb29-14">        g_losses.append(g_loss)</span>
<span id="cb29-15">        d_losses.append(d_loss)</span>
<span id="cb29-16"></span>
<span id="cb29-17">    generator.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb29-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> torch.inference_mode():</span>
<span id="cb29-19">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generator(benchmark_noise)</span>
<span id="cb29-20">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.detach().cpu()</span>
<span id="cb29-21">    </span>
<span id="cb29-22">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_grid(images, nrow<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, normalize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb29-23">    </span>
<span id="cb29-24">        animation.append(images)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [04:55&lt;00:00,  2.95s/epochs]</code></pre>
</div>
</div>
<div class="cell page-columns page-full" data-execution_count="32">

<div class="no-row-height column-margin column-container"><div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-10-10-vanilla-GANs/index_files/figure-html/cell-32-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Generator and Discriminator loss evolution over epochs using DCGAN on the MNIST fashion dataset.</figcaption>
</figure>
</div>
</div></div></div>
<div class="quarto-video"><video id="video_shortcode_videojs_video4" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="./figures/dcgan-mnist-fashion.mp4"></video></div>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Generative Adversarial Networks (GANs) represent an innovative class of unsupervised neural networks that have significantly impacted the field of artificial intelligence (AI). They consist of two components: a Generator that improves its output and a Discriminator that enhances its evaluative skills. In a competitive yet symbiotic relationship, these two networks converge towards a dynamic equilibrium. This interaction exemplifies the strength of GANs and the adaptability of adversarial learning in AI, blending creative generation with critical assessment.</p>
<p>In this post, I explore the original GAN, often referred to as the “vanilla” GAN. My aim was to understand the basic mechanics of how GANs operate. Meanwhile, others have advanced this technology, applying it to a range of innovative and fascinating new areas.</p>
<ul>
<li><a href="https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/">18 Impressive Applications of Generative Adversarial Networks (GANs)</a></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-goodfellow2014generative" class="csl-entry">
Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. <span>“Generative Adversarial Networks.”</span> <a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a>.
</div>
</div></section></div> ]]></description>
  <category>pytorch</category>
  <category>GAN</category>
  <guid>gcerar.github.io/posts/2023-10-10-vanilla-GANs/index.html</guid>
  <pubDate>Tue, 10 Oct 2023 00:00:00 GMT</pubDate>
  <media:content url="gcerar.github.io/posts/2023-10-10-vanilla-GANs/featured.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Neural Style Transfer</title>
  <dc:creator>Gregor Cerar</dc:creator>
  <link>gcerar.github.io/posts/2023-09-15-neural-style-transfer/index.html</link>
  <description><![CDATA[ 



<section id="introduction" class="level1">
<h1>Introduction</h1>
<p><a href="https://en.wikipedia.org/wiki/Neural_style_transfer">Neural Style Transfer</a> (NST) is a deep learning technique that combines the <strong>content</strong> of one image with the <strong>style</strong> of another, like giving your photo a Van Gogh-esque makeover.</p>
<p>Using convolutional neural networks, NST examines both images’ features and creates a new image that merges the content’s structure with the style’s attributes. This technique became a hit due to its novel outcomes, leading to its adoption in various apps and platforms and highlighting deep learning’s prowess in image transformation.</p>
<p>Introduced initially in “<a href="https://arxiv.org/abs/1508.06576">A Neural Algorithm of Artistic Style</a>” <span class="citation" data-cites="gatys2015neural">(Gatys, Ecker, and Bethge 2015)</span>, this method transfers art styles between images. Eager to learn how it works, I’ve implemented the original approach from scratch and presented a few cherry-picked transformed examples.</p>
</section>
<section id="prerequisites" class="level1">
<h1>Prerequisites</h1>
<p>Before we get started, we need to install <a href="https://numpy.org/">NumPy</a>, <a href="https://matplotlib.org/">Matplotlib</a>, <a href="https://pytorch.org/">PyTorch</a> deep learning framework, and finally, <a href="https://pytorch.org/vision/stable/index.html">Torchvision</a> library.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pathlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Path</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List, Union</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> matplotlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-6"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>config InlineBackend.figure_formats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retina'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'png'</span>}</span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> PIL <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Image</span>
<span id="cb1-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Tensor, nn, optim</span>
<span id="cb1-11"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> functional <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> F</span>
<span id="cb1-12"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> models, transforms <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> T</span>
<span id="cb1-13"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision.transforms <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> functional <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> VF</span>
<span id="cb1-14"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision.utils <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_grid</span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> tqdm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tqdm</span>
<span id="cb1-17"></span>
<span id="cb1-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Random seed for reproducibility</span></span>
<span id="cb1-19">SEED <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span></span>
<span id="cb1-20"></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Size of the output image</span></span>
<span id="cb1-22">IMG_SIZE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span></span></code></pre></div>
</div>
<p>Although it is possible to run neural networks on a CPU, using compute accelerators, such as GPU, will do transformation much faster. Here, I utilize my NVIDIA RTX 3090, where I also took advantage of available tensor cores and reduced precision data type <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a> for faster transformation.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">AMP_ENABLED <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb2-2"></span>
<span id="cb2-3">device <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.device(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cpu"</span>)</span>
<span id="cb2-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> torch.cuda.is_available():</span>
<span id="cb2-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Allow CuDNN internal benchmarking for architecture-specific optimizations</span></span>
<span id="cb2-6">    torch.backends.cudnn.benchmark <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb2-7"></span>
<span id="cb2-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Enable bfloat16 if supported</span></span>
<span id="cb2-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> torch.cuda.is_bf16_supported():</span>
<span id="cb2-10">        AMP_ENABLED <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb2-11">        torch.set_float32_matmul_precision(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'medium'</span>)</span></code></pre></div>
</div>
</section>
<section id="implementation" class="level1">
<h1>Implementation</h1>
<div id="fig-architecture" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-09-15-neural-style-transfer/figures/neural-style-transfer.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: The Neural Style Transfer framework introduced by Gatys <em>et al.</em> distinguishes style and content features from designated layers.</figcaption>
</figure>
</div>
<p>Implementing NST was initially confusing since it does not follow the typical boilerplate used in deep learning. In the following sections, I’ll delve into its implementation step by step and often refer back to Figure&nbsp;1. The steps are as follows:</p>
<ul>
<li>Prepare the content, style, and target images.</li>
<li>Prepare a pre-trained VGG neural network and prevent changes to its weights.</li>
<li>Introduce three unique loss metrics.</li>
<li>Adjust the neural network to extract features during forward-backward passes, applying gradient modifications to the target image. The neural network stays unchanged in the process.</li>
<li>Iterate through this process.</li>
</ul>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Weights for different features (were these used by original authors?)</span></span>
<span id="cb3-2">STYLE_LAYERS_DEFAULT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb3-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"conv1_1"</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>,</span>
<span id="cb3-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"conv2_1"</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>,</span>
<span id="cb3-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"conv3_1"</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,</span>
<span id="cb3-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"conv4_1"</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,</span>
<span id="cb3-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"conv5_1"</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>,</span>
<span id="cb3-8">}</span>
<span id="cb3-9"></span>
<span id="cb3-10">CONTENT_LAYERS_DEFAULT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"conv5_2"</span>,)</span>
<span id="cb3-11"></span>
<span id="cb3-12">CONTENT_WEIGHT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "alpha" in the literature (default: 8)</span></span>
<span id="cb3-13">STYLE_WEIGHT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "beta" in the literature (default: 70)</span></span>
<span id="cb3-14">TV_WEIGHT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># "gamma" in the literature (default: 10)</span></span>
<span id="cb3-15"></span>
<span id="cb3-16"></span>
<span id="cb3-17">LEARNING_RATE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.004</span></span>
<span id="cb3-18">N_EPOCHS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5_000</span></span></code></pre></div>
</div>
<section id="loss-metrics" class="level2">
<h2 class="anchored" data-anchor-id="loss-metrics">Loss metrics</h2>
<p>To effectively implement Neural Style Transfer, we need to quantify how well the generated image matches both the <strong>content</strong> and <strong>style</strong> of our source images. This is done using loss metrics. Let’s delve into the specifics of these metrics and how they drive the NST process.</p>
<section id="content-loss-metric" class="level3">
<h3 class="anchored" data-anchor-id="content-loss-metric">Content loss metric</h3>
<p>Content loss is calculated through Euclidean distance (<em>i.e.,</em> mean squared error) between the respective intermediate higher-level feature representation <img src="https://latex.codecogs.com/png.latex?F%5El"> and <img src="https://latex.codecogs.com/png.latex?P%5El"> of original input image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D"> and the content image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bp%7D"> at layer <img src="https://latex.codecogs.com/png.latex?l">.</p>
<p>Hence, a given input image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D"> is encoded in each layer of the CNN by the filter responses to that image. A layer with <img src="https://latex.codecogs.com/png.latex?N_l"> distinct filters has <img src="https://latex.codecogs.com/png.latex?N_l"> feature maps of size <img src="https://latex.codecogs.com/png.latex?M_l">, where <img src="https://latex.codecogs.com/png.latex?M_l"> is the height times the width of the feature map. So the response in a layer <img src="https://latex.codecogs.com/png.latex?l"> can be stored in a matrix <img src="https://latex.codecogs.com/png.latex?F%5El%20%5Cin%20%5Cmathcal%7BR%7D%5E%7BN_l%20%5Ctimes%20M_l%7D"> where <img src="https://latex.codecogs.com/png.latex?F_%7Bij%7D%5E%7Bl%7D"> is the activation of the <img src="https://latex.codecogs.com/png.latex?i%5E%7Bth%7D"> filter at position <img src="https://latex.codecogs.com/png.latex?j"> in layer <img src="https://latex.codecogs.com/png.latex?l">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D_%7Bcontent%7D(%5Cvec%7Bp%7D,%20%5Cvec%7Bx%7D,%20l)%20=%20%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi,j%7D%20(F%5E%7Bl%7D_%7Bij%7D%20-%20P%5E%7Bl%7D_%7Bij%7D)%5E2%0A"></p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> content_loss_func(target_features, precomputed_content_features):</span>
<span id="cb4-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Calculate content loss metric for give layers."""</span></span>
<span id="cb4-3"></span>
<span id="cb4-4">    content_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span></span>
<span id="cb4-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> precomputed_content_features:</span>
<span id="cb4-6">        target_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> target_features[layer]</span>
<span id="cb4-7">        content_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> precomputed_content_features[layer]</span>
<span id="cb4-8"></span>
<span id="cb4-9">        content_layer_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.mse_loss(target_feature, content_feature)</span>
<span id="cb4-10">        content_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> content_layer_loss</span>
<span id="cb4-11"></span>
<span id="cb4-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> content_loss</span></code></pre></div>
</div>
</section>
<section id="style-loss" class="level3">
<h3 class="anchored" data-anchor-id="style-loss">Style loss</h3>
<p>The style loss is more convolved than the content loss. We compute it by comparing the Gram matrices of the feature maps from the style image and the generated image.</p>
<p>First, let’s understand the <a href="https://en.wikipedia.org/wiki/Gram_matrix">Gram matrix</a>. Given the feature map <img src="https://latex.codecogs.com/png.latex?F"> of size <img src="https://latex.codecogs.com/png.latex?C%20%5Ctimes%20(H%20%5Ctimes%20W)">, where <img src="https://latex.codecogs.com/png.latex?C"> is the number of channels and <img src="https://latex.codecogs.com/png.latex?H%20%5Ctimes%20W"> are the spatial dimensions, the Gram matrix <img src="https://latex.codecogs.com/png.latex?G"> is of size <img src="https://latex.codecogs.com/png.latex?C%20%5Ctimes%20C"> and is computed as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AG%5El_%7Bij%7D%20=%20%5Csum_k%20F%5El_%7Bik%7D%20F%5El_%7Bjk%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?G_%7Bij%7D"> is the inner product between vectorized feature maps <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j">. This results in a matrix that captures the correlation between different feature maps and, thus, the style information.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> gram_matrix(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb5-2">    (b, c, h, w) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>.size()</span>
<span id="cb5-3"></span>
<span id="cb5-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># reshape into (C x (H x W))</span></span>
<span id="cb5-5">    features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>.view(b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> c, h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> w)</span>
<span id="cb5-6"></span>
<span id="cb5-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute the gram product</span></span>
<span id="cb5-8">    gram <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.mm(features, features.t())</span>
<span id="cb5-9"></span>
<span id="cb5-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> gram</span></code></pre></div>
</div>
<p>The style loss between the Gram matrix of the generated image <img src="https://latex.codecogs.com/png.latex?G"> and that of style image <img src="https://latex.codecogs.com/png.latex?A"> (at a specific layer <img src="https://latex.codecogs.com/png.latex?l">) is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE_l%20=%20%5Cfrac%7B1%7D%7B4%20N%5E%7B2%7D_%7Bl%7D%20M%5E%7B2%7D_%7Bl%7D%7D%20%5Csum_%7Bi,j%7D(G%5El_%7Bij%7D%20-%20A%5El_%7Bij%7D)%5E2%0A"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?E_l"> is the style loss for layer <img src="https://latex.codecogs.com/png.latex?l">, <img src="https://latex.codecogs.com/png.latex?N_l"> and <img src="https://latex.codecogs.com/png.latex?M_l"> are the numbers of channels and height times width in the feature representation of layer <img src="https://latex.codecogs.com/png.latex?l">, respectively. <img src="https://latex.codecogs.com/png.latex?G_%7Bij%7D%5El"> and <img src="https://latex.codecogs.com/png.latex?A_%7Bij%7D%5El"> are the gram matrices of the intermediate representation of the style image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Ba%7D"> and the input base image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D"> respectively.</p>
<p>The total style loss is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D_%7Bstyle%7D(%5Cvec%7Ba%7D,%20%5Cvec%7Bx%7D)%20=%20%5Csum_%7Bl=0%7D%5E%7BL%7D%20w_l%20E_l%0A"></p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> style_loss_func(target_features, style_features, precomputed_style_grams):</span>
<span id="cb6-2">    style_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span></span>
<span id="cb6-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> style_features:</span>
<span id="cb6-4">        target_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> target_features[layer]</span>
<span id="cb6-5">        target_gram <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gram_matrix(target_feature)</span>
<span id="cb6-6"></span>
<span id="cb6-7">        style_gram <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> precomputed_style_grams[layer]</span>
<span id="cb6-8"></span>
<span id="cb6-9">        _, c, h, w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> target_feature.shape</span>
<span id="cb6-10"></span>
<span id="cb6-11">        weight <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> STYLE_LAYERS_DEFAULT[layer]</span>
<span id="cb6-12">        layer_style_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weight <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> F.mse_loss(target_gram, style_gram) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> w)</span>
<span id="cb6-13">        style_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> layer_style_loss</span>
<span id="cb6-14"></span>
<span id="cb6-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> style_loss</span></code></pre></div>
</div>
</section>
<section id="total-variation-loss" class="level3">
<h3 class="anchored" data-anchor-id="total-variation-loss">Total Variation Loss</h3>
<p>Total Variation (TV) loss, also known as Total Variation Regularization, is commonly added to the Neural Style Transfer objective to encourage spatial smoothness in the generated image. Without it, the output might exhibit noise or oscillations, particularly in regions where the content and style objectives don’t offer much guidance.</p>
<p>Given an image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D"> of size <img src="https://latex.codecogs.com/png.latex?H%20%5Ctimes%20W%20%5Ctimes%20C"> (height, width, channels), the Total Variation loss is defined as the sum of the absolute differences between neighboring pixel values:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D_%7BTV%7D(%5Cvec%7Bx%7D)%20=%20%5Csum_%7Bi,j%7D%20((x_%7Bi,j+1%7D%20-%20x_%7Bi,j%7D)%5E2%20+%20(x_%7Bi+1,j%7D%20-%20x_%7Bi,j%7D)%5E2)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?x_%7Bi,j%7D"> is the pixel value at position <img src="https://latex.codecogs.com/png.latex?(i,j)">.</p>
<p>In simple terms, this loss penalizes abrupt changes in pixel values from one to its neighbors. By minimizing this loss, the generated image becomes smoother, reducing artifacts and unwanted noise. When combined with content and style losses, the TV loss ensures that the resulting image not only captures the content and style of the source images but also looks visually coherent and smooth.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> total_variance_loss_func(target: Tensor):</span>
<span id="cb7-2">    tv_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.l1_loss(target[:, :, :, :<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], target[:, :, :, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb7-3">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> F.l1_loss(target[:, :, :<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, :], target[:, :, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:, :])</span>
<span id="cb7-4"></span>
<span id="cb7-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> tv_loss</span></code></pre></div>
</div>
</section>
<section id="total-loss" class="level3">
<h3 class="anchored" data-anchor-id="total-loss">Total Loss</h3>
<p>The total loss combines three loss metric components, each targeting a specific aspect of the image generation process. Let’s recap the components:</p>
<ol type="1">
<li><strong>Content Loss</strong>: Ensures the generated image resembles the content image’s content.</li>
<li><strong>Style Loss</strong>: Ensures the generated image captures the stylistic features of the style image.</li>
<li><strong>Total Variation Loss</strong>: Encourages spatial smoothness in the generated image, reducing artifacts and noise.</li>
</ol>
<p>Given the above components, the total loss <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D_%7Btotal%7D"> for Neural Style Transfer can be formulated as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D_%7Btotal%7D(%5Cvec%7Bp%7D,%5Cvec%7Ba%7D,%5Cvec%7Bx%7D)%20=%20%5Calpha%5Cmathcal%7BL%7D_%7Bcontent%7D(%5Cvec%7Bp%7D,%5Cvec%7Bx%7D)%20+%20%5Cbeta%5Cmathcal%7BL%7D_%7Bstyle%7D(%5Cvec%7Ba%7D,%5Cvec%7Bx%7D)%20+%20%5Cgamma%5Cmathcal%7BL%7D_%7BTV%7D(%5Cvec%7Bx%7D)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Calpha">, <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, and <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> are weight factors that determine the relative importance of the content, style, and the total variation losses, respectively. By adjusting these weights, one can control the balance between content preservation, style transfer intensity, and the smoothness of the generated image. The algorithm aims to adjust the generated image to minimize the total loss.</p>
</section>
</section>
<section id="input-preparation" class="level2">
<h2 class="anchored" data-anchor-id="input-preparation">Input preparation</h2>
<p>Here we specify path to content and style images:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">content_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./bridge.jpg"</span></span>
<span id="cb8-2">style_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./walking-in-the-rain.jpg"</span></span></code></pre></div>
</div>
</section>
<section id="neural-style-transfer-process" class="level2">
<h2 class="anchored" data-anchor-id="neural-style-transfer-process">Neural Style Transfer Process</h2>
<p>For feature extraction, we’ll leverage <a href="https://arxiv.org/abs/1409.1556">VGG19</a>, pre-trained on <a href="https://www.image-net.org/">ImageNet</a>, same as the original authors. Note that we set the model to evaluation mode, ensuring we only use VGG19 to extract features without altering its weights. We also transfer the neural network (NN) to a chosen device, ideally a GPU, for optimal performance.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>An intriguing choice by Gatys <em>et al.</em> was to modify VGG-19, replacing max pooling with average pooling, aiming for visually superior results. However, a challenge arises: our NN was initially trained with <code>MaxPool2d</code> layers. Substituting them can affect activations due to reduced output values. To counteract this, we’ve introduced a custom <code>ScaledAvgPool2d</code>.</p>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We will use a frozen pre-trained VGG neural network for feature extraction.</span></span>
<span id="cb9-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># In the original paper, authors have used VGG19 (without batch normalization)</span></span>
<span id="cb9-3">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> models.vgg19(weights<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>models.VGG19_Weights.IMAGENET1K_V1).features</span>
<span id="cb9-4"></span>
<span id="cb9-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Authors in the original paper suggested using AvgPool instead of MaxPool</span></span>
<span id="cb9-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># for more pleasing results. However, changing the pooling also affects</span></span>
<span id="cb9-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># activation, so the input needs to be scaled (can't find the original source).</span></span>
<span id="cb9-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> ScaledAvgPool2d(nn.Module):</span>
<span id="cb9-9">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, kernel_size, stride, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, scale_factor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span>):</span>
<span id="cb9-10">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb9-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.avgpool <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.nn.AvgPool2d(kernel_size, stride, padding)</span>
<span id="cb9-12">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.scale_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scale_factor</span>
<span id="cb9-13"></span>
<span id="cb9-14">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb9-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.avgpool(x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.scale_factor</span>
<span id="cb9-16"></span>
<span id="cb9-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (OPTIONAL) Replace max-pooling layers with custom average pooling layers</span></span>
<span id="cb9-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#for i, layer in enumerate(model):</span></span>
<span id="cb9-19"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   if isinstance(layer, torch.nn.MaxPool2d):</span></span>
<span id="cb9-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#       model[i] = ScaledAvgPool2d(kernel_size=2, stride=2, padding=0)</span></span>
<span id="cb9-21"></span>
<span id="cb9-22">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>().requires_grad_(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).to(device)</span></code></pre></div>
</div>
<p>The pretrained VGG model used normalized ImageNet samples for better performance. For effective style transfer, we’ll follow suit to improve feature extraction. Though images will appear altered post-normalization, they are reverted to their original state after the NST process. Next, we’ll transform the content and style images by:</p>
<ul>
<li>Loading them from storage.</li>
<li>Resizing while maintaining aspect ratio.</li>
<li>Converting to tensors.</li>
<li>Normalizing using ImageNet weights.</li>
</ul>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ImageNet normalization weights per channel</span></span>
<span id="cb10-2">IMAGENET_MEAN <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.485</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.456</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.406</span>)</span>
<span id="cb10-3">IMAGENET_STD <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.229</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.224</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.225</span>)</span>
<span id="cb10-4"></span>
<span id="cb10-5">transform <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> T.Compose([</span>
<span id="cb10-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Shorter edge of the image will be matched to `IMG_SIZE`</span></span>
<span id="cb10-7">    T.Resize(IMG_SIZE),</span>
<span id="cb10-8">    T.ToTensor(),</span>
<span id="cb10-9">    T.Normalize(IMAGENET_MEAN, IMAGENET_STD),</span>
<span id="cb10-10">])</span>
<span id="cb10-11"></span>
<span id="cb10-12"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> load_image(path: Union[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, Path]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb10-13">    image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Image.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(path).convert(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"RGB"</span>)</span>
<span id="cb10-14"></span>
<span id="cb10-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Transform images into tensors</span></span>
<span id="cb10-16">    image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> transform(image)</span>
<span id="cb10-17"></span>
<span id="cb10-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add dimension to imitate batch size equal to 1: (C,H,W) -&gt; (B,C,H,W)</span></span>
<span id="cb10-19">    image <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb10-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> image</span></code></pre></div>
</div>
<p>The following code will prepares content <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bp%7D">, style <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Ba%7D">, and target <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D"> images. The target image is a clone of the content image and we enable computation of gradients on it.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The "style" image from which we obtain style</span></span>
<span id="cb11-2">style <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_image(style_path).to(device)</span>
<span id="cb11-3"></span>
<span id="cb11-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The "content" image on which we apply style</span></span>
<span id="cb11-5">content <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_image(content_path).to(device)</span>
<span id="cb11-6"></span>
<span id="cb11-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The "target" image to store the outcome</span></span>
<span id="cb11-8">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> content.clone().requires_grad_(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).to(device)</span></code></pre></div>
</div>
<p>The function below retrieves feature maps from designated layers. As shown in Figure&nbsp;1:</p>
<ul>
<li>Content feature map comes from <code>relu5_2</code>.</li>
<li>Style feature maps are sourced from <code>relu1_1</code>, <code>relu2_1</code>, <code>relu3_1</code>, <code>relu4_1</code>, and <code>relu5_1</code>.</li>
</ul>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_features(image: Tensor, model: nn.Module, layers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb12-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> layers <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb12-3">        layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>(STYLE_LAYERS_DEFAULT) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> CONTENT_LAYERS_DEFAULT</span>
<span id="cb12-4"></span>
<span id="cb12-5">    features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb12-6">    block_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb12-7">    conv_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb12-8"></span>
<span id="cb12-9">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image</span>
<span id="cb12-10"></span>
<span id="cb12-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> model:</span>
<span id="cb12-12">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer(x)</span>
<span id="cb12-13"></span>
<span id="cb12-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(layer, nn.Conv2d):</span>
<span id="cb12-15">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># produce layer name to find matching convolutions from the paper</span></span>
<span id="cb12-16">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># and store their output for further processing.</span></span>
<span id="cb12-17">            conv_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb12-18">            name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"conv</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>block_num<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>conv_num<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb12-19">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> name <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> layers:</span>
<span id="cb12-20">                features[name] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x</span>
<span id="cb12-21"></span>
<span id="cb12-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">elif</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(layer, (nn.MaxPool2d, nn.AvgPool2d, ScaledAvgPool2d)):</span>
<span id="cb12-23">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># In VGG, each block ends with max/avg pooling layer.</span></span>
<span id="cb12-24">            block_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb12-25">            conv_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb12-26"></span>
<span id="cb12-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">elif</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(layer, (nn.BatchNorm2d, nn.ReLU)):</span>
<span id="cb12-28">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">pass</span></span>
<span id="cb12-29"></span>
<span id="cb12-30">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb12-31">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Unknown layer: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>layer<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb12-32"></span>
<span id="cb12-33">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> features</span></code></pre></div>
</div>
<p>Since content and style images never change, we can precompute their feature maps and grams to speed up the NST process.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Precompute content features, style features, and style gram matrices.</span></span>
<span id="cb13-2">content_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_features(content, model, CONTENT_LAYERS_DEFAULT)</span>
<span id="cb13-3">style_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_features(style, model, STYLE_LAYERS_DEFAULT)</span>
<span id="cb13-4"></span>
<span id="cb13-5">style_grams <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {layer: gram_matrix(style_features[layer]) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> style_features}</span></code></pre></div>
</div>
<p>Next, we will use <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam</a> optimizer, where we specify that only target image <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D"> is considered for optimization.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.Adam([target], lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>LEARNING_RATE)</span></code></pre></div>
</div>
<p>The final step of NST is to transfer style using everything we’ve implemented. We extract feature maps, compute total loss, perform steps using gradient descent, and repeat the process <code>N_EPOCHS</code> times. Gradient changes will apply only to the target image.</p>
<p>To notably enhance NST speed, I utilized mixed precision with the unique <code>bfloat16</code> found in newer hardware. Traditional half-precision float16 doesn’t yield the same results. I’ve tested it. Probably because of the issue with gradient scaling.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">pbar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tqdm(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(N_EPOCHS))</span>
<span id="cb15-2"></span>
<span id="cb15-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> step <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> pbar:</span>
<span id="cb15-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> torch.autocast(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cuda'</span>, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.bfloat16, enabled<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>AMP_ENABLED):</span>
<span id="cb15-5">        target_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_features(target, model)</span>
<span id="cb15-6"></span>
<span id="cb15-7">        content_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CONTENT_WEIGHT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> content_loss_func(target_features, content_features)</span>
<span id="cb15-8">        style_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> STYLE_WEIGHT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> style_loss_func(target_features, style_features, style_grams)</span>
<span id="cb15-9">        tv_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TV_WEIGHT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> total_variance_loss_func(target)</span>
<span id="cb15-10"></span>
<span id="cb15-11">        total_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> content_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> style_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> tv_loss</span>
<span id="cb15-12"></span>
<span id="cb15-13">    optimizer.zero_grad(set_to_none<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb15-14">    total_loss.backward()  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># do I need to `retain_graph=True`?</span></span>
<span id="cb15-15"></span>
<span id="cb15-16">    optimizer.step()</span>
<span id="cb15-17"></span>
<span id="cb15-18">    pbar.set_postfix_str(</span>
<span id="cb15-19">        <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"total_loss=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>total_loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>item()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> "</span></span>
<span id="cb15-20">        <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"content_loss=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>content_loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>item()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> "</span></span>
<span id="cb15-21">        <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"style_loss=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>style_loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>item()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> "</span></span>
<span id="cb15-22">        <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"tv_loss=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tv_loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>item()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> "</span></span>
<span id="cb15-23">    )</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 5000/5000 [01:30&lt;00:00, 55.36it/s, total_loss=43.90 content_loss=8.71 style_loss=29.08 tv_loss=6.11 ]     </code></pre>
</div>
</div>
<p>As mentioned before, images need to be denormalized (<em>i.e.</em> reverted back) to correct colors. After that we compare content, style and target images side-by-side.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> InverseNormalize(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">object</span>):</span>
<span id="cb17-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, mean, std):</span>
<span id="cb17-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean</span>
<span id="cb17-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.std <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> std</span>
<span id="cb17-5"></span>
<span id="cb17-6">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__call__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb17-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> t, m, s <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(x, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mean, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.std):</span>
<span id="cb17-8">            t.mul_(s).add_(m)</span>
<span id="cb17-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> x</span>
<span id="cb17-10"></span>
<span id="cb17-11"></span>
<span id="cb17-12"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Clip(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">object</span>):</span>
<span id="cb17-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, vmin: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, vmax: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>):</span>
<span id="cb17-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.vmin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vmin</span>
<span id="cb17-15">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.vmax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vmax</span>
<span id="cb17-16"></span>
<span id="cb17-17">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__call__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb17-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> torch.clamp(x, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.vmin, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.vmax)</span>
<span id="cb17-19"></span>
<span id="cb17-20"></span>
<span id="cb17-21">inv_transform_preview <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> T.Compose([</span>
<span id="cb17-22">    InverseNormalize(IMAGENET_MEAN, IMAGENET_STD),</span>
<span id="cb17-23">    T.Resize(IMG_SIZE, antialias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb17-24">    T.CenterCrop((IMG_SIZE, IMG_SIZE)),</span>
<span id="cb17-25">    Clip(),</span>
<span id="cb17-26">])</span>
<span id="cb17-27"></span>
<span id="cb17-28">imgs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb17-29">    inv_transform_preview(i.detach().squeeze().cpu())</span>
<span id="cb17-30">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> (content, style, target)</span>
<span id="cb17-31">]</span>
<span id="cb17-32"></span>
<span id="cb17-33">grid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_grid(imgs)</span>
<span id="cb17-34"></span>
<span id="cb17-35"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> show(imgs):</span>
<span id="cb17-36">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(imgs, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>):</span>
<span id="cb17-37">        imgs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [imgs]</span>
<span id="cb17-38"></span>
<span id="cb17-39">    fig, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(ncols<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(imgs), figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">21</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>), squeeze<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, dpi<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">92</span>, tight_layout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, frameon<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb17-40">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i, img <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(imgs):</span>
<span id="cb17-41">        img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> img.detach()</span>
<span id="cb17-42">        img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> VF.to_pil_image(img)</span>
<span id="cb17-43">        axs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, i].imshow(np.asarray(img))</span>
<span id="cb17-44">        axs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, i].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(xticklabels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[], yticklabels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[], xticks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[], yticks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[])</span>
<span id="cb17-45"></span>
<span id="cb17-46"></span>
<span id="cb17-47">show(grid)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gcerar.github.io/posts/2023-09-15-neural-style-transfer/index_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Successfuly applied neural style transfer. The <strong>content</strong> image (left), the <strong>style</strong> image (center), and final <strong>target</strong> image (right).</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>Neural Style Transfer (NST) was a breakthrough deep learning approach that can transfer <strong>artistic style</strong> from one image to another. The key takeaway from my experience is the incredible potential of neural networks in merging art and tech, seamlessly blending the styles of different artworks with original images.</p>
<p>What stood out was the use of a pre-trained neural network for feature extraction, extracting feature maps from particular layers, and then the ability to balance the content and style weight parameters to maintain the essence of the original image while effectively imitating the artistic style.</p>
<p>Although the NST achieves pleasing results, it was soon overshadowed by faster and more advanced methods, such as <a href="https://en.wikipedia.org/wiki/DALL-E">DALL-E</a>, <a href="https://en.wikipedia.org/wiki/Stable_Diffusion">Stable Diffusion</a>, and <a href="https://en.wikipedia.org/wiki/Midjourney">Midjourney</a>. However, it represented a significant milestone toward artistic AI and generative AI models.</p>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>Helpful articles and code repositories while writing my implementation:</p>
<ul>
<li>Gregor Koehler <em>et al.</em> <a href="https://nextjournal.com/gkoehler/pytorch-neural-style-transfer">gkoehler/pytorch-neural-style-transfer</a> (best resource in my opinion)</li>
<li>Ritul’s <a href="https://medium.com/udacity-pytorch-challengers/style-transfer-using-deep-nural-network-and-pytorch-3fae1c2dd73e">Medium article</a> (good resource)</li>
<li>Pragati Baheti <a href="https://www.v7labs.com/blog/neural-style-transfer">blog</a> visually present style extraction</li>
<li>Aleksa Gordić (<a href="https://github.com/gordicaleksa/pytorch-neural-style-transfer">gordicaleksa/pytorch-neural-style-transfer</a>)</li>
<li><a href="https://github.com/ProGamerGov/neural-style-pt/blob/master/neural_style.py">ProGamerGov/neural-style-pt</a></li>
<li>Katherine Crowson (<a href="https://github.com/crowsonkb/style-transfer-pytorch/blob/master/style_transfer/style_transfer.py">rowsonkb/style-transfer-pytorch</a>)</li>
<li>Derrick Mwiti’s <a href="https://heartbeat.comet.ml/neural-style-transfer-with-pytorch-49e7c1fe3bea">Medium article</a></li>
<li>Aman Kumar Mallik’s <a href="https://towardsdatascience.com/implementing-neural-style-transfer-using-pytorch-fd8d43fb7bfa">Medium article</a></li>
</ul>
<p>I want to acknowledge the following artworks:</p>
<ul>
<li>“Gray Bridge and Trees” by Martin Damboldt</li>
<li>“Walking in the Rain” by Leonid Afremov</li>
<li>“The Starry Night” by Vincent van Gogh</li>
</ul>
<p>For a complete list of acknowledgments, please visit my GitHub repository:</p>
<ul>
<li><a href="https://github.com/gcerar/pytorch-neural-style-transfer/#acknowledgment">gcerar/pytorch-neural-style-transfer</a></li>
</ul>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="examples" class="level2">
<h2 class="anchored" data-anchor-id="examples">Examples</h2>
<p>A few cherry-picked examples of style transfer:</p>
<p><img src="gcerar.github.io/posts/2023-09-15-neural-style-transfer/examples/bridge+walking-in-the-rain.webp" class="img-fluid" alt="bridge + Walking in the Rain"> <img src="gcerar.github.io/posts/2023-09-15-neural-style-transfer/examples/walking-in-the-rain+bridge.webp" class="img-fluid" alt="Walking in the Rain + bridge"> <img src="gcerar.github.io/posts/2023-09-15-neural-style-transfer/examples/bridge+starry-night-crop.webp" class="img-fluid" alt="bridge + Starry Night"> <img src="gcerar.github.io/posts/2023-09-15-neural-style-transfer/examples/bridge+colorful-whirlpool.webp" class="img-fluid" alt="bridge + colorful whirlpool"></p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-gatys2015neural" class="csl-entry">
Gatys, Leon A, Alexander S Ecker, and Matthias Bethge. 2015. <span>“A Neural Algorithm of Artistic Style.”</span> <em>arXiv Preprint arXiv:1508.06576</em>.
</div>
</div></section></div> ]]></description>
  <category>pytorch</category>
  <category>NST</category>
  <guid>gcerar.github.io/posts/2023-09-15-neural-style-transfer/index.html</guid>
  <pubDate>Fri, 15 Sep 2023 00:00:00 GMT</pubDate>
  <media:content url="gcerar.github.io/posts/2023-09-15-neural-style-transfer/featured.webp" medium="image" type="image/webp"/>
</item>
</channel>
</rss>
